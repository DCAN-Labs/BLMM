{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLS_sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AS5zhoWCDZ8E",
        "2EWsCZjCQcc9",
        "rkd2LdwKLWtP",
        "RStOLwF_LlTE",
        "enYnjHWXBVmO",
        "mzUVeNWeDLmy",
        "TTQFQbRLDKmq",
        "ZFb5eO8jBWSf",
        "l1fjCdxgH3aV",
        "t9CsWWXKGk-I",
        "hqbOzGU-It3f",
        "-a7uYOZzPQ2W",
        "wW9A8JM3SRW7"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIMtrhFKB3Ay",
        "colab_type": "text"
      },
      "source": [
        "# PLS implementation in python\n",
        "\n",
        "This code implements the PLS algorithm for estimating the parameters of linear mixed effects models as described in [Bates (2015)](https://www.jstatsoft.org/article/view/v067i01/v67i01.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNFQ0-MpQuBm",
        "colab_type": "text"
      },
      "source": [
        "## Pip Installations\n",
        "\n",
        "Pip install everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX02P0KvBWJr",
        "colab_type": "code",
        "outputId": "f21cedf7-e476-4e69-cbd2-51d11f8b85a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        }
      },
      "source": [
        "!pip install numpy\n",
        "!pip install cvxopt\n",
        "!pip install pandas\n",
        "!pip install scipy\n",
        "!pip install matplotlib\n",
        "!pip install sparse\n",
        "!pip install nilearn\n",
        "!pip install nibabel\n",
        "!pip install dask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.17.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.17.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.6.0)\n",
            "Collecting sparse\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/d8/ac37573cab7e2000879cd1dfb194f317cce45c23844bf30bc6d8891a6836/sparse-0.8.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sparse) (1.17.4)\n",
            "Collecting numba>=0.45\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/34/22b6c2ded558976b5367be01b851ae679a0d1ba994de002d54afe84187b5/numba-0.46.0-cp36-cp36m-manylinux1_x86_64.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from sparse) (1.3.2)\n",
            "Requirement already satisfied: llvmlite>=0.30.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.45->sparse) (0.30.0)\n",
            "Installing collected packages: numba, sparse\n",
            "  Found existing installation: numba 0.40.1\n",
            "    Uninstalling numba-0.40.1:\n",
            "      Successfully uninstalled numba-0.40.1\n",
            "Successfully installed numba-0.46.0 sparse-0.8.0\n",
            "Collecting nilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/65/ba76e7cd544dafc28960e60b099d6f906a2096034c560158beaf2ff299bc/nilearn-0.5.2-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nibabel>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from nilearn) (2.3.3)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.0.2->nilearn) (1.17.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.0.2->nilearn) (1.12.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.0.2->nilearn) (0.98)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.5.2\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.6/dist-packages (2.3.3)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from nibabel) (1.17.4)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel) (0.98)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from nibabel) (1.12.0)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.6/dist-packages (1.1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4JYRICVBtjl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Python Imports\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkTBWbRKQ5ah",
        "colab_type": "text"
      },
      "source": [
        "We need:\n",
        " - `numpy` for matrix handling.\n",
        " - `scipy` for minimising the likelihood.\n",
        " - `cvxopt` for sparse cholesky.\n",
        " - `pandas` for quick reading and writing of csv files.\n",
        " - `os` for basic commandline functions\n",
        " - `time` for timing functions.\n",
        " - `matplotlib` for making displays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tebSlxvBBruv",
        "colab_type": "code",
        "outputId": "4f374a12-894e-42f2-e224-548bb5f57fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import numpy as np\n",
        "import cvxopt\n",
        "from cvxopt import cholmod, umfpack, amd, matrix, spmatrix, lapack\n",
        "import pandas as pd\n",
        "from scipy.optimize import root\n",
        "from scipy.optimize import minimize\n",
        "import scipy.sparse\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import sparse\n",
        "import nibabel as nib\n",
        "import nilearn\n",
        "import sparse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAB3qmDMHa8",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "This section contains miscellaneous functions used to help the `PLS` function as well as the general manipulation of the Sparse Cholesky decomposition given by `cvxopt`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOtDwtrfQk_Y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Sparse Cholesky Decomposition function\n",
        "\n",
        "This function takes in a square matrix **M** and outputs **P** and **L** from it's sparse cholesky decomposition of the form **PAP'=LL**'.\n",
        "\n",
        "Note: P is given as a permutation vector rather than a matrix. Also cholmod.options['supernodal'] must be set to 2.\n",
        " \n",
        "---\n",
        " \n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **M**: The matrix to be sparse cholesky decomposed as an spmatrix from the cvxopt package.\n",
        " - **perm**: Input permutation (*optional*, one will be calculated if not)\n",
        " - **retF**: Return the factorisation object or not\n",
        " - **retP**: Return the permutation or not\n",
        " - **retL**: Return the lower cholesky or not\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpyEbgrhNHzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_chol(M, perm=None, retF=False, retP=True, retL=True):\n",
        "\n",
        "    # Quick check that M is square\n",
        "    if M.size[0]!=M.size[1]:\n",
        "        raise Exception('M must be square.')\n",
        "\n",
        "    if not perm is None:\n",
        "        # Make an expression for the factorisation\n",
        "        F=cholmod.symbolic(M,p=perm)\n",
        "    else:\n",
        "        # Make an expression for the factorisation\n",
        "        F=cholmod.symbolic(M)\n",
        "\n",
        "    # Calculate the factorisation\n",
        "    cholmod.numeric(M, F)\n",
        "\n",
        "    # Empty factorisation object\n",
        "    factorisation = {}\n",
        "\n",
        "    if (retF and retL) or (retF and retP):\n",
        "\n",
        "        # Calculate the factorisation again (buggy if returning L for\n",
        "        # some reason)\n",
        "        F2=cholmod.symbolic(M,p=perm)\n",
        "        cholmod.numeric(M, F2)\n",
        "\n",
        "        # If we want to return the F object, add it to the dictionary\n",
        "        factorisation['F']=F2\n",
        "        \n",
        "    else:\n",
        "      \n",
        "      factorisation['F']=F\n",
        "\n",
        "    if retP:\n",
        "\n",
        "        # Set p to [0,...,n-1]\n",
        "        P = cvxopt.matrix(range(M.size[0]), (M.size[0],1), tc='d')\n",
        "\n",
        "        # Solve and replace p with the true permutation used\n",
        "        cholmod.solve(F, P, sys=7)\n",
        "\n",
        "        # Convert p into an integer array; more useful that way\n",
        "        P=cvxopt.matrix(np.array(P).astype(np.int64),tc='i')\n",
        "\n",
        "        # If we want to return the permutation, add it to the dictionary\n",
        "        factorisation['P']=P\n",
        "\n",
        "    if retL:\n",
        "\n",
        "        # Get the sparse cholesky factor\n",
        "        L=cholmod.getfactor(F)\n",
        "        \n",
        "        # If we want to return the factor, add it to the dictionary\n",
        "        factorisation['L']=L\n",
        "\n",
        "    # Return P and L\n",
        "    return(factorisation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xYlGmnNNnit",
        "colab_type": "text"
      },
      "source": [
        "### Inverse mapping function\n",
        "\n",
        "This function takes in a lower triangular \n",
        "block diagonal matrix, lambda, and maps it to the original vector of parameters, theta.\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **Lambda**: The sparse lower triangular block diagonal matrix.\n",
        " \n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59GHirQ8OGe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inv_mapping(Lambda):\n",
        "\n",
        "    # List the unique elements of lambda (in the\n",
        "    # correct order; pandas does this, numpy does\n",
        "    # not)\n",
        "    theta = pd.unique(list(cvxopt.spmatrix.trans(Lambda)))\n",
        "    return(theta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsYGm--5isJY",
        "colab_type": "text"
      },
      "source": [
        "### Calculate mapping function\n",
        "\n",
        "This function takes in a vector of parameters, theta, and returns indices which maps them the to lower triangular block diagonal matrix, lambda.\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **nlevels**: a vector of the number of levels for each grouping factor. e.g. nlevels=[10,2] means there are 10 levels for factor 1 and 2 levels for factor 2.\n",
        " - **nparams**: a vector of the number of variables for each grouping factor. e.g. nparams=[3,4] means there are 3 variables for factor 1 and 4 variables for factor 2.\n",
        "\n",
        "All arrays must be np arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgZfxqOUitYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mapping(nlevels, nparams):\n",
        "\n",
        "    # Work out how many factors there are\n",
        "    n_f = len(nlevels)\n",
        "\n",
        "    # Quick check that nlevels and nparams are the same length\n",
        "    if len(nlevels)!=len(nparams):\n",
        "        raise Exception('The number of parameters and number of levels should be recorded for every grouping factor.')\n",
        "\n",
        "    # Work out how many lambda components needed for each factor\n",
        "    n_lamcomps = (np.multiply(nparams,(nparams+1))/2).astype(np.int64)\n",
        "\n",
        "    # Block index is the index of the next un-indexed diagonal element\n",
        "    # of Lambda\n",
        "    block_index = 0\n",
        "\n",
        "    # Row indices and column indices of theta\n",
        "    row_indices = np.array([])\n",
        "    col_indices = np.array([])\n",
        "\n",
        "    # This will have the values of theta repeated several times, once\n",
        "    # for each time each value of theta appears in lambda\n",
        "    theta_repeated_inds = np.array([])\n",
        "    \n",
        "    # Loop through factors generating the indices to map theta to.\n",
        "    for i in range(0,n_f):\n",
        "\n",
        "        # Work out the indices of a lower triangular matrix\n",
        "        # of size #variables(factor) by #variables(factor)\n",
        "        row_inds_tri, col_inds_tri = np.tril_indices(nparams[i])\n",
        "\n",
        "        # Work out theta for this block\n",
        "        theta_current_inds = np.arange(np.sum(n_lamcomps[0:i]),np.sum(n_lamcomps[0:(i+1)]))\n",
        "\n",
        "        # Work out the repeated theta\n",
        "        theta_repeated_inds = np.hstack((theta_repeated_inds, np.tile(theta_current_inds, nlevels[i])))\n",
        "\n",
        "        # For each level of the factor we must repeat the lower\n",
        "        # triangular matrix\n",
        "        for j in range(0,nlevels[i]):\n",
        "\n",
        "            # Append the row/column indices to the running list\n",
        "            row_indices = np.hstack((row_indices, (row_inds_tri+block_index)))\n",
        "            col_indices = np.hstack((col_indices, (col_inds_tri+block_index)))\n",
        "\n",
        "            # Move onto the next block\n",
        "            block_index = block_index + nparams[i]\n",
        "\n",
        "    # Create lambda as a sparse matrix\n",
        "    #lambda_theta = spmatrix(theta_repeated.tolist(), row_indices.astype(np.int64), col_indices.astype(np.int64))\n",
        "\n",
        "    # Return lambda\n",
        "    return(theta_repeated_inds, row_indices, col_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdRkv7PrrgsI",
        "colab_type": "text"
      },
      "source": [
        "### Apply mapping function\n",
        "\n",
        "The below function applies a mapping to a vector of parameters.\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **theta**: the vector of theta parameters.\n",
        " - **theta_inds**: A vector specifying how many times each theta parameter should be repeated. For example, if theta=[0.1,0.8,0.3] and theta_inds=[1,1,1,2,3,3], then the values to be mapped into the sparse matrix would be [0.1,0.1,0.1,0.8,0.3,0.3].\n",
        " - **r_inds**: The row indices of the elements mapped into the sparse matrix.\n",
        " - **c_inds**: The column indices of the elements mapped into the sparse matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxUv8_r0rhFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapping(theta, theta_inds, r_inds, c_inds):\n",
        "\n",
        "    return(spmatrix(theta[theta_inds.astype(np.int64)].tolist(), r_inds.astype(np.int64), c_inds.astype(np.int64)))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRhb0e2brrVI",
        "colab_type": "text"
      },
      "source": [
        "###Matrix to Vector function\n",
        "\n",
        "This function takes in a (symmetric, square) matrix and half-vectorizes it (i.e. transforms it to a vector of each of the columns of the matrix, below and including the diagonal, stacked on top of one another). Example:\n",
        "\n",
        "$$\\begin{bmatrix} a & b & c \\\\ b & d & e \\\\ c & e & f \\\\\\end{bmatrix} \\rightarrow \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix}$$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MDStAhnrrgU",
        "colab_type": "code",
        "outputId": "85e2af42-2fde-4703-ae16-a9fb5b9ee215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "def mat2vech(matrix):\n",
        "  \n",
        "  # Get lower triangular indices\n",
        "  rowinds, colinds = np.tril_indices(matrix.shape[0])\n",
        "  \n",
        "  # They're in the wrong order so we need to order them\n",
        "  # To do this we first hash them\n",
        "  indhash = colinds*matrix.shape[0]+rowinds\n",
        "  \n",
        "  # Sort permutation\n",
        "  perm=np.argsort(indhash)\n",
        "  \n",
        "  # Return vectorised half-matrix\n",
        "  return(np.array([matrix[rowinds[perm],colinds[perm]]]).transpose())\n",
        "\n",
        "# Example:\n",
        "matrix1 = np.eye(3,3)\n",
        "print(matrix1*matrix1.transpose())\n",
        "print(mat2vech(matrix1*matrix1.transpose()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "[[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS5zhoWCDZ8E",
        "colab_type": "text"
      },
      "source": [
        "## Toy Dataset\n",
        "\n",
        "This section read ins and formats a toy dataset. The files used here were generated in `R` and with **True** values (those with postfix `True`) being those used to generate the data and **Estimated** (those with postfix `REst`) values being the estimates `R`'s `lmer` package generated from this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy39zwuhkn4C",
        "colab_type": "code",
        "outputId": "b5bc2eae-2d44-46be-d9b9-d11a78d96e6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Make a data directory\n",
        "if not os.path.isdir('/Data'):\n",
        "  os.mkdir('/Data')\n",
        "  \n",
        "os.chdir('/Data')\n",
        "\n",
        "# Clone small git repo containg some csv files.\n",
        "if not os.path.isdir('/Data/BLMM-testdata'):\n",
        "  !git clone https://github.com/TomMaullin/BLMM-testdata.git\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BLMM-testdata'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 25 (delta 2), reused 11 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EWsCZjCQcc9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Z matrix\n",
        "\n",
        "The below reads in Z and makes an image of Z transpose.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoKOwHqcEKDT",
        "colab_type": "code",
        "outputId": "102244b8-0bcc-47fd-ff44-f78795bb8550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Read in random effects design matrix and convert it into it's sparse format in\n",
        "# cvxopt.\n",
        "Z_3col=pd.read_csv('/Data/BLMM-testdata/Z_3col.csv',header=None).values\n",
        "Z = cvxopt.spmatrix(Z_3col[:,2].tolist(), (Z_3col[:,0]-1).astype(np.int64), \\\n",
        "                    (Z_3col[:,1]-1).astype(np.int64))\n",
        "\n",
        "# Create an image of Z'\n",
        "imshow(np.array(cvxopt.matrix(cvxopt.spmatrix.trans(Z))), \\\n",
        "       interpolation='nearest', vmin=-5, vmax=5, aspect='auto')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb7e7125c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD6CAYAAAC4RRw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkd2LdwKLWtP",
        "colab_type": "text"
      },
      "source": [
        "### Estimated Random Effects matrix\n",
        "\n",
        "The below reads in the Random effects variance predicted by `R`'s `lmer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOqQRAROqdkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in estimated variance\n",
        "RFXVar_REst =pd.read_csv('/Data/BLMM-testdata/estd_rfxvar.csv',header=None).values\n",
        "\n",
        "# The RFX variance was stored in a strange way so we need to reformat slightly\n",
        "RFXVar_REst = spmatrix(RFXVar_REst[RFXVar_REst!=0],[0,0,1,1,2,2,3,3],[0,1,0,1,2,3,2,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKE_Rr53qeHx",
        "colab_type": "text"
      },
      "source": [
        "### Estimated Theta\n",
        "\n",
        "The below calculates and outputs the `theta` parameter values which `lmer` estimated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeaYoXU0Lnlr",
        "colab_type": "code",
        "outputId": "e81bac7f-07da-4225-b48b-1689df8458de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We now need the Sparse Cholesky decomposition of the RFX variance to obtain \n",
        "# the theta values.\n",
        "f = sparse_chol(RFXVar_REst)\n",
        "\n",
        "# We map the L matrix from the decomposition to theta\n",
        "theta_REst = inv_mapping(f['L'])\n",
        "\n",
        "# This is the desired outcome/theta value of the PLS algorithm. It should match \n",
        "# the R output.\n",
        "print(theta_REst)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.11365482  0.32356815  2.22872418  4.54822071 -0.17500295  0.42370817]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RStOLwF_LlTE",
        "colab_type": "text"
      },
      "source": [
        "### Y vector\n",
        "\n",
        "The response vector is read in here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG8eWNdpPQOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y=matrix(pd.read_csv('/Data/BLMM-testdata/Y.csv',header=None).values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHm6DjdbPTvg",
        "colab_type": "text"
      },
      "source": [
        "### X matrix\n",
        "\n",
        "The fixed effects design matrix is read in here. It consists of an intercept and two random (Gaussian) columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqTKH4n_Po8e",
        "colab_type": "code",
        "outputId": "2768b92f-ed11-478b-c10a-16c42e687f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X=matrix(pd.read_csv('/Data/BLMM-testdata/X.csv',header=None).values)\n",
        "\n",
        "# Image of the first 20 rows of X\n",
        "imshow(X[1:20,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb7e6c6bb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFYAAAD4CAYAAAB2QBFpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niRDrRIh1IsQ6EWKdCLFO/A/Tno1q3pV2hwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_sXGD8qzskp",
        "colab_type": "text"
      },
      "source": [
        "### Number of Levels and Parameters\n",
        "\n",
        "The number of levels is given by a vector with one entry for each grouping factor. e.g. nlevels=[10,2] means there are 10 levels for factor 1 and 2 levels for factor 2. \n",
        "\n",
        "The number of parameters is given by a vector with one entry for each grouping factor. e.g. nparams=[3,4] means there are 3 variables for factor 1 and 4 variables for factor 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e5bUA2DztCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlevels = np.array([20,3])\n",
        "nparams = np.array([2,2])\n",
        "\n",
        "# Number of subjects\n",
        "n = X.size[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0byKXygpCa1P",
        "colab_type": "text"
      },
      "source": [
        "### True b values\n",
        "\n",
        "The true recorded values of the random effects b vector in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzwuuWSdCbCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_True=matrix(pd.read_csv('/Data/BLMM-testdata/true_b.csv',header=None).values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvfns6c-CbPd",
        "colab_type": "text"
      },
      "source": [
        "### True beta values\n",
        "\n",
        "The true fixed effects parameters used to generate this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlvolWwvCbbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beta_True=matrix(pd.read_csv('/Data/BLMM-testdata/true_beta.csv',header=None).values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjWoiZtYyuRh",
        "colab_type": "text"
      },
      "source": [
        "### Product Matrices\n",
        "\n",
        "It is useful to calculate all product of matrices before hand as it is both more computationally efficient and also similar to the setting we are interested in. In `cvxopt`, transposing matrices is surprisingly costly in terms of time so even transposes are calculated here to save them having to be recalculated during each iteration of the minimisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMX_8BRYzFhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Z tranpose X\n",
        "ZtX=cvxopt.spmatrix.trans(Z)*X\n",
        "\n",
        "# Z tranpose Y\n",
        "ZtY=cvxopt.spmatrix.trans(Z)*Y\n",
        "\n",
        "# X tranpose X\n",
        "XtX=cvxopt.matrix.trans(X)*X\n",
        "\n",
        "# Z tranpose Z\n",
        "ZtZ=cvxopt.spmatrix.trans(Z)*Z\n",
        "\n",
        "# X tranpose Y\n",
        "XtY=cvxopt.matrix.trans(X)*Y\n",
        "\n",
        "# Y tranpose X\n",
        "YtX=cvxopt.matrix.trans(Y)*X\n",
        "\n",
        "# Y transpose Z\n",
        "YtZ=cvxopt.matrix.trans(Y)*Z\n",
        "\n",
        "# X tranpose Z\n",
        "XtZ=cvxopt.matrix.trans(X)*Z\n",
        "\n",
        "# Y tranpose Y\n",
        "YtY=cvxopt.matrix.trans(Y)*Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhOHZ6F2DU5e",
        "colab_type": "text"
      },
      "source": [
        "## Calculating the Minimised Log-Likelihood\n",
        "\n",
        "To calculate the minimised log-likelihood the method given by Bates (2015) is used. This involves minimising for a parameter vector theta, a penalized least squares function which is based on the sparse cholesky decomposition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVtC2_vLtsY2",
        "colab_type": "text"
      },
      "source": [
        "### PLS function\n",
        "\n",
        "This function calculates the log likelihood value for parameter vector theta using **P**enalized **L**east **S**quares..\n",
        "\n",
        "---\n",
        "\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **theta**: The parameter estimate.\n",
        " - **ZtX**: Z transpose multiplied by X.\n",
        " - **ZtY**: Z transpose multiplied by Y.\n",
        " - **XtX**: X transpose multiplied by X.\n",
        " - **ZtZ**: Z transpose multiplied by Z.\n",
        " - **XtY**: X transpose multiplied by Y.\n",
        " - **YtX**: Y transpose multiplied by X.\n",
        " - **YtZ**: Y transpose multiplied by Z.\n",
        " - **XtZ**: X transpose multiplied by Z.\n",
        " - **YtY**: Y transpose multiplied by Y.\n",
        " - **P**: The sparse permutation for Lamda'Z'ZLambda+I\n",
        " - **tinds**: A vector specifying how many times each theta parameter should be repeated. For example, if theta=[0.1,0.8,0.3] and theta_inds=[1,1,1,2,3,3], then the values to be mapped into the sparse matrix would be [0.1,0.1,0.1,0.8,0.3,0.3].\n",
        " - **r_inds**: The row indices of the elements mapped into the sparse matrix.\n",
        " - **c_inds**: The column indices of the elements mapped into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR3bUshSts74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PLS(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds):\n",
        "\n",
        "    # Obtain Lambda\n",
        "    t1 = time.time()\n",
        "    Lambda = mapping(theta, tinds, rinds, cinds)\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#3.170967102050781e-05   9\n",
        "    \n",
        "    \n",
        "    # Obtain Lambda'\n",
        "    t1 = time.time()\n",
        "    Lambdat = spmatrix.trans(Lambda)\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)# 3.5762786865234375e-06\n",
        "\n",
        "    # Obtain Lambda'Z'Y and Lambda'Z'X\n",
        "    t1 = time.time()\n",
        "    LambdatZtY = Lambdat*ZtY\n",
        "    LambdatZtX = Lambdat*ZtX\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#1.049041748046875e-05   13\n",
        "    \n",
        "    # Obtain the cholesky decomposition\n",
        "    t1 = time.time()\n",
        "    LambdatZtZLambda = Lambdat*(ZtZ*Lambda)\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#3.790855407714844e-05   2\n",
        "    \n",
        "    t1 = time.time()\n",
        "    chol_dict = sparse_chol(LambdatZtZLambda+I, perm=P, retF=True, retP=False, retL=False)\n",
        "    F = chol_dict['F']\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#0.0001342296600341797   1\n",
        "\n",
        "    # Obtain C_u (annoyingly solve writes over the second argument,\n",
        "    # whereas spsolve outputs)\n",
        "    t1 = time.time()\n",
        "    Cu = LambdatZtY[P,:]\n",
        "    cholmod.solve(F,Cu,sys=4)\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#1.5974044799804688e-05   5\n",
        "\n",
        "    # Obtain RZX\n",
        "    t1 = time.time()\n",
        "    RZX = LambdatZtX[P,:]\n",
        "    cholmod.solve(F,RZX,sys=4)\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#1.2159347534179688e-05   7\n",
        "\n",
        "    # Obtain RXtRX\n",
        "    t1 = time.time()\n",
        "    RXtRX = XtX - matrix.trans(RZX)*RZX\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#9.775161743164062e-06  11\n",
        "\n",
        "    # Obtain beta estimates (note: gesv also replaces the second\n",
        "    # argument)\n",
        "    t1 = time.time()\n",
        "    betahat = XtY - matrix.trans(RZX)*Cu\n",
        "    lapack.gesv(RXtRX, betahat)\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#1.7404556274414062e-05   6\n",
        "\n",
        "    # Obtain u estimates\n",
        "    t1 = time.time()\n",
        "    uhat = Cu-RZX*betahat\n",
        "    cholmod.solve(F,uhat,sys=5)\n",
        "    cholmod.solve(F,uhat,sys=8)\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#1.2874603271484375e-05   8\n",
        "    \n",
        "    # Obtain b estimates\n",
        "    t1 = time.time()\n",
        "    bhat = Lambda*uhat\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#2.86102294921875e-06  15\n",
        "    \n",
        "    # Obtain residuals sum of squares\n",
        "    t1 = time.time()\n",
        "    resss = YtY-2*YtX*betahat-2*YtZ*bhat+2*matrix.trans(betahat)*XtZ*bhat+matrix.trans(betahat)*XtX*betahat+matrix.trans(bhat)*ZtZ*bhat\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#3.409385681152344e-05   4\n",
        "    \n",
        "    # Obtain penalised residual sum of squares\n",
        "    t1 = time.time()\n",
        "    pss = resss + matrix.trans(uhat)*uhat\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#2.6226043701171875e-06  16\n",
        "    \n",
        "    # Obtain Log(|L|^2)\n",
        "    t1 = time.time()\n",
        "    logdet = 2*sum(cvxopt.log(cholmod.diag(F)))\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#1.5735626220703125e-05   14\n",
        "    \n",
        "    # Obtain log likelihood\n",
        "    t1 = time.time()\n",
        "    logllh = -logdet/2-n/2*(1+np.log(2*np.pi*pss[0,0])-np.log(n))\n",
        "    t2 = time.time()\n",
        "    #print(t2-t1)#4.506111145019531e-05   3\n",
        "    \n",
        "    return(-logllh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzkEDjxsxHCl",
        "colab_type": "text"
      },
      "source": [
        "## Minimising the Log-Likelihood\n",
        "\n",
        "To minimise the log-likelihood  we use the `scipy optimize` functions on the PLS algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwPiugXwyFpW",
        "colab_type": "code",
        "outputId": "c6d80bae-ffe4-4fca-9e99-a2411dee0f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "# Initial theta value. Bates (2005) suggests using vec(I) where I is the identity matrix\n",
        "theta0=np.array([1,0,1,1,0,1])\n",
        "\n",
        "# Obtain a random Lambda matrix with the correct sparsity for the permutation vector\n",
        "tinds,rinds,cinds=get_mapping(nlevels, nparams)\n",
        "Lam=mapping(np.random.randn(6),tinds,rinds,cinds)\n",
        "\n",
        "# Obtain Lambda'Z'ZLambda\n",
        "LamtZt = spmatrix.trans(Lam)*spmatrix.trans(Z)\n",
        "LamtZtZLam = LamtZt*spmatrix.trans(LamtZt)\n",
        "\n",
        "# Obtaining permutation for PLS\n",
        "P=cvxopt.amd.order(LamtZtZLam)\n",
        "\n",
        "# Identity (Actually quicker to calculate outside of estimation)\n",
        "I = spmatrix(1.0, range(Lam.size[0]), range(Lam.size[0]))\n",
        "\n",
        "# Set cholmod options\n",
        "cholmod.options['supernodal']=2\n",
        "\n",
        "# Obtain estimate using `Nelder-Mead`\n",
        "theta_Est_nm=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='Nelder-Mead', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `Powell`\n",
        "theta_Est_pow=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='Powell', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `CG`\n",
        "theta_Est_cg=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='CG', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `BFGS`\n",
        "theta_Est_bfgs=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='BFGS', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `L-BFGS-B`\n",
        "theta_Est_lbfgsb=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `TNC`\n",
        "theta_Est_tnc=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='TNC', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `COBYLA`\n",
        "theta_Est_cobyla=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='COBYLA', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `SLSQP`\n",
        "theta_Est_slsqp=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='SLSQP', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `trust-constr`\n",
        "theta_Est_trcon=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='trust-constr', tol=1e-6).x\n",
        "\n",
        "# Print lmer estimate\n",
        "print('Lmer Estimate')\n",
        "print(theta_REst)\n",
        "\n",
        "# Print estimates\n",
        "print('Nelder-Mead Python Estimate')\n",
        "print(theta_Est_nm)\n",
        "print('Powell Python Estimate')\n",
        "print(theta_Est_pow)\n",
        "print('CG Python Estimate')\n",
        "print(theta_Est_cg)\n",
        "print('BFGS Python Estimate')\n",
        "print(theta_Est_bfgs)\n",
        "print('L-BFGS-B Python Estimate')\n",
        "print(theta_Est_lbfgsb)\n",
        "print('TNC Python Estimate')\n",
        "print(theta_Est_tnc)\n",
        "print('COBYLA Python Estimate')\n",
        "print(theta_Est_cobyla)\n",
        "print('SLSQP Python Estimate')\n",
        "print(theta_Est_slsqp)\n",
        "print('trust-constr Python Estimate')\n",
        "print(theta_Est_trcon)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lmer Estimate\n",
            "[ 1.11365482  0.32356815  2.22872418  4.54822071 -0.17500295  0.42370817]\n",
            "Nelder-Mead Python Estimate\n",
            "[ 1.11126947  0.32242321  2.2240467   4.53967691 -0.17465665  0.42288958]\n",
            "Powell Python Estimate\n",
            "[ 1.11126202  0.32242066  2.22404517  4.53963679 -0.17464939 -0.42289163]\n",
            "CG Python Estimate\n",
            "[ 1.11097483  0.32486049  2.2204823   4.46851015 -0.17170456  0.42445066]\n",
            "BFGS Python Estimate\n",
            "[ 1.1133477   0.32432107  2.23520283  4.73015087 -0.19917473  0.4166644 ]\n",
            "L-BFGS-B Python Estimate\n",
            "[ 1.11069312  0.32626445  2.22179103  4.43127593 -0.17151019  0.42308999]\n",
            "TNC Python Estimate\n",
            "[ 1.317788    0.08469002  2.24681162  3.09562961 -0.04547548  0.38196296]\n",
            "COBYLA Python Estimate\n",
            "[ 1.11129921  0.32238177  2.22400895  4.51698988 -0.17302663  0.42316139]\n",
            "SLSQP Python Estimate\n",
            "[ 1.1119122   0.31467079  2.22144594  4.39343954 -0.16444859  0.42634387]\n",
            "trust-constr Python Estimate\n",
            "[ 1.11011197  0.32224344  2.22245804  4.51683091 -0.1711171   0.42292119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxNYlQkQ_asL",
        "colab_type": "text"
      },
      "source": [
        "### Time efficiency\n",
        "\n",
        "The below runs each method 100 times and works out the average time taken to produce the estimates. In this example, it is clear that `Powell` performs much quicker than `Nelder-Mead`. From now on, in the following sections `Powell` is the default method used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYq_1-3U_nMn",
        "colab_type": "code",
        "outputId": "d1a3a30e-f8bc-4cfc-e592-6a0b81c072b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# Average time\n",
        "time_nm = 0;\n",
        "time_p = 0;\n",
        "time_cg = 0;\n",
        "time_bfgs = 0;\n",
        "time_lbfgsb = 0;\n",
        "time_tnc = 0;\n",
        "time_cobyla = 0;\n",
        "time_slsqp = 0;\n",
        "time_trcon = 0;\n",
        "\n",
        "# Number of iterations\n",
        "n = 10;\n",
        "\n",
        "for i in range(n):\n",
        "  \n",
        "  # Obtain estimate using `Nelder-Mead`\n",
        "  t1 = time.time()\n",
        "  theta_Est_nm=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='Nelder-Mead', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_nm = time_nm + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `Powell`\n",
        "  t1 = time.time()\n",
        "  theta_Est_p=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='Powell', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_p = time_p + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `CG`\n",
        "  t1 = time.time()\n",
        "  theta_Est_cg=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='CG', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_cg = time_cg + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `BFGS`\n",
        "  t1 = time.time()\n",
        "  theta_Est_bfgs=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='BFGS', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_bfgs = time_bfgs + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `L-BFGS-B`\n",
        "  t1 = time.time()\n",
        "  theta_Est_lbfgsb=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_lbfgsb = time_lbfgsb + t2 - t1\n",
        "\n",
        "  # Obtain estimate using `TNC`\n",
        "  t1 = time.time()\n",
        "  theta_Est_tnc=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='TNC', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_tnc = time_tnc + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `COBYLA`\n",
        "  t1 = time.time()\n",
        "  theta_Est_cobyla=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='COBYLA', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_cobyla = time_cobyla + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `SLSQP`\n",
        "  t1 = time.time()\n",
        "  theta_Est_slsqp=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='SLSQP', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_slsqp = time_slsqp + t2 - t1\n",
        "\n",
        "  # Obtain estimate using `trust-constr`\n",
        "  t1 = time.time()\n",
        "  theta_Est_trcon=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds,rinds, cinds), method='trust-constr', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_trcon = time_trcon + t2 - t1\n",
        "\n",
        "  \n",
        "\n",
        "# Convert to averages\n",
        "time_nm = time_nm/n\n",
        "time_p = time_p/n\n",
        "time_cg = time_cg/n\n",
        "time_bfgs = time_bfgs/n\n",
        "time_lbfgsb = time_lbfgsb/n\n",
        "time_tnc = time_tnc/n\n",
        "time_cobyla = time_cobyla/n\n",
        "time_slsqp = time_slsqp/n\n",
        "time_trcon = time_trcon/n\n",
        "\n",
        "# Print\n",
        "print('Average Nelder-Mead estimation time:')\n",
        "print(time_nm)\n",
        "print('Average Powell estimation time:')\n",
        "print(time_p)\n",
        "print('Average CG estimation time:')\n",
        "print(time_cg)\n",
        "print('Average BFGS estimation time:')\n",
        "print(time_bfgs)\n",
        "print('Average L-BFGS-B estimation time:')\n",
        "print(time_lbfgsb)\n",
        "print('Average TNC estimation time:')\n",
        "print(time_tnc)\n",
        "print('Average COBYLA estimation time:')\n",
        "print(time_cobyla)\n",
        "print('Average SLSQP estimation time:')\n",
        "print(time_slsqp)\n",
        "print('Average trust-contstr estimation time:')\n",
        "print(time_trcon)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Nelder-Mead estimation time:\n",
            "0.3104363441467285\n",
            "Average Powell estimation time:\n",
            "0.05762038230895996\n",
            "Average CG estimation time:\n",
            "0.3760460138320923\n",
            "Average BFGS estimation time:\n",
            "0.21434075832366944\n",
            "Average L-BFGS-B estimation time:\n",
            "0.09066901206970215\n",
            "Average TNC estimation time:\n",
            "0.14951188564300538\n",
            "Average COBYLA estimation time:\n",
            "0.1998685598373413\n",
            "Average SLSQP estimation time:\n",
            "0.06683473587036133\n",
            "Average trust-contstr estimation time:\n",
            "0.3898428678512573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nazpuh-A5vx",
        "colab_type": "text"
      },
      "source": [
        "## Scaling up the computation (Two voxels)\n",
        "\n",
        "This section has several implemented ideas for scaling up the computation to compute several similar models at once. For simplicity it is assumed here that X and Z are the same across voxels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enYnjHWXBVmO",
        "colab_type": "text"
      },
      "source": [
        "### Toy data for Neighbouring voxel\n",
        "\n",
        "To assess the potential of the following ideas a toy data example is created below. The idea behind this is that we wish to calculate both the model in the example used in the previous sections and, additionally a similar model from a neighbouring voxel (variables related to the neighbouring voxel will have postfix `_n`). \n",
        "\n",
        "This is not a rigourous test, but just a toy example to see if we can lower the computational time in an example vaguely similar to what we may expect in reality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzUVeNWeDLmy",
        "colab_type": "text"
      },
      "source": [
        "#### Beta vector\n",
        "\n",
        "In a neighbouring voxel, we would expect similar Beta values but not necessarily the same. To simulate this, I have added some normal noise with variance, 1/2, to the original beta values to obtain a new beta value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hKzWuueBbeI",
        "colab_type": "code",
        "outputId": "d914b03b-f251-432d-aaaa-0121f743c127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Given a beta vector this function makes a beta \n",
        "# vector for the neighbouring voxel\n",
        "def beta_n(beta):\n",
        "  return(beta + cvxopt.normal(beta.size[0],1)/np.sqrt(2))\n",
        "\n",
        "# Example\n",
        "beta_True_n = beta_n(beta_True)\n",
        "  \n",
        "# print betas for comparison\n",
        "print(\"Beta for voxel 1\")\n",
        "print(beta_True)\n",
        "print(\"Beta for voxel 2\")\n",
        "print(beta_True_n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beta for voxel 1\n",
            "[ 1]\n",
            "[ 2]\n",
            "[ 3]\n",
            "\n",
            "Beta for voxel 2\n",
            "[ 1.09e+00]\n",
            "[ 1.94e+00]\n",
            "[ 4.18e+00]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTQFQbRLDKmq",
        "colab_type": "text"
      },
      "source": [
        "#### b vector\n",
        "\n",
        "In a neighbouring voxel, we may also expect similar b values. To simulate this, I have added some normal noise with variance, 1/2, to the original b values to obtain a new b value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsyjopBUFNt0",
        "colab_type": "code",
        "outputId": "051359a6-e6e0-413a-b9d0-001c9e22b33e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Given a b vector this function makes a b\n",
        "# vector for the neighbouring voxel\n",
        "def b_n(b):\n",
        "  return(b + cvxopt.normal(b.size[0],1)/np.sqrt(2))\n",
        "\n",
        "# Example\n",
        "b_True_n = b_n(b_True)\n",
        "  \n",
        "# print bs for comparison\n",
        "print(\"b for voxel 1 (first 5 elements)\")\n",
        "print(b_True[1:5])\n",
        "print(\"b for voxel 2 (first 5 elements)\")\n",
        "print(b_True_n[1:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b for voxel 1 (first 5 elements)\n",
            "[-4.23e+00]\n",
            "[ 1.44e+00]\n",
            "[ 1.41e+00]\n",
            "[ 1.35e+00]\n",
            "\n",
            "b for voxel 2 (first 5 elements)\n",
            "[-4.37e+00]\n",
            "[ 1.13e+00]\n",
            "[ 1.50e+00]\n",
            "[ 1.65e+00]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFb5eO8jBWSf",
        "colab_type": "text"
      },
      "source": [
        " #### Y vector (New response)\n",
        " \n",
        "I now generate a new response vector with the new beta and b values for the neighbouring voxel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V59rDqEGU5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neighbouring voxels response vector\n",
        "Y_n = X*beta_True_n+Z*b_True_n+cvxopt.normal(1000,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1fjCdxgH3aV",
        "colab_type": "text"
      },
      "source": [
        "### Product Matrices\n",
        "\n",
        "All products of matrices are calculated beforehand as it is both more computationally efficient and also similar to the setting we are interested in. For the neighbouring voxel only those involving the Y vector (response) need be recalculated as X and Z have not changed between voxel in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnx7aj6OH3tA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Z tranpose Y_n\n",
        "ZtY_n=cvxopt.spmatrix.trans(Z)*Y_n\n",
        "\n",
        "# X tranpose Y_n\n",
        "XtY_n=cvxopt.matrix.trans(X)*Y_n\n",
        "\n",
        "# Y_n tranpose X\n",
        "YtX_n=cvxopt.matrix.trans(Y_n)*X\n",
        "\n",
        "# Y_n transpose Z\n",
        "YtZ_n=cvxopt.matrix.trans(Y_n)*Z\n",
        "\n",
        "# Y_n tranpose Y_n\n",
        "YtY_n=cvxopt.matrix.trans(Y_n)*Y_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9CsWWXKGk-I",
        "colab_type": "text"
      },
      "source": [
        "### Idea 1: Initial Estimate\n",
        "\n",
        "The idea behind this is relatively simple. Once we have estimated the model for one voxel, we use the estimated theta vector from that voxel as the initial estimate for it's neighbours.\n",
        "\n",
        "However, it didn't work/no meaningful time improvement was observed in the toy example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5c3WZF5HDm-",
        "colab_type": "code",
        "outputId": "8d87bcef-2812-40f9-fc77-6979ff7eb8b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Estimate first voxel\n",
        "theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Estimate second voxel\n",
        "theta_Est_n=minimize(PLS, theta_Est, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Print results\n",
        "print('Estimate for voxel 1')\n",
        "print(theta_Est)\n",
        "\n",
        "# Print results for neighbouring voxel\n",
        "print('Estimate for voxel 2')\n",
        "print(theta_Est_n)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimate for voxel 1\n",
            "[-3.82632372e-07  7.88461899e-09 -8.92650112e-08  1.41769524e-06\n",
            " -4.58399429e-07 -4.63805416e-08]\n",
            "Estimate for voxel 2\n",
            "[-3.79166294e-07 -1.82553801e-08  8.04283532e-08  1.40643049e-06\n",
            " -1.61603437e-07 -1.88357309e-08]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqbOzGU-It3f",
        "colab_type": "text"
      },
      "source": [
        "#### Time efficiency\n",
        "\n",
        "What is really important here is not the estimate values (assuming they are correct), but the time taken to do this for both voxels. In the below, the two voxels are estimated 100 times twice, once reusing the estmate from the first voxel in estimating the second, and once computing the voxels completely seperately.\n",
        "\n",
        "**Conclusion:** This idea didn't really work... the time complexity was not meaningfully reduced (reduction of <0.01s in most runs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OM3CTjNJXp_",
        "colab_type": "code",
        "outputId": "6e7725c6-08c5-411b-adfc-d7139528b9e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Average time\n",
        "time_original = 0;\n",
        "time_idea1 = 0;\n",
        "\n",
        "# Number of iterations\n",
        "n = 100;\n",
        "\n",
        "for i in range(n):\n",
        "  \n",
        "  # New neigbouring voxel response\n",
        "  Y_n = X*beta_n(beta_True)+Z*b_n(b_True)+cvxopt.normal(1000,1)\n",
        "  \n",
        "  # Z tranpose Y_n\n",
        "  ZtY_n=cvxopt.spmatrix.trans(Z)*Y_n\n",
        "\n",
        "  # X tranpose Y_n\n",
        "  XtY_n=cvxopt.matrix.trans(X)*Y_n\n",
        "\n",
        "  # Y_n tranpose X\n",
        "  YtX_n=cvxopt.matrix.trans(Y_n)*X\n",
        "\n",
        "  # Y_n transpose Z\n",
        "  YtZ_n=cvxopt.matrix.trans(Y_n)*Z\n",
        "\n",
        "  # Y_n tranpose Y_n\n",
        "  YtY_n=cvxopt.matrix.trans(Y_n)*Y_n\n",
        "  \n",
        "  # Obtain the voxels estimates independently\n",
        "  t1 = time.time()\n",
        "  theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_original = time_original + t2 - t1\n",
        "  \n",
        "  # Obtain the voxels estimates by reusing the estimate from one voxel as the initial estimate for the other\n",
        "  t1 = time.time()\n",
        "  theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  theta_Est_n=minimize(PLS, theta_Est, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_idea1 = time_idea1 + t2 - t1\n",
        "\n",
        "# Convert to averages\n",
        "time_original = time_original/n\n",
        "time_idea1 = time_idea1/n\n",
        "\n",
        "# Print\n",
        "print('Average Original estimation time:')\n",
        "print(time_original)\n",
        "print('Average Idea 1 estimation time:')\n",
        "print(time_idea1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Original estimation time:\n",
            "0.07912943363189698\n",
            "Average Idea 1 estimation time:\n",
            "0.07006490707397461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JP8KZC-Mamk",
        "colab_type": "text"
      },
      "source": [
        "### Idea 2: Penalize the difference between beta values in the objective function\n",
        "\n",
        "The PLS algorithm works by penalizing, at every voxel, the objective function:\n",
        "\n",
        "$$||Y-X\\beta+Z\\Lambda u+\\epsilon||^2_2 + ||u||^2_2$$\n",
        "\n",
        "Suppose, now that we wish to estimate the value at one voxel but already have beta values for one of it's neighbours. When the algorithm searches the parameter space, it stands to reason we want it to start by looking at beta values around the neighbouring voxels beta values. To do this we could add an additional term to the objective function:\n",
        "\n",
        "$$||Y-X\\beta+Z\\Lambda u+\\epsilon||^2_2 + ||u||^2_2 + ||\\beta - \\beta_n||^2_2$$\n",
        "\n",
        "Where $\\beta_n$ is the value of beta at the neighbouring voxel. This is the idea tested here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krn-dunmRTZl",
        "colab_type": "text"
      },
      "source": [
        "#### Return Beta from PLS\n",
        "\n",
        "This function returns the beta estimates for a given theta using PLS.\n",
        "\n",
        "---\n",
        "\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **theta**: The parameter estimate.\n",
        " - **ZtX**: Z transpose multiplied by X.\n",
        " - **ZtY**: Z transpose multiplied by Y.\n",
        " - **XtX**: X transpose multiplied by X.\n",
        " - **ZtZ**: Z transpose multiplied by Z.\n",
        " - **XtY**: X transpose multiplied by Y.\n",
        " - **YtX**: Y transpose multiplied by X.\n",
        " - **YtZ**: Y transpose multiplied by Z.\n",
        " - **XtZ**: X transpose multiplied by Z.\n",
        " - **YtY**: Y transpose multiplied by Y.\n",
        " - **P**: The sparse permutation for Lamda'Z'ZLambda+I\n",
        " - **tinds**: A vector specifying how many times each theta parameter should be repeated. For example, if theta=[0.1,0.8,0.3] and theta_inds=[1,1,1,2,3,3], then the values to be mapped into the sparse matrix would be [0.1,0.1,0.1,0.8,0.3,0.3].\n",
        " - **r_inds**: The row indices of the elements mapped into the sparse matrix.\n",
        " - **c_inds**: The column indices of the elements mapped into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEUPtjalRQpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PLS_getBeta(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, tinds, rinds, cinds):\n",
        "\n",
        "    # Obtain Lambda\n",
        "    Lambda = mapping(theta, tinds, rinds, cinds)\n",
        "    \n",
        "    # Obtain Lambda'\n",
        "    Lambdat = spmatrix.trans(Lambda)\n",
        "\n",
        "    # Obtain Lambda'Z'Y and Lambda'Z'X\n",
        "    LambdatZtY = Lambdat*ZtY\n",
        "    LambdatZtX = Lambdat*ZtX\n",
        "    \n",
        "    # Set the factorisation to use LL' instead of LDL'\n",
        "    cholmod.options['supernodal']=2\n",
        "\n",
        "    # Obtain the cholesky decomposition\n",
        "    LambdatZtZLambda = Lambdat*ZtZ*Lambda\n",
        "    I = spmatrix(1.0, range(Lambda.size[0]), range(Lambda.size[0]))\n",
        "    chol_dict = sparse_chol(LambdatZtZLambda+I, perm=P, retF=True, retP=False, retL=False)\n",
        "    F = chol_dict['F']\n",
        "\n",
        "    # Obtain C_u (annoyingly solve writes over the second argument,\n",
        "    # whereas spsolve outputs)\n",
        "    Cu = LambdatZtY[P,:]\n",
        "    cholmod.solve(F,Cu,sys=4)\n",
        "\n",
        "    # Obtain RZX\n",
        "    RZX = LambdatZtX[P,:]\n",
        "    cholmod.solve(F,RZX,sys=4)\n",
        "\n",
        "    # Obtain RXtRX\n",
        "    RXtRX = XtX - matrix.trans(RZX)*RZX\n",
        "\n",
        "    # Obtain beta estimates (note: gesv also replaces the second\n",
        "    # argument)\n",
        "    betahat = XtY - matrix.trans(RZX)*Cu\n",
        "    lapack.posv(RXtRX, betahat)\n",
        "\n",
        "    return(betahat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a7uYOZzPQ2W",
        "colab_type": "text"
      },
      "source": [
        "#### Neighbour-based PLS objective function\n",
        "\n",
        "The following function calculates the log likelihood based on the above objective function. It is the same as the original PLS algorithm in most respects, except it now also uses as input `beta_n`, the beta value from the run used on a neighbouring voxel. \n",
        "\n",
        "---\n",
        "\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **theta**: The parameter estimate.\n",
        " - **betan**: The beta estimate for a neighbouring voxel.\n",
        " - **ZtX**: Z transpose multiplied by X.\n",
        " - **ZtY**: Z transpose multiplied by Y.\n",
        " - **XtX**: X transpose multiplied by X.\n",
        " - **ZtZ**: Z transpose multiplied by Z.\n",
        " - **XtY**: X transpose multiplied by Y.\n",
        " - **YtX**: Y transpose multiplied by X.\n",
        " - **YtZ**: Y transpose multiplied by Z.\n",
        " - **XtZ**: X transpose multiplied by Z.\n",
        " - **YtY**: Y transpose multiplied by Y.\n",
        " - **P**: The sparse permutation for Lamda'Z'ZLambda+I\n",
        " - **tinds**: A vector specifying how many times each theta parameter should be repeated. For example, if theta=[0.1,0.8,0.3] and theta_inds=[1,1,1,2,3,3], then the values to be mapped into the sparse matrix would be [0.1,0.1,0.1,0.8,0.3,0.3].\n",
        " - **r_inds**: The row indices of the elements mapped into the sparse matrix.\n",
        " - **c_inds**: The column indices of the elements mapped into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E1CS6j7P1_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PLSneighbour(theta, betan, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds):\n",
        "\n",
        "    # Neighbour changes, the below is actually all that needs changing to account\n",
        "    # for the new objective function\n",
        "    XtX = XtX+ spmatrix(1.0, range(XtX.size[0]), range(XtX.size[0]))\n",
        "    XtY = XtY + betan\n",
        "    YtX = YtX + matrix.trans(betan)\n",
        "\n",
        "    # Return PLS on the newly adjusted data\n",
        "    return(PLS(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW9A8JM3SRW7",
        "colab_type": "text"
      },
      "source": [
        "#### Correctness\n",
        "\n",
        "To check that we are still estimating the same quantity, here we estimate the neighbour first and then use the beta estimate from the neighbouring voxel to estimate the original toy datasets theta. As can be seen the resultant estimate is still relatively close.\n",
        "\n",
        "Interestingly, the only beta estimate that has changed is the intercept, which `R` had trouble estimating anyway (it took me a while to find a toy dataset were `R` was able to correctly estimate the intercept... it could have just been due to chance/my own p-hacking that it happened in this case). Perhaps this is due to the model I used for this example, in which there is a fixed effects intercept and then an intercept for both grouping factors... this could just be a difficult/impossible model intercept to estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCsbTsTpSRtl",
        "colab_type": "code",
        "outputId": "7cbee541-caa1-4efd-96e4-a9bcec2e9f94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compute theta estimate for neighbouring voxel\n",
        "theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Retrieve beta\n",
        "beta_Est_n = PLS_getBeta(theta_Est_n,ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, tinds, rinds, cinds)\n",
        "\n",
        "# Minimise for the original voxel using the neighbour function\n",
        "theta_Est=minimize(PLSneighbour, theta0, args=(beta_Est_n, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Print results\n",
        "print(\"R estimated theta:\")\n",
        "print(theta_REst)\n",
        "print(\"Neighbourhood based estimated theta:\")\n",
        "print(theta_Est)\n",
        "\n",
        "# Print betas\n",
        "print(\"Beta (True)\")\n",
        "print(beta_True)\n",
        "print(\"Beta (R estimated)\")\n",
        "print(PLS_getBeta(theta_REst,ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P,tinds, rinds, cinds))\n",
        "print(\"Beta (Neighbourhood estimated)\")\n",
        "print(PLS_getBeta(theta_Est,ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P,tinds, rinds, cinds))\n",
        "\n",
        "# Double check we get the same as R\n",
        "#print(matrix(pd.read_csv('/Data/BLMM-testdata/estd_beta.csv',header=None).values))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R estimated theta:\n",
            "[ 1.11365482  0.32356815  2.22872418  4.54822071 -0.17500295  0.42370817]\n",
            "Neighbourhood based estimated theta:\n",
            "[ 0.2307727   0.09109081  0.53652211  1.12067893 -0.04111018  0.10321058]\n",
            "Beta (True)\n",
            "[ 1]\n",
            "[ 2]\n",
            "[ 3]\n",
            "\n",
            "Beta (R estimated)\n",
            "[ 1.05e+00]\n",
            "[ 1.98e+00]\n",
            "[ 3.03e+00]\n",
            "\n",
            "Beta (Neighbourhood estimated)\n",
            "[ 1.02e+00]\n",
            "[ 1.98e+00]\n",
            "[ 3.04e+00]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aANxmrO3WIgP",
        "colab_type": "text"
      },
      "source": [
        "To further investigate whether this is a consistent result, I have run the code 100 times with different a neighbouring voxel in each run and recorded the average squared difference between the true value of beta and the estimated value of beta for both the neighbour approach and the original PLS algorithm.\n",
        "\n",
        "**Conclusion**: The previous supposed loss of efficiency was indeed a fluke - after running 100 different neighbouring voxel iterations it has become extremely clear that the added penalty significantly improves accuracy in the toy example. Main points of interest:\n",
        "\n",
        " - The standard PLS algorithm works well for a majority of cases but rarely beats the neighbourhood version. \n",
        " - In some cases the PLS algorithm can massively go wrong estimating the intercept, whereas the neighbourhood method appears more robust to this.\n",
        " - Of course, this is heavily dependent on the simulation and would need more investigation for standard neuroimaging data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3iC0KRtWIqq",
        "colab_type": "code",
        "outputId": "83474160-7aa1-48c8-e87e-dcb7fc017912",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Average time\n",
        "diff_original = 0;\n",
        "diff_neighbour = 0;\n",
        "\n",
        "# Number of iterations\n",
        "n = 1;\n",
        "\n",
        "for i in range(n):\n",
        "  \n",
        "  # New neigbouring voxel response\n",
        "  beta_True_n = beta_n(beta_True)\n",
        "  Y_n = X*beta_True_n+Z*b_n(b_True)+cvxopt.normal(1000,1)\n",
        "  \n",
        "  #print(beta_True_n)\n",
        "  \n",
        "  # Z tranpose Y_n\n",
        "  ZtY_n=cvxopt.spmatrix.trans(Z)*Y_n\n",
        "\n",
        "  # X tranpose Y_n\n",
        "  XtY_n=cvxopt.matrix.trans(X)*Y_n\n",
        "\n",
        "  # Y_n tranpose X\n",
        "  YtX_n=cvxopt.matrix.trans(Y_n)*X\n",
        "\n",
        "  # Y_n transpose Z\n",
        "  YtZ_n=cvxopt.matrix.trans(Y_n)*Z\n",
        "\n",
        "  # Y_n tranpose Y_n\n",
        "  YtY_n=cvxopt.matrix.trans(Y_n)*Y_n\n",
        "  \n",
        "  # Obtain the voxels estimates independently\n",
        "  theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  \n",
        "  # Get the beta value from seperate calculation\n",
        "  beta_Est_n = PLS_getBeta(theta_Est_n,ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, tinds, rinds, cinds)\n",
        "  \n",
        "  #print(beta_Est_n)\n",
        "  \n",
        "  # Get average difference between estimated and truth\n",
        "  diff_original = np.linalg.norm(np.array(beta_Est_n-beta_True_n))**2 + diff_original\n",
        "  \n",
        "  #print(np.linalg.norm(np.array(beta_Est_n-beta_True_n))**2)\n",
        "  \n",
        "  # Obtain the voxels estimates by using the neighbouring approach\n",
        "  theta_Est= minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  beta_Est = PLS_getBeta(theta_Est,ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P,tinds, rinds, cinds)\n",
        "  theta_Est_n = minimize(PLSneighbour, theta0, args=(beta_Est, ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "  # Get the beta value from seperate calculation\n",
        "  beta_Est_n = PLS_getBeta(theta_Est_n,ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, tinds, rinds, cinds)\n",
        "  \n",
        "  #print(beta_Est_n)\n",
        "  #print(np.linalg.norm(np.array(beta_Est_n-beta_True_n))**2)\n",
        "  \n",
        "  # Get average difference between estimated and truth\n",
        "  diff_neighbour = np.linalg.norm(np.array(beta_Est_n-beta_True_n))**2 + diff_neighbour\n",
        "\n",
        "  #print('')\n",
        "# Convert to averages\n",
        "diff_original = diff_original/n\n",
        "diff_neighbour = diff_neighbour/n\n",
        "\n",
        "# Print\n",
        "print('Average Original squared difference from truth:')\n",
        "print(diff_original)\n",
        "print('Average Idea 2 squared difference from truth:')\n",
        "print(diff_neighbour)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Original squared difference from truth:\n",
            "7.086544577528563\n",
            "Average Idea 2 squared difference from truth:\n",
            "7.086544580078364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAdjGFGxR_3w",
        "colab_type": "text"
      },
      "source": [
        "#### Time efficiency\n",
        "\n",
        "What is important here is the time taken to do this for both voxels. In the below, the two voxels are estimated 100 times twice, once reusing the estmate from the first voxel in estimating the second, and once computing the voxels completely seperately.\n",
        "\n",
        "**Conclusion:** Some significant time improvement was observed when using `Powell` (around 0.07s quicker) but not for `L-BFGS-B`. A more extensive test suite is needed. Also get singular matrices occasionally causing errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8FROJLxR__g",
        "colab_type": "code",
        "outputId": "b9be4da3-73aa-44d8-aa2a-5f3481440025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Average time\n",
        "time_original = 0;\n",
        "time_idea2 = 0;\n",
        "\n",
        "# Number of iterations\n",
        "n = 100;\n",
        "failureno=0;\n",
        "\n",
        "for i in range(n):\n",
        "  \n",
        "  # New neigbouring voxel response\n",
        "  Y_n = X*beta_n(beta_True)+Z*b_n(b_True)+cvxopt.normal(1000,1)\n",
        "  \n",
        "  # Z tranpose Y_n\n",
        "  ZtY_n=cvxopt.spmatrix.trans(Z)*Y_n\n",
        "\n",
        "  # X tranpose Y_n\n",
        "  XtY_n=cvxopt.matrix.trans(X)*Y_n\n",
        "\n",
        "  # Y_n tranpose X\n",
        "  YtX_n=cvxopt.matrix.trans(Y_n)*X\n",
        "\n",
        "  # Y_n transpose Z\n",
        "  YtZ_n=cvxopt.matrix.trans(Y_n)*Z\n",
        "\n",
        "  # Y_n tranpose Y_n\n",
        "  YtY_n=cvxopt.matrix.trans(Y_n)*Y_n\n",
        "  \n",
        "  # Obtain the voxels estimates independently\n",
        "  t1 = time.time()\n",
        "  theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n,P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_original = time_original + t2 - t1\n",
        "  \n",
        "  # Obtain the voxels estimates by using the neighbouring approach\n",
        "  t1 = time.time()\n",
        "  theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  beta_Est_n = PLS_getBeta(theta_Est_n,ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, n, P, tinds, rinds, cinds)\n",
        "  try:\n",
        "    theta_Est=minimize(PLSneighbour, theta0, args=(beta_Est_n, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "    t2 = time.time()\n",
        "    time_idea2 = time_idea2 + t2 - t1\n",
        "  except:\n",
        "    print(\"Voxel that failed, number:\")\n",
        "    print(i)\n",
        "    failureno = failureno + 1\n",
        "\n",
        "# Convert to averages\n",
        "time_original = time_original/n\n",
        "time_idea2 = time_idea2/n\n",
        "\n",
        "# Print\n",
        "print('Average Original estimation time:')\n",
        "print(time_original)\n",
        "print('Average Idea 2 estimation time:')\n",
        "print(time_idea2)\n",
        "\n",
        "print('Number of Failures')\n",
        "print(failureno)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Voxel that failed, number:\n",
            "3\n",
            "Voxel that failed, number:\n",
            "30\n",
            "Average Original estimation time:\n",
            "0.07740530729293824\n",
            "Average Idea 2 estimation time:\n",
            "0.0791367793083191\n",
            "Number of Failures\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBr0APoAXKl3",
        "colab_type": "text"
      },
      "source": [
        "## Scaling up the computation (Random field)\n",
        "\n",
        "This section has several implemented ideas for scaling up the computation to compute several similar models at once. For simplicity it is assumed here that X and Z are the same across voxels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5XoW6q2NaJ20"
      },
      "source": [
        "### Toy dataset (for a random field)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LTG_EYiZpjM",
        "colab_type": "text"
      },
      "source": [
        "#### Matrix Dimensions\n",
        "\n",
        "Below are the matrix dimensions used for **one voxel** in this example. If the model has form:\n",
        "\n",
        "$$Y=X\\beta+Zb+\\epsilon$$ With $\\epsilon \\sim N(0,\\sigma^2I_n)$ and $b \\sim N(0,\\sigma^2D)$, then the dimensions of each matrix are as follows:\n",
        "\n",
        " - $Y$: $(n \\times 1)$\n",
        " - $X$: $(n \\times p)$\n",
        " - $\\beta$: $(p \\times 1)$\n",
        " - $Z$: $(n \\times q)$\n",
        " - $b$: $(q \\times 1)$\n",
        " - $\\epsilon$: $(n \\times 1)$\n",
        " - $D$: $(p\\times p)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LAfqqiCXO4k",
        "colab_type": "code",
        "outputId": "91351616-ba5c-49eb-e308-ba798e86a3e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "# Number of factors, random integer between 1 and 3\n",
        "r = 2#np.random.randint(2,4)#np.random.randint(1,4)\n",
        "print(\"Number of grouping factors for random effects:\")\n",
        "print(r)\n",
        "\n",
        "# Number of levels, random number between 2 and 8\n",
        "nlevels = np.random.randint(2,8,size=(r))\n",
        "# Let the first number of levels be a little larger (typically like subjects)\n",
        "nlevels[0] = np.random.randint(2,35,size=1)\n",
        "nlevels = np.array([30,10])#np.sort(nlevels)[::-1]\n",
        "print(\"Number of levels for each factor:\")\n",
        "print(nlevels)\n",
        "\n",
        "# Number of parameters, random number between 1 and 5\n",
        "nparams = np.array([3,1])#np.random.randint(1,6,size=(r))\n",
        "print(\"Number of parameters for each factor:\")\n",
        "print(nparams)\n",
        "\n",
        "# Dimension of D\n",
        "print(\"Dimension of D, q:\")\n",
        "q = np.sum(nlevels*nparams)\n",
        "print(q)\n",
        "\n",
        "# Number of fixed effects, random number between 6 and 30\n",
        "p = 5#np.random.randint(6,31)\n",
        "print(\"Number of fixed effects:\")\n",
        "print(p)\n",
        "\n",
        "# Number of subjects, n\n",
        "n = 1000\n",
        "print(\"Number of subjects:\")\n",
        "print(n)\n",
        "\n",
        "# Voxel dimensions\n",
        "dimv = [20,20,20]\n",
        "nv = np.prod(dimv)\n",
        "print(\"Number of voxels:\")\n",
        "print(nv)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of grouping factors for random effects:\n",
            "2\n",
            "Number of levels for each factor:\n",
            "[30 10]\n",
            "Number of parameters for each factor:\n",
            "[3 1]\n",
            "Dimension of D, q:\n",
            "100\n",
            "Number of fixed effects:\n",
            "5\n",
            "Number of subjects:\n",
            "1000\n",
            "Number of voxels:\n",
            "8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57CJIErlbDS5",
        "colab_type": "text"
      },
      "source": [
        "#### Fixed Effects matrix (X)\n",
        "\n",
        "For simplicity, in this example $X$ is the same for all voxels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wnJHsOxdUne",
        "colab_type": "code",
        "outputId": "3b5d9bfb-827a-4ddb-97ae-4648d8c6c904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Initialize empty x\n",
        "X = np.zeros((n,p))\n",
        "\n",
        "# First column is intercept\n",
        "X[:,0] = 1\n",
        "\n",
        "# Rest of the columns we will make random noise \n",
        "X[:,1:] = np.random.randn(n*(p-1)).reshape((n,(p-1)))\n",
        "\n",
        "# Image of the last 20 rows of X\n",
        "imshow(X[-20:-1,:])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb7bd037828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAD4CAYAAAAaRkNsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6Ysdq0ZHUS4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaBzkwl0ekWr",
        "colab_type": "text"
      },
      "source": [
        "#### Random Effects matrix (Z)\n",
        "\n",
        "For simplicity, in this example $Z$ is the same for all voxels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb1S4WdhepI9",
        "colab_type": "code",
        "outputId": "2150c24a-8f67-4a8e-a952-04955bce21e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "# We need to create a block of Z for each level of each factor\n",
        "for i in np.arange(r):\n",
        "  \n",
        "  Zdata_factor = np.random.randn(n,nparams[i])\n",
        "  \n",
        "  if i==0:\n",
        "    \n",
        "    #The first factor should be block diagonal, so the factor indices are grouped\n",
        "    factorVec = np.repeat(np.arange(nlevels[i]), repeats=np.floor(n/max(nlevels[i],1)))\n",
        "    \n",
        "    if len(factorVec) < n:\n",
        "      \n",
        "      # Quick fix incase rounding leaves empty columns\n",
        "      factorVecTmp = np.zeros(n)\n",
        "      factorVecTmp[0:len(factorVec)] = factorVec\n",
        "      factorVecTmp[len(factorVec):n] = nlevels[i]-1\n",
        "      factorVec = np.int64(factorVecTmp)\n",
        "      \n",
        "    \n",
        "    # Crop the factor vector - otherwise have a few too many\n",
        "    factorVec = factorVec[0:n]\n",
        "    \n",
        "    # Give the data an intercept\n",
        "    Zdata_factor[:,0]=1\n",
        "    \n",
        "  else:\n",
        "    \n",
        "    # The factor is randomly arranged across subjects\n",
        "    factorVec = np.random.randint(0,nlevels[i],size=n) \n",
        "  \n",
        "  # Build a matrix showing where the elements of Z should be\n",
        "  indicatorMatrix_factor = np.zeros((n,nlevels[i]))\n",
        "  indicatorMatrix_factor[np.arange(n),factorVec] = 1\n",
        "  \n",
        "  # Need to repeat for each parameter the factor has \n",
        "  indicatorMatrix_factor = np.repeat(indicatorMatrix_factor, nparams[i], axis=1)\n",
        "  \n",
        "  # Enter the Z values\n",
        "  indicatorMatrix_factor[indicatorMatrix_factor==1]=Zdata_factor.reshape(Zdata_factor.shape[0]*Zdata_factor.shape[1])\n",
        "  \n",
        "  # Make sparse\n",
        "  Zfactor = scipy.sparse.csr_matrix(indicatorMatrix_factor)\n",
        "\n",
        "  # Put all the factors together\n",
        "  if i == 0:\n",
        "    Z = Zfactor\n",
        "  else:\n",
        "    Z = scipy.sparse.hstack((Z, Zfactor))\n",
        "\n",
        "\n",
        "Z2 = sparse.COO.from_scipy_sparse(Z)\n",
        "\n",
        "# Create an image of Z\n",
        "imshow(Z.toarray(), \\\n",
        "       interpolation='nearest', vmin=-5, vmax=5, aspect='auto')\n",
        "\n",
        "print(nlevels)\n",
        "print(nparams)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[30 10]\n",
            "[3 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nImpADHciogbEcCciakAMdyKiBvT/AR7rrA++cW/KAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTqsJgFrYP85",
        "colab_type": "text"
      },
      "source": [
        "#### Smooth random beta\n",
        "Smooth random beta image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUeftuE-YQjP",
        "colab_type": "code",
        "outputId": "178b95f4-f236-47a6-8e21-8bdbb217283b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# Random 4D matrix (unsmoothed)\n",
        "beta_us = np.random.randn(nv*p).reshape(dimv[0],dimv[1],dimv[2],p)*20\n",
        "beta_us[10:15,10:15,10:15,3] = beta_us[10:15,10:15,10:15,3] + 100\n",
        "\n",
        "t1 = time.time()\n",
        "# Some random affine, not important for this simulation\n",
        "affine = np.diag([1, 1, 1, 1])\n",
        "beta_us_nii = nib.Nifti1Image(beta_us, affine)\n",
        "\n",
        "# Smoothed beta nifti\n",
        "beta_s_nii = nilearn.image.smooth_img(beta_us_nii, 5)\n",
        "\n",
        "# Final beta\n",
        "beta = beta_s_nii.get_fdata()\n",
        "\n",
        "t2 = time.time()\n",
        "print(t2-t1)\n",
        "\n",
        "# Show unsmoothed\n",
        "imshow(beta_us[10,:,:,3].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')\n",
        "\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.004777193069458008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7fb7bcf9b978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD4CAYAAAAjKGdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDuTgbWbWgf4/gf8nqjWAB0oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOs4ge-_zgIr",
        "colab_type": "code",
        "outputId": "449dc31d-2361-4f51-9c19-ecb75fa9146b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Show smoothed\n",
        "imshow(beta[10,:,:,3].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7fb7bced00b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAD4CAYAAADfPUyRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZv4/nyR5d6H54m8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vabjCMt0eMq",
        "colab_type": "text"
      },
      "source": [
        "#### Smooth random b\n",
        "Smooth random b image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bSmo7Af0ecp",
        "colab_type": "code",
        "outputId": "98d41f4f-b47b-4543-eddb-dedc3f88dc78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Random 4D matrix (unsmoothed)\n",
        "b_us = np.random.randn(nv*q).reshape(dimv[0],dimv[1],dimv[2],q)*20\n",
        "\n",
        "# Some random affine, not important for this simulation\n",
        "affine = np.diag([1, 1, 1, 1])\n",
        "b_us_nii = nib.Nifti1Image(b_us, affine)\n",
        "\n",
        "# Smoothed beta nifti\n",
        "b_s_nii = nilearn.image.smooth_img(b_us_nii, 5)\n",
        "\n",
        "# Final beta\n",
        "b = b_s_nii.get_fdata()\n",
        "\n",
        "# Show unsmoothed\n",
        "imshow(b_us[3,:,:,1].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb7bce463c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAbE0lEQVR4nO3df5BddZnn8fenfyQhnSbkB4T8ghAI\nDMhCwBhBUEEQIcWCzrIurDuAwkZUdqVm3Fl2rEXXqZrVtXS2FFYmCgtYiBSjQcqJQAanJsAaIMQQ\ngkASYiBpkjQkIb9/dfezf/SJ0zb3Jjf93HQ3pz6vqq4+95zv9zlPnz557snp+/0eRQRmZlZeDQOd\ngJmZHV4u9GZmJedCb2ZWci70ZmYl50JvZlZyTQOdQCVNw1uieeToVIxIvoVFY65/d5B8iMbddUgj\neSy6WrrSOTTsyF9TdA1Nh6B5ezKHepwXddDZWoeTq0Op7g1DOtMpxO78AY2mOhyLSB6L5vyxYFvu\nWOzbuomOXTsq/iCDstA3jxzNCdf/eSpG5xG5HPaMyhe3xj25kwdg5KvpEOwbkctj+wd2pXMYsSj5\nCwG2nZD/nUxYkCsKu0fX4T/B+R+DTR/dkw/yTnOqe+txW9Mp7Hr5qHSMfUfvS8fQnlyRbZmwLZ1D\nwz/njsXK+79bPXYqspmZDXqpQi/pUkmvSlop6dYK24dKerDY/oykKZn9mZnZoetzoZfUCNwBXAac\nBlwj6bRezW4ANkfEScDfAt/q6/7MzKxvMlf0M4GVEbEqIvYCPwWu7NXmSuDeYvnvgYsk5W9cm5lZ\nzTKFfiKwpsfrtcW6im0iogPYAoypFEzSbEmLJC3q2LkjkZaZmfU0aP4YGxFzImJGRMxoGt4y0OmY\nmZVGptC3AZN7vJ5UrKvYRlITMBLYmNinmZkdokyhfw6YJukESUOAq4FHerV5BLiuWL4K+HV4XmQz\ns37V5wFTEdEh6WbgMaARuDsiXpL0DWBRRDwC3AX8WNJKYBPdbwZmZtaPUiNjI2IeMK/Xutt6LO8G\n/u2hxu1qhD2jcxf+nclh+6N/m//zxY7J+Q8Yta7dm44x85uLUv2f+p/npHN482Md6RhjnssPl2+7\nOHdejJq4KZ3D1FH5u5ebFp+UjnHi6b3vtB6aLfdNSucw6bFV6Rgrvjw1HWPfqNwUBp+dtjCdwx07\nPprq3/Vw9XN70Pwx1szMDg8XejOzknOhNzMrORd6M7OSc6E3Mys5F3ozs5JzoTczKzkXejOzknOh\nNzMrORd6M7OSG5QPB2/cAyNX5GLsGZ182G97fsj+xvPyT4FePS7/Xrz3bz6Y6r/lxHwOJ9+zMx1j\n+Y1D0jGGv5aLse/3FR+ncEiWteRjxHH5h4OPHJJ76PvqU9MpsPfIE9MxOiflH15/8oS3Uv3vvevS\ndA5dJydrTmf1KVd8RW9mVnIu9GZmJedCb2ZWci70ZmYl50JvZlZyLvRmZiXnQm9mVnJ9LvSSJkv6\nJ0m/k/SSpC9XaHOBpC2SlhRft1WKZWZmh09mwFQH8BcRsVhSK/C8pPkR8bte7Z6MiMsT+zEzs4Q+\nX9FHxLqIWFwsbwNeBibWKzEzM6uPukyBIGkKcBbwTIXN50p6AXgT+EpEvFQlxmxgNsCQllE07YpU\nTiNeyg0nfuOS3BQKAMdPak/HePuJCekY25Nvv7uOyU/lsP6DI9IxTj7hjXSMFTtzB2Po+PxUDs3P\ntqZjnHvq8nSMxesnpfoflU+BzRfnj+eQV4enY/zN+T9P9b/pov+QzoG3kufFAS7b03+MlTQC+Blw\nS0Rs7bV5MXB8RJwJfB94uFqciJgTETMiYkbT0JZsWmZmVkgVeknNdBf5+yPiXW+JEbE1IrYXy/OA\nZkljM/s0M7NDk/nUjYC7gJcj4rtV2hxbtEPSzGJ/G/u6TzMzO3SZe/TnAX8GvChpSbHur4DjACLi\nTuAq4AuSOoBdwNURkbv5bmZmh6TPhT4ingKqT4Dc3eZ24Pa+7sPMzPI8MtbMrORc6M3MSs6F3sys\n5FzozcxKzoXezKzk6jIFQr11tAYbPpabwmDIuuZU/2jdl+oPsPatUekYGp3/NOqJH8hNHbD28ePT\nOYx+NX882+cel44RZ+9N9T/2rmHpHIa/vCYd46mzTkzHaFqemzpgaGf+3OzcmS9BTY35PP79szem\n+o/9WX4aBp2XDNBZ/UOQvqI3Mys5F3ozs5JzoTczKzkXejOzknOhNzMrORd6M7OSc6E3Mys5F3oz\ns5JzoTczK7lBOTK2cYcYszA3snXjWZ2p/kPXDEn1B2hpy4/Y+/KfP5SO8e2XL0n1n/Bk/gHOM+94\nPh3joVfPSsc44sXcQ8rb8ylwxMTcQ7kBmpYf8FEQNRm+Lnd+bjwzf34PG7knHWPPkPzD6z96/O9T\n/ReP/1fpHGjI1awDhj5skc3MbFBwoTczK7l0oZe0WtKLkpZIWlRhuyR9T9JKSUslnZ3dp5mZ1a5e\n9+gvjIi3q2y7DJhWfH0Q+EHx3czM+kF/3Lq5Ergvui0EjpI0vh/2a2Zm1KfQB/C4pOclza6wfSLQ\ncwLutcW6PyJptqRFkhZ17N5Rh7TMzAzqc+vm/Ihok3QMMF/SKxGx4FCDRMQcYA7A8KMn5z+3ZWZm\nQB2u6COirfjeDswFZvZq0gZM7vF6UrHOzMz6QarQS2qR1Lp/GbgEWNar2SPAtcWnb84BtkTEusx+\nzcysdtlbN+OAuZL2x/pJRDwq6SaAiLgTmAfMAlYCO4HPJvdpZmaHIFXoI2IVcGaF9Xf2WA7gS4cS\nt3N4sPH9ueHAJ92fexj167MaU/0BVIe/NHz9savSMaaf9Vqq//rJ+QdR/2TpB9Ixhr6WfzB36xu5\nX0rL+vxDzrdNyk3vAdB5Sv4DC8f+302p/o2781M5vHXU0HSMU6bkbxA8vfqEVP8pi/LThFx87dJU\n/5+MqH5OeGSsmVnJudCbmZWcC72ZWcm50JuZlZwLvZlZybnQm5mVnAu9mVnJudCbmZWcC72ZWcm5\n0JuZlVy9njBVX41BQ0tuqPm6845I9Z/ySH6I+d7/sSUdY+eTE9Ix1v+f3BQGez6TGyoPoNdGp2N0\nDs3PKfHWx/am+m8e2pHOoWH5kHQMVg9Ph/j9tS2p/rum7knn0HxEfkqJFUsnH7zRwXQlc7gx/3Ps\n25T7OXZ2Vj+vfEVvZlZyLvRmZiXnQm9mVnIu9GZmJedCb2ZWci70ZmYl50JvZlZyfS70kk6RtKTH\n11ZJt/Rqc4GkLT3a3JZP2czMDkWfB0xFxKvAdABJjUAbMLdC0ycj4vK+7sfMzHLqdevmIuC1iHi9\nTvHMzKxO6jUFwtXAA1W2nSvpBeBN4CsR8VKlRpJmA7MBho1r5aQJb6USWrkx94T61V/OD7fXs+PT\nMY5cm89j+PrcsP+G5vyw/+3blI4RzekQxN7ctY3W5KcemPLLbekYr/2bEekYw9cnz62uoekcmnbn\nY9TDUStz5/juUflpLV4/+9hU/727qv8DSV/RSxoCXAE8VGHzYuD4iDgT+D7wcLU4ETEnImZExIzm\nkbl5aszM7F/U49bNZcDiiNjQe0NEbI2I7cXyPKBZ0tg67NPMzGpUj0J/DVVu20g6VpKK5ZnF/jbW\nYZ9mZlaj1D16SS3Ax4HP91h3E0BE3AlcBXxBUgewC7g6IvI3nc3MrGapQh8RO4Axvdbd2WP5duD2\nzD7MzCzHI2PNzErOhd7MrORc6M3MSs6F3sys5FzozcxKrl5TINTVnn1NvLb+6FSMrmG5x7qP+VVL\nqj/ApBtWpmMsmZx/wv3WE3PDzE85Ymc6h4aZ69Mxts/NDREH2N6VO+WH1GEqh+XX5adR+LMPL0jH\n+PE/n5/q37Avfyz2jd+djnHMw8PSMd46M3dedA6rw1QlbY2p/g37DrAtFdnMzAY9F3ozs5JzoTcz\nKzkXejOzknOhNzMrORd6M7OSc6E3Mys5F3ozs5JzoTczKzkXejOzkhuUUyA07mhgxMLcA8K7Ltqc\n6j/22dxT4QHe2jo1HaPx3Px7cUtbrv/Kx/M/R+cR+SHiU5bsSMcY/4v2VP9Vs/PH4piF+akDHhj7\n/nSM7JD7pl3pFNg6LDc9B8CFf/V0OsaT//3cVP/NN2xL57B97ZGp/l1Dqm/zFb2ZWcnVVOgl3S2p\nXdKyHutGS5ovaUXxfVSVvtcVbVZIuq5eiZuZWW1qvaK/B7i017pbgSciYhrwRPH6j0gaDXwN+CAw\nE/hatTcEMzM7PGoq9BGxANjUa/WVwL3F8r3AJyt0/QQwPyI2RcRmYD7vfsMwM7PDKHOPflxErCuW\n1wPjKrSZCKzp8Xptsc7MzPpJXf4YGxEBpD5WIWm2pEWSFnXsyn+6wszMumUK/QZJ4wGK75U+t9YG\n9HxE0qRi3btExJyImBERM5qOyD/dyczMumUK/SPA/k/RXAf8okKbx4BLJI0q/gh7SbHOzMz6Sa0f\nr3wA+A1wiqS1km4Avgl8XNIK4OLiNZJmSPoRQERsAv4aeK74+kaxzszM+klNI2Mj4poqmy6q0HYR\ncGOP13cDd/cpOzMzSxuUUyB0DoWt0zpTMaa15oYkz3vil6n+AO/7/hfTMcacnhuyD9C8cEyq/zun\npVPg4x9cmo4x/8gz0jEm/fq4VP9xz+1L57Dm47mpBwBanx6RjrFzYm5aij2j0ykw8nf5Y/GTjvPS\nMVpOzX0u5Yjm/JQp2xqz04RU7+8pEMzMSs6F3sys5FzozcxKzoXezKzkXOjNzErOhd7MrORc6M3M\nSs6F3sys5FzozcxKzoXezKzkBuUUCE07YNxC5YK8L9f9ghv/Yy4A0PWBdAg6fnZ0OsbbuQfcE825\n6SgAlt92ejrGsBn54fJT/uuygzc6gKeeOzWdw6hlyXMbGPZOVzrGljNyv9fmt/Plox7TKIyamp8n\ncfTpu1L9X2/P/yAtr+eOZ8Pe6ueVr+jNzErOhd7MrORc6M3MSs6F3sys5FzozcxKzoXezKzkXOjN\nzEruoIVe0t2S2iUt67Hu25JekbRU0lxJR1Xpu1rSi5KWSFpUz8TNzKw2tVzR3wNc2mvdfOD0iDgD\nWA78twP0vzAipkfEjL6laGZmGQct9BGxANjUa93jEbH/abgLgUmHITczM6uDekyB8DngwSrbAnhc\nUgB/FxFzqgWRNBuYDTDi2BbOuOWFVFK/++YZqf5tn84/1Z2G3ekQn7hiaTrG03fk5mJofSM/BcL6\nc4amY5z2ieXpGE8vPC3Vf8y0/HD7TV354fLT/svidIxjhr4/1b/93Px50bwtX4L08Jh0jG17ItV/\n34fyU1JMv/zVVP91P69eb1JHWdJXgQ7g/ipNzo+INknHAPMlvVL8D+FdijeBOQBHnzYmd9TNzOwP\n+vypG0nXA5cDn4mIioU5ItqK7+3AXGBmX/dnZmZ906dCL+lS4C+BKyJiZ5U2LZJa9y8DlwC5qQPN\nzOyQ1fLxygeA3wCnSFor6QbgdqCV7tsxSyTdWbSdIGle0XUc8JSkF4BngX+IiEcPy09hZmZVHfQe\nfURcU2H1XVXavgnMKpZXAWemsjMzszSPjDUzKzkXejOzknOhNzMrORd6M7OSc6E3Myu5ekyBUHc7\n9g1h0YbJqRgd125N9W/YmR+yrzeOSMdoP7E1HePtD+SGqreurf50+VoN3ZwOwRt3TUvHaPiTXP/N\nL+eH23eN3ZuO0fnouHSMt5fmBqCPWJUvHxNnvZ6OsXFnSzpG6zeHp/rvG57/t/7akpNT/fdsHFZ1\nm6/ozcxKzoXezKzkXOjNzErOhd7MrORc6M3MSs6F3sys5FzozcxKzoXezKzkXOjNzEpuUI6M7exs\nYMvW3Gi3lkW5kWonPZkbWQtw5P/OP8z6udePT8eY+ERuZOvr1+YfAj36ifxjgP/k8y+lYzQql8f/\nm396Oodxv2pOx9izd3w6RsvU3HXejuPyD8Te19WYjrF5S35kbMf0XL0YviH/b6R9Ru730XmAwfy+\nojczKzkXejOzkqvlmbF3S2qXtKzHuq9LaiueF7tE0qwqfS+V9KqklZJurWfiZmZWm1qu6O8BLq2w\n/m8jYnrxNa/3RkmNwB3AZcBpwDWSTsska2Zmh+6ghT4iFgCb+hB7JrAyIlZFxF7gp8CVfYhjZmYJ\nmXv0N0taWtzaGVVh+0RgTY/Xa4t1FUmaLWmRpEWd23Yk0jIzs576Wuh/AJwITAfWAd/JJhIRcyJi\nRkTMaGzNf1zKzMy69anQR8SGiOiMiC7gh3TfpumtDej5mKhJxTozM+tHfSr0knqO1vgUsKxCs+eA\naZJOkDQEuBp4pC/7MzOzvjvoyFhJDwAXAGMlrQW+BlwgaToQwGrg80XbCcCPImJWRHRIuhl4DGgE\n7o6I/NBGMzM7JAct9BFxTYXVd1Vp+yYwq8frecC7Pnp5MA07Gmh5Njckeeg7uaHuv78y/1DuKV/J\nj0ebcPSQdIwtJ+SGmbf8Nv/g4zFLtqRjPP9m7oHxALveHJHqf2R7/kHpI5dtTMeYfM+agzc6iAtH\nvpLqf8etn07nsOrIY9Ixxjydn1Ji+/G5erFnVH4qh47WjlyAxuo/g0fGmpmVnAu9mVnJudCbmZWc\nC72ZWcm50JuZlZwLvZlZybnQm5mVnAu9mVnJudCbmZWcC72ZWckddAqEAdHaScdHckPmf/r+H6b6\n33TzLan+AKH8cPl3puZ/RVun78kF2JMf3r3q6CPTMRrqMFNSjO1M9d+b/zFo/9CYdIydXxuZjvHk\njLNS/U+6ZVU6h7alx6dj7P3X76RjDE32f+isH6Vz+OJ1/ynVf/Pm6tt8RW9mVnIu9GZmJedCb2ZW\nci70ZmYl50JvZlZyLvRmZiXnQm9mVnK1PDP2buByoD0iTi/WPQicUjQ5CngnIqZX6Lsa2AZ0Ah0R\nMaNOeZuZWY1qGY1zD3A7cN/+FRHx7/YvS/oOcKDRTRdGxNt9TdDMzHJqeTj4AklTKm2TJODTwMfq\nm5aZmdVLdnz9h4ENEbGiyvYAHpcUwN9FxJxqgSTNBmYDNI0cRdeLuSHen+y4KdV/7+Wp7gC0Hrs3\nHWP8J19Ix2j40odS/XdOqP50+VqNPrs9HWPrgnHpGE3bctM5HLN4XzqHtRflp5RQZ35qjKGbcv1X\n/uPUdA4a3ZWOse+3o9IxRqzJneOX7/5COofWv9ie6t9xS/XpPbJnyzXAAwfYfn5EtEk6Bpgv6ZWI\nWFCpYfEmMAdg2MTJ+cpiZmZA4lM3kpqAPwUerNYmItqK7+3AXGBmX/dnZmZ9k/l45cXAKxGxttJG\nSS2SWvcvA5cAyxL7MzOzPjhooZf0APAb4BRJayXdUGy6ml63bSRNkDSveDkOeErSC8CzwD9ExKP1\nS93MzGpRy6durqmy/voK694EZhXLq4Azk/mZmVmSR8aamZWcC72ZWcm50JuZlZwLvZlZybnQm5mV\nXH4c9WEQTcHucR2pGEctGJHqP+6Kdan+AGvaxqRjbL/v7HSMsb/ODTT+z5/6ZTqH783Nzykx8fn8\nlBIb3zck1f+dac3pHJqO25aOsfHofB5nTGlL9V/x6InpHE496/V0jJULpqRjbJ+sVP/rT3smncMD\nP74o1T+2VS/nvqI3Mys5F3ozs5JzoTczKzkXejOzknOhNzMrORd6M7OSc6E3Mys5F3ozs5JzoTcz\nKzkXejOzklPE4HsOt6S3gAONjR4LvN1P6WQ4z/p5L+QIzrPenGftjo+IoyttGJSF/mAkLYqIGQOd\nx8E4z/p5L+QIzrPenGd9+NaNmVnJudCbmZXce7XQzxnoBGrkPOvnvZAjOM96c5518J68R29mZrV7\nr17Rm5lZjVzozcxKblAXekmXSnpV0kpJt1bYPlTSg8X2ZyRNGYAcJ0v6J0m/k/SSpC9XaHOBpC2S\nlhRft/V3nkUeqyW9WOSwqMJ2SfpecTyXSso/x/DQ8julxzFaImmrpFt6tRmQYynpbkntkpb1WDda\n0nxJK4rvo6r0va5os0LSdQOQ57clvVL8TudKOqpK3wOeH/2Q59cltfX43c6q0veAdaEf8nywR46r\nJS2p0rffjudBRcSg/AIagdeAqcAQ4AXgtF5tvgjcWSxfDTw4AHmOB84ulluB5RXyvAD45SA4pquB\nsQfYPgv4FSDgHOCZAf79r6d7EMiAH0vgI8DZwLIe6/4XcGuxfCvwrQr9RgOriu+jiuVR/ZznJUBT\nsfytSnnWcn70Q55fB75Sw3lxwLpwuPPstf07wG0DfTwP9jWYr+hnAisjYlVE7AV+ClzZq82VwL3F\n8t8DF0nKPeX3EEXEuohYXCxvA14GJvZnDnV0JXBfdFsIHCVp/ADlchHwWkTknx5dBxGxANjUa3XP\n8+9e4JMVun4CmB8RmyJiMzAfuLQ/84yIxyOio3i5EJh0uPZfqyrHsxa11IW6OVCeRa35NPDA4dp/\nvQzmQj8RWNPj9VreXUD/0KY4kbcAY/oluwqKW0dnAZUeCX+upBck/UrS+/o1sX8RwOOSnpc0u8L2\nWo55f7ma6v+ABsOxBBgXEeuK5fXAuAptBtMxBfgc3f9rq+Rg50d/uLm4xXR3lVthg+l4fhjYEBEr\nqmwfDMcTGNyF/j1F0gjgZ8AtEbG11+bFdN+COBP4PvBwf+dXOD8izgYuA74k6SMDlMcBSRoCXAE8\nVGHzYDmWfyS6/68+qD+rLOmrQAdwf5UmA31+/AA4EZgOrKP7tshgdg0Hvpof6OP5B4O50LcBk3u8\nnlSsq9hGUhMwEtjYL9n1IKmZ7iJ/f0T8vPf2iNgaEduL5XlAs6Sx/ZwmEdFWfG8H5tL93+Ceajnm\n/eEyYHFEbOi9YbAcy8KG/be2iu/tFdoMimMq6XrgcuAzxZvSu9RwfhxWEbEhIjojogv4YZX9D5bj\n2QT8KfBgtTYDfTx7GsyF/jlgmqQTiiu8q4FHerV5BNj/KYargF9XO4kPl+I+3V3AyxHx3Sptjt3/\ntwNJM+k+7v36hiSpRVLr/mW6/0C3rFezR4Bri0/fnANs6XFroj9VvVIaDMeyh57n33XALyq0eQy4\nRNKo4lbEJcW6fiPpUuAvgSsiYmeVNrWcH4dVr78HfarK/mupC/3hYuCViFhbaeNgOJ5/ZKD/Gnyg\nL7o/BbKc7r+yf7VY9w26T1iAYXT/934l8CwwdQByPJ/u/7IvBZYUX7OAm4CbijY3Ay/R/QmBhcCH\nBiDPqcX+Xyhy2X88e+Yp4I7ieL8IzBiAPFvoLtwje6wb8GNJ9xvPOmAf3feFb6D770FPACuAfwRG\nF21nAD/q0fdzxTm6EvjsAOS5ku772vvPz/2fVJsAzDvQ+dHPef64OO+W0l28x/fOs3j9rrrQn3kW\n6+/Zf072aDtgx/NgX54Cwcys5AbzrRszM6sDF3ozs5JzoTczKzkXejOzknOhNzMrORd6M7OSc6E3\nMyu5/w9E8WVUjLJN8wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXEPETmY0w4z",
        "colab_type": "code",
        "outputId": "fe806f8b-6527-491f-d06f-1988ed97fe21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Show smoothed\n",
        "imshow(b[3,:,:,1].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb7bce24e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzdoPEEm6eUC",
        "colab_type": "text"
      },
      "source": [
        " #### Y vector (New response)\n",
        " \n",
        "Generate response for the whole field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQokPoYh6eoY",
        "colab_type": "code",
        "outputId": "06984d7c-ee4c-4d04-85f1-0ce8c2dd506f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "# Reshape X\n",
        "X = X.reshape(1, X.shape[0], X.shape[1])\n",
        "\n",
        "# Reshape beta\n",
        "beta = beta.reshape(beta.shape[0]*beta.shape[1]*beta.shape[2],beta.shape[3],1)\n",
        "\n",
        "# Reshape Z (note: This step is slow because of the sparse to dense conversion;\n",
        "# it could probably be made quicker but this is only for one simulation at current)\n",
        "Ztmp = Z.toarray().reshape(1, Z.shape[0], Z.shape[1])\n",
        "\n",
        "# Reshape b\n",
        "b = b.reshape(b.shape[0]*b.shape[1]*b.shape[2],b.shape[3],1)\n",
        "\n",
        "print(X.shape)\n",
        "print(Ztmp.shape)\n",
        "print(beta.shape)\n",
        "print(b.shape)\n",
        "\n",
        "# Generate Y\n",
        "Y = np.matmul(X,beta)+np.matmul(Ztmp,b) + np.random.randn(n,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1000, 5)\n",
            "(1, 1000, 100)\n",
            "(8000, 5, 1)\n",
            "(8000, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Laa68OGOvyw",
        "colab_type": "text"
      },
      "source": [
        "Check Y looks reasonable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lTEeBqSOz1B",
        "colab_type": "code",
        "outputId": "7034d9c0-40fd-438a-a796-928f7a20a064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "print(Y.shape)\n",
        "\n",
        "Y_imageformat = Y.reshape((dimv[0],dimv[1],dimv[2],n))\n",
        "\n",
        "imshow(Y_imageformat[10,:,:,1].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 1000, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb7bcd90b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAYFklEQVR4nO3df6xc5X3n8ffnzv1h7BhjQwPEOCFp\nERLbLhRZbrJLK1JSaiwU2irbtVVtScPKTbdIjdqoYjcSibL/bFqllVqiEDdYkColqD9IrdZNcNNK\nNFIgONSASUhwKBU2YC92sMG/7o/57h9z3J1cZq7H9/vce4enn5c0mjPnPM9zvnPm3O+ce+Y8z1FE\nYGZm9RpZ6gDMzGxhOdGbmVXOid7MrHJO9GZmlXOiNzOr3OhSB9DL6PIVMbZqzVKHYWcMy4VZBeJQ\nto0SMbSHpY3kmynyeRRopMT+ORRXHypV+9Tkq0xOHe/ZyFAm+rFVa7j8Q7+dayS3zYbHECQWzSx9\nDKXiGJnO1W9N5j+Q0ZP5NsZO5Ntoncx9KK3T+Q+1dTr/oWqqQBszBXbQrJHcCZZH9n6uf9Opls3M\nbOilEr2kjZK+K2mfpDt6LJ+Q9ECz/FFJl2fWZ2Zm527eiV5SC/gMcBNwFbBF0lWzit0G/CAifgz4\nQ+BT812fmZnNT+aIfgOwLyKei4hJ4EvALbPK3ALc10z/BXCDpFrOnpuZvSlkEv1a4IWu1/ubeT3L\nRMQ0cBS4sFdjkrZK2i1p9/SJ44mwzMys29D8GBsR2yJifUSsH12+YqnDMTOrRibRHwDWdb2+rJnX\ns4ykUWAVcDixTjMzO0eZRP8YcIWkd0oaBzYDO2aV2QHc2kx/APiH8LjIZmaLat4dpiJiWtLtwFeB\nFrA9Ip6W9Elgd0TsAO4B/lTSPuAInS8DMzNbRKmesRGxE9g5a96dXdOngP9y7g1D63Qmsnqku+xD\nvnftEPTOhTLbIt3tv0QHyhK9/gtcuxatXCPtsXwQivzPhEV+aBxZ+osBI3tB4hzVh+bHWDMzWxhO\n9GZmlXOiNzOrnBO9mVnlnOjNzCrnRG9mVjknejOzyjnRm5lVzonezKxyTvRmZpUbypuDawbGXs/1\nE093lx+GoQegyE3O073Ml753eDHZG4yPlLhR+rDsF61k/QLjMEyPJIMARsbyx6uazuabEuNaJOvP\nMYyDj+jNzCrnRG9mVjknejOzyjnRm5lVzonezKxyTvRmZpVzojczq9y8E72kdZL+UdK3JT0t6bd6\nlLle0lFJe5rHnb3aMjOzhZPpMDUN/E5EPC5pJfAtSbsi4tuzyv1TRNycWI+ZmSXM+4g+Il6KiMeb\n6deA7wBrSwVmZmZlFBkCQdLlwE8Cj/ZY/B5JTwAvAh+NiKf7tLEV2Aowvnw149khENqp6mimRD/1\nvChwd/p28lNuj+ZjyHa3ByjQ477AflEghgJtlBheI7tvzYwPx9gYJTan2kubb4D0ZzrXUCfpRC/p\nLcBfAh+JiGOzFj8OvCMiXpe0CfgycEXPICO2AdsAVly4bjiyrJlZBVJX3Ugao5PkvxgRfzV7eUQc\ni4jXm+mdwJikizLrNDOzc5O56kbAPcB3IuIP+pS5pCmHpA3N+g7Pd51mZnbuMqdu/jPw34CnJO1p\n5v0v4O0AEXE38AHgNyRNAyeBzRElxvM0M7NBzTvRR8TXOcuo2BFxF3DXfNdhZmZ57hlrZlY5J3oz\ns8o50ZuZVc6J3sysck70ZmaVKzIEQmlqw+ipbJfkZP3pVPVOGwWuJC0x/MAwdFVvq8T4BQWaSHZV\nHykwNEZ23+y0kW4ircSwFiX273aBOLI7l0pcNJ5so93q/x58RG9mVjknejOzyjnRm5lVzonezKxy\nTvRmZpVzojczq5wTvZlZ5Zzozcwq50RvZla5oewZW0Syl1mJXq0lbuBcogdkkV57w6BEB99kGyVu\nUF5EgTiy72VYem3PjKWbGI5D3uTf+lw3Bx+Gt2dmZgvIid7MrHLpRC/peUlPSdojaXeP5ZL0R5L2\nSXpS0rXZdZqZ2eBKnaN/b0S80mfZTcAVzeOngM82z2ZmtggW49TNLcAXouMR4AJJly7Ces3MjDKJ\nPoCHJH1L0tYey9cCL3S93t/M+yGStkraLWn31OnXC4RlZmZQ5tTNdRFxQNJbgV2SnomIh8+1kYjY\nBmwDeMvqdbVcEGhmtuTSR/QRcaB5PgQ8CGyYVeQAsK7r9WXNPDMzWwSpRC9phaSVZ6aBG4G9s4rt\nAH61ufrm3cDRiHgps14zMxtc9tTNxcCD6twPdBT4s4j4iqQPA0TE3cBOYBOwDzgB/FpynWZmdg5S\niT4ingOu7jH/7q7pAH7znBpWvnu1kielYmQ4+rrP1a15UNltGXPcdHjgGArcwLnEzajTMZTYLwo0\n0Z7Jt5GOYazEEAgl4si3UWIfzweRrD/HW3DPWDOzyjnRm5lVzonezKxyTvRmZpVzojczq5wTvZlZ\n5Zzozcwq50RvZlY5J3ozs8o50ZuZVa7UHaaKCuXv7K7sLe4LKBFCiS732aEDSnQxnynQXb5EHMNw\naKOZ/LbQdIE42ks/Gvgw7N+dOPJtZMlDIJiZ2Xw50ZuZVc6J3sysck70ZmaVc6I3M6ucE72ZWeWc\n6M3MKjfvRC/pSkl7uh7HJH1kVpnrJR3tKnNnPmQzMzsX8+4wFRHfBa4BkNQCDgAP9ij6TxFx83zX\nY2ZmOaVO3dwAfD8i/rVQe2ZmVkipIRA2A/f3WfYeSU8ALwIfjYinexWStBXYCjC+YvWc3XkH0U52\nrS7RrbpdYOsWiSN5h/so8D6KDKMwkW9j+rxcP/NhGYZB7QJhTObqt07lhy8YmUo3wUiB4SBIbs/0\n8AULLL3LSRoH3g/8eY/FjwPviIirgT8GvtyvnYjYFhHrI2L96MSKbFhmZtYocermJuDxiDg4e0FE\nHIuI15vpncCYpIsKrNPMzAZUItFvoc9pG0mXSFIzvaFZ3+EC6zQzswGlzr5KWgH8HPDrXfM+DBAR\ndwMfAH5D0jRwEtgcEUN+NsvMrC6pRB8Rx4ELZ827u2v6LuCuzDrMzCzHPWPNzCrnRG9mVjknejOz\nyjnRm5lVzonezKxypYZAKErkuzW3R3NXcc4khw0AmBnPt1Giy312GIUSwzDMjOfbmF6ZvzJ36oKZ\nVP2xVafTMZx3XnLsAaDENconTuTGlJj8QX5MirEj+WPN8dfyf2etU7n6yu1WHdkPdY76PqI3M6uc\nE72ZWeWc6M3MKudEb2ZWOSd6M7PKOdGbmVXOid7MrHJO9GZmlXOiNzOrnBO9mVnlhnIIBNrB6Knc\nbdlnxnPfYe0CW2Ym3zO7yPADkXwv7RIxFBjKYfq8fMf/iTUnU/WvuuTldAw/serFdBsTSo4RAvzL\nydztm7918LJ0DK9yQbqN1un8DtrK7Ra0JgsMSpFLeR4Cwczs37OBEr2k7ZIOSdrbNW+NpF2Snm2e\nV/epe2tT5llJt5YK3MzMBjPoEf29wMZZ8+4AvhYRVwBfa17/EElrgI8DPwVsAD7e7wvBzMwWxkCJ\nPiIeBo7Mmn0LcF8zfR/wCz2q/jywKyKORMQPgF288QvDzMwWUOYc/cUR8VIz/TJwcY8ya4EXul7v\nb+aZmdkiKfJjbEQEyWHzJW2VtFvS7unTx0uEZWZm5BL9QUmXAjTPh3qUOQCs63p9WTPvDSJiW0Ss\nj4j1oxMrEmGZmVm3TKLfAZy5iuZW4K97lPkqcKOk1c2PsDc288zMbJEMennl/cA3gCsl7Zd0G/B/\ngJ+T9CzwvuY1ktZL+jxARBwB/jfwWPP4ZDPPzMwWyUB9JiNiS59FN/Qouxv4712vtwPb5xWdmZml\nDeUQCGpD62S2P3BOdggF6LyPdBuRH0chkr2zVaB3dxGj+UBWLj+dqn/1qp4/MZ2Tm85/It3GJa3c\n+wD49vILU/WnI/838vCx5ek22oeXpdtQO/d3NjKZDoGR6dz+PVe+8RAIZmaVc6I3M6ucE72ZWeWc\n6M3MKudEb2ZWOSd6M7PKOdGbmVXOid7MrHJO9GZmlXOiNzOr3HAOgRDQOp0bPyBGkl2aJ1LVO21M\n5duI/A3u83eXL3A4UGAkBzSVb6SdHEVheYGhB0oMX/D20bek25iKw6n6a5e9mo5hfGI63Ua7wP6p\n5I7RmswPz9GaSg6BMEd1H9GbmVXOid7MrHJO9GZmlXOiNzOrnBO9mVnlnOjNzCrnRG9mVrmzJnpJ\n2yUdkrS3a97vS3pG0pOSHpR0QZ+6z0t6StIeSbtLBm5mZoMZ5Ij+XmDjrHm7gB+PiP8IfA/4n3PU\nf29EXBMR6+cXopmZZZw10UfEw8CRWfMeiogzXdoeAS5bgNjMzKyAEkMgfAh4oM+yAB6SFMDnImJb\nv0YkbQW2AiybWMXIVK7ffquV6y7fmsz/fDF6Kt3EnHd2H1Qk30q2PgDt/PAFo8fzbRx7bXmq/r+c\n/JF0DM+dd366DTiWbuHATG4YhVenctsSYHoqP8bH6HR+vxiZzg0/kK0PMJIcAoHoXz+V6CV9DJgG\nvtinyHURcUDSW4Fdkp5p/kPoEWNsA7YBnL9ybX6rmZkZkLjqRtIHgZuBX4no/VUSEQea50PAg8CG\n+a7PzMzmZ16JXtJG4HeB90fEiT5lVkhaeWYauBHY26usmZktnEEur7wf+AZwpaT9km4D7gJW0jkd\ns0fS3U3Zt0na2VS9GPi6pCeAbwJ/GxFfWZB3YWZmfZ31HH1EbOkx+54+ZV8ENjXTzwFXp6IzM7M0\n94w1M6ucE72ZWeWc6M3MKudEb2ZWOSd6M7PKlRgCobx2MHI6f3f4jNHRfLdqyHfvbk3mo4jsWymw\nKUam8m20x/OBHD88kar/2PlvT8cwqpl0G+uWHTl7obN4+fSqVP1/Prw2HcP00fF0GxMn003QOp2r\nP1IgXWWHUdAc1X1Eb2ZWOSd6M7PKOdGbmVXOid7MrHJO9GZmlXOiNzOrnBO9mVnlnOjNzCrnRG9m\nVrmh7BmrdjDyWu7O2jqVe2uaGkvVB2idzrcxM5H/Lo6RXI/SSN5oHWCkwA2co5XfFjMTud7Kr4xe\nkI7h709dmW5jxbJ8l+mTk7n98/gr+ZuDLzuYT0HjR/O3mB49lbw5ePbG3gDtZH33jDUz+/fLid7M\nrHKD3DN2u6RDkvZ2zfuEpAPN/WL3SNrUp+5GSd+VtE/SHSUDNzOzwQxyRH8vsLHH/D+MiGuax87Z\nCyW1gM8ANwFXAVskXZUJ1szMzt1ZE31EPAzMZ0zUDcC+iHguIiaBLwG3zKMdMzNLyJyjv13Sk82p\nndU9lq8FXuh6vb+Z15OkrZJ2S9o9OX0iEZaZmXWbb6L/LPCjwDXAS8Cns4FExLaIWB8R68dH85dt\nmZlZx7wSfUQcjIiZiGgDf0LnNM1sB4B1Xa8va+aZmdkimleil3Rp18tfBPb2KPYYcIWkd0oaBzYD\nO+azPjMzm7+zdkuTdD9wPXCRpP3Ax4HrJV1Dpy/W88CvN2XfBnw+IjZFxLSk24Gv0rl56vaIeHpB\n3oWZmfV11kQfEVt6zL6nT9kXgU1dr3cCb7j08qymZ+DVY+dcrdvIaHIIhFO5m0gDjJxelm9jef7m\nyTGe6xc3M1aiX13+Ruljx/PdzCeOZIdiyHfZn3x9ZbqNU+P5bTEymdsW5x3LD2sxcTjdBMteLTAE\nwonc+AMjU9nxC+a+uXeWe8aamVXOid7MrHJO9GZmlXOiNzOrnBO9mVnlnOjNzCrnRG9mVjknejOz\nyjnRm5lVzonezKxy+f7cCyBmZmi/ejTVhrJDIEzmh0oeaRfo06x8N/PpVnIYhYl8DJEfAaFIF/HR\nU7lG4mh+W7SSQw8AxEi+jZGpXP2x1wsMSXE038b4sZl0G61TuSEMVORvPVl9jhB8RG9mVjknejOz\nyjnRm5lVzonezKxyTvRmZpVzojczq5wTvZlZ5Qa5Z+x24GbgUET8eDPvAeDKpsgFwKsRcU2Pus8D\nrwEzwHRErC8Ut5mZDWiQXkX3AncBXzgzIyL+65lpSZ8G5urd9N6IeGW+AZqZWc4gNwd/WNLlvZZJ\nEvDLwM+WDcvMzErJDoHw08DBiHi2z/IAHpIUwOciYlu/hiRtBbYCLGM5cfp0LrKZXLdotQr02V82\nkW8jCnStbuX6Vs9M5H/KmV6Wb2NmvMDQAckmWpP5z0Pt/PsoIftexo4XGL7gtfzwBaMn8m2MTObb\nyEoPazFHrsgm+i3A/XMsvy4iDkh6K7BL0jMR8XCvgs2XwDaA87WmQHYzMzNIXHUjaRT4JeCBfmUi\n4kDzfAh4ENgw3/WZmdn8ZP6ffh/wTETs77VQ0gpJK89MAzcCexPrMzOzeThropd0P/AN4EpJ+yXd\n1izazKzTNpLeJmln8/Ji4OuSngC+CfxtRHylXOhmZjaIQa662dJn/gd7zHsR2NRMPwdcnYzPzMyS\n3DPWzKxyTvRmZpVzojczq5wTvZlZ5Zzozcwql+0ZO7QieVf2SA6hAKASwxco312+PZr7Pp+ZyMdQ\npI2xdBNEgZEtskamSwyjUCCOyVz9EsNBjEwVaKPA8AUjU7kNmh1aA4BW8rh7jk3pI3ozs8o50ZuZ\nVc6J3sysck70ZmaVc6I3M6ucE72ZWeWc6M3MKudEb2ZWOSd6M7PKOdGbmVVOUaKbfmGS/i/wr3MU\nuQh4ZZHCyXCc5bwZYgTHWZrjHNw7IuJHei0YykR/NpJ2R8T6pY7jbBxnOW+GGMFxluY4y/CpGzOz\nyjnRm5lV7s2a6LctdQADcpzlvBliBMdZmuMs4E15jt7MzAb3Zj2iNzOzATnRm5lVbqgTvaSNkr4r\naZ+kO3osn5D0QLP8UUmXL0GM6yT9o6RvS3pa0m/1KHO9pKOS9jSPOxc7ziaO5yU91cSwu8dySfqj\nZns+KenaRY7vyq5ttEfSMUkfmVVmSbalpO2SDkna2zVvjaRdkp5tnlf3qXtrU+ZZSbcuQZy/L+mZ\n5jN9UNIFferOuX8sQpyfkHSg67Pd1KfunHlhEeJ8oCvG5yXt6VN30bbnWUXEUD6AFvB94F3AOPAE\ncNWsMv8DuLuZ3gw8sARxXgpc20yvBL7XI87rgb8Zgm36PHDRHMs3AX8HCHg38OgSf/4v0+kEsuTb\nEvgZ4Fpgb9e83wPuaKbvAD7Vo94a4LnmeXUzvXqR47wRGG2mP9UrzkH2j0WI8xPARwfYL+bMCwsd\n56zlnwbuXOrtebbHMB/RbwD2RcRzETEJfAm4ZVaZW4D7mum/AG6QCtxN+xxExEsR8Xgz/RrwHWDt\nYsZQ0C3AF6LjEeACSZcuUSw3AN+PiLl6SC+aiHgYODJrdvf+dx/wCz2q/jywKyKORMQPgF3AxsWM\nMyIeiojp5uUjwGULtf5B9dmegxgkLxQzV5xNrvll4P6FWn8pw5zo1wIvdL3ezxsT6L+VaXbko8CF\nixJdD82po58EHu2x+D2SnpD0d5L+w6IG9v8F8JCkb0na2mP5INt8sWym/x/QMGxLgIsj4qVm+mXg\n4h5lhmmbAnyIzn9tvZxt/1gMtzenmLb3ORU2TNvzp4GDEfFsn+XDsD2B4U70byqS3gL8JfCRiDg2\na/HjdE5BXA38MfDlxY6vcV1EXAvcBPympJ9ZojjmJGkceD/w5z0WD8u2/CHR+V99qK9VlvQxYBr4\nYp8iS71/fBb4UeAa4CU6p0WG2RbmPppf6u35b4Y50R8A1nW9vqyZ17OMpFFgFXB4UaLrImmMTpL/\nYkT81ezlEXEsIl5vpncCY5IuWuQwiYgDzfMh4EE6/wZ3G2SbL4abgMcj4uDsBcOyLRsHz5zaap4P\n9SgzFNtU0geBm4Ffab6U3mCA/WNBRcTBiJiJiDbwJ33WPyzbcxT4JeCBfmWWent2G+ZE/xhwhaR3\nNkd4m4Eds8rsAM5cxfAB4B/67cQLpTlPdw/wnYj4gz5lLjnz24GkDXS2+6J+IUlaIWnlmWk6P9Dt\nnVVsB/CrzdU37waOdp2aWEx9j5SGYVt26d7/bgX+ukeZrwI3SlrdnIq4sZm3aCRtBH4XeH9EnOhT\nZpD9Y0HN+j3oF/usf5C8sBjeBzwTEft7LRyG7flDlvrX4LkedK4C+R6dX9k/1sz7JJ0dFmAZnX/v\n9wHfBN61BDFeR+df9ieBPc1jE/Bh4MNNmduBp+lcIfAI8J+WIM53Net/oonlzPbsjlPAZ5rt/RSw\nfgniXEEnca/qmrfk25LOF89LwBSd88K30fk96GvAs8DfA2uasuuBz3fV/VCzj+4Dfm0J4txH57z2\nmf3zzJVqbwN2zrV/LHKcf9rsd0/SSd6Xzo6zef2GvLCYcTbz7z2zT3aVXbLtebaHh0AwM6vcMJ+6\nMTOzApzozcwq50RvZlY5J3ozs8o50ZuZVc6J3sysck70ZmaV+39JWcCypM3TFgAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-MhOA6XP-op",
        "colab_type": "text"
      },
      "source": [
        "#### Transpose products\n",
        "\n",
        "Calculate X'Y, X'Z, X'Y, Y'Y, Y'Z, Y'X Z'Z, Z'X and Z'Y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCFYLn3sQVU2",
        "colab_type": "code",
        "outputId": "f3d627e7-3e17-45f0-9cca-6050813dbd5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "# X'Z\\Z'X\n",
        "XtZ = np.matmul(X.transpose(0,2,1),Ztmp)\n",
        "ZtX = XtZ.transpose(0,2,1)\n",
        "\n",
        "# Z'Y\\Y'Z\n",
        "YtZ = np.matmul(Y.transpose(0,2,1),Ztmp)\n",
        "ZtY = YtZ.transpose(0,2,1)\n",
        "\n",
        "# Y'X/X'Y\n",
        "YtX = np.matmul(Y.transpose(0,2,1),X)\n",
        "XtY = YtX.transpose(0,2,1)\n",
        "\n",
        "# YtY\n",
        "YtY = np.matmul(Y.transpose(0,2,1),Y)\n",
        "\n",
        "# ZtZ\n",
        "ZtZ = np.matmul(Ztmp.transpose(0,2,1),Ztmp)\n",
        "\n",
        "# X'X\n",
        "XtX = np.matmul(X.transpose(0,2,1),X)\n",
        "\n",
        "\n",
        "print(XtZ.shape)\n",
        "print(ZtX.shape)\n",
        "\n",
        "print(XtY.shape)\n",
        "print(YtX.shape)\n",
        "\n",
        "print(YtZ.shape)\n",
        "print(ZtY.shape)\n",
        "\n",
        "print(XtX.shape)\n",
        "\n",
        "print(YtY.shape)\n",
        "\n",
        "print(ZtZ.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 5, 100)\n",
            "(1, 100, 5)\n",
            "(8000, 5, 1)\n",
            "(8000, 1, 5)\n",
            "(8000, 1, 100)\n",
            "(8000, 100, 1)\n",
            "(1, 5, 5)\n",
            "(8000, 1, 1)\n",
            "(1, 100, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYxYXW3AUu3H",
        "colab_type": "text"
      },
      "source": [
        "### Demonstration: Time taken just looping\n",
        "\n",
        "This is a demonstration showing how long PLS takes when doing each voxel seperately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgah7SUnUt5k",
        "colab_type": "code",
        "outputId": "26377872-1af4-4068-92c5-7559839a682f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# Initialize empty estimates\n",
        "beta_est = np.zeros(beta.shape)\n",
        "\n",
        "# Initialize temporary X'X, X'Z, Z'X and Z'Z\n",
        "XtZtmp = matrix(XtZ[0,:,:])\n",
        "ZtXtmp = matrix(ZtX[0,:,:])\n",
        "ZtZtmp = cvxopt.sparse(matrix(ZtZ[0,:,:]))\n",
        "XtXtmp = matrix(XtX[0,:,:])\n",
        "\n",
        "# Initial theta value. Bates (2005) suggests using [vech(I_q1),...,vech(I_qr)] where I is the identity matrix\n",
        "theta0 = np.array([])\n",
        "for i in np.arange(r):\n",
        "  theta0 = np.hstack((theta0, mat2vech(np.eye(nparams[i])).reshape(np.int64(nparams[i]*(nparams[i]+1)/2))))\n",
        "  \n",
        "# Obtain a random Lambda matrix with the correct sparsity for the permutation vector\n",
        "tinds,rinds,cinds=get_mapping(nlevels, nparams)\n",
        "Lam=mapping(np.random.randn(theta0.shape[0]),tinds,rinds,cinds)\n",
        "\n",
        "# Obtain Lambda'Z'ZLambda\n",
        "LamtZtZLam = spmatrix.trans(Lam)*cvxopt.sparse(matrix(ZtZtmp))*Lam\n",
        "\n",
        "\n",
        "# Identity (Actually quicker to calculate outside of estimation)\n",
        "I = spmatrix(1.0, range(Lam.size[0]), range(Lam.size[0]))\n",
        "\n",
        "# Obtaining permutation for PLS\n",
        "P=cvxopt.amd.order(LamtZtZLam)\n",
        "\n",
        "demo = False\n",
        "\n",
        "if demo:\n",
        "  \n",
        "  nv_tmp = beta.shape[0]\n",
        "  \n",
        "else:\n",
        "  \n",
        "  nv_tmp = 5\n",
        "\n",
        "t1 = time.time()\n",
        "for i in np.arange(nv_tmp):\n",
        "  \n",
        "  print(i)\n",
        "  \n",
        "  XtYtmp = matrix(XtY[i,:,:]) \n",
        "  ZtYtmp = matrix(ZtY[i,:,:]) \n",
        "  YtYtmp = matrix(YtY[i,:,:]) \n",
        "  YtZtmp = matrix(YtZ[i,:,:])\n",
        "  YtXtmp = matrix(YtX[i,:,:])\n",
        "  \n",
        "  theta_est = minimize(PLS, theta0, args=(ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6)\n",
        "  nit = theta_est.nit\n",
        "  theta_est = theta_est.x\n",
        "  print(nit)\n",
        "  \n",
        "  # Get current beta\n",
        "  beta_current = PLS_getBeta(theta_est, ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P,tinds, rinds, cinds)\n",
        "  \n",
        "  beta_est[i,:,:] = beta_current[:]\n",
        "  \n",
        "t2 = time.time()\n",
        "print(\"Time taken in seconds for this example:\")\n",
        "print(t2-t1)\n",
        "print(\"Estimated time taken for this example on a nifti of size (100x100x100), in hours:\")\n",
        "print(100*100*100*(t2-t1)/(nv_tmp*60*60))\n",
        "\n",
        "print(\"True beta (3)\")\n",
        "\n",
        "if demo:\n",
        "  beta_map=beta.reshape(dimv[0],dimv[1],dimv[2],beta.shape[1])\n",
        "  beta_est_map=beta_est.reshape(dimv[0],dimv[1],dimv[2],beta.shape[1])\n",
        "\n",
        "\n",
        "  # Show true beta, 3rd x-slice, 3rd parameter\n",
        "  imshow(beta_map[3,:,:,3].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "21\n",
            "1\n",
            "20\n",
            "2\n",
            "18\n",
            "3\n",
            "28\n",
            "4\n",
            "21\n",
            "Time taken in seconds for this example:\n",
            "0.5967667102813721\n",
            "Estimated time taken for this example on a nifti of size (100x100x100), in hours:\n",
            "33.15370612674289\n",
            "True beta (3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQyp_7qj56-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if demo:\n",
        "  print(\"Estimated beta (3)\")\n",
        "\n",
        "  # Show estimated beta, 3rd x-slice, 3rd parameter\n",
        "  imshow(beta_est_map[3,:,:,3].reshape(10,10), \\\n",
        "                    interpolation='nearest', aspect='auto')\n",
        "  plt.colorbar()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzs1yhAt6WTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if demo:\n",
        "  # Sanity check, lets look at the differences\n",
        "  imshow((beta_est_map[3,:,:,3]-beta_map[3,:,:,3]).reshape(10,10), \\\n",
        "                    interpolation='nearest', aspect='auto')\n",
        "  plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jr5TiVBqzbD",
        "colab_type": "text"
      },
      "source": [
        "Now lets try the same but with Dask delayed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBHSWUbNqwvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dask\n",
        "\n",
        "@dask.delayed\n",
        "def minPLS(theta0, ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tinds, rinds, cinds):\n",
        "  \n",
        "  theta_est = minimize(PLS, theta0, args=(ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6)\n",
        "  nit = theta_est.nit\n",
        "  print(nit)\n",
        "  theta_est = theta_est.x\n",
        "  \n",
        "  return(theta_est)\n",
        "\n",
        "@dask.delayed\n",
        "def getBeta(theta_est, ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P,tinds, rinds, cinds, beta_est, i):\n",
        "\n",
        "  beta_est = copy(beta_est)\n",
        "  beta_current = PLS_getBeta(theta_est, ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P,tinds, rinds, cinds)\n",
        "  beta_est[i,:,:] = beta_current[:]\n",
        "  \n",
        "  return(beta_est)\n",
        "  \n",
        "# Initialize empty estimates\n",
        "beta_est = np.zeros(beta.shape)\n",
        "t1 = time.time()\n",
        "for i in np.arange(beta.shape[0]):\n",
        "  \n",
        "  theta_est = minPLS(theta0, ZtXtmp, matrix(ZtY[i,:,:]), XtXtmp, ZtZtmp, matrix(XtY[i,:,:]), matrix(YtX[i,:,:]), matrix(YtZ[i,:,:]), XtZtmp, matrix(YtY[i,:,:]), n, P, I, tinds, rinds, cinds)\n",
        "  \n",
        "  # Get current beta\n",
        "  beta_est = getBeta(theta_est, ZtXtmp, matrix(ZtY[i,:,:]), XtXtmp, ZtZtmp, matrix(XtY[i,:,:]), matrix(YtX[i,:,:]), matrix(YtZ[i,:,:]), XtZtmp, matrix(YtY[i,:,:]), n, P,tinds, rinds, cinds, beta_est, i)\n",
        "  \n",
        "beta_est.compute()\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"Time taken in seconds for this example:\")\n",
        "print(t2-t1)\n",
        "print(\"Estimated time taken for this example on a nifti of size (100x100x100), in hours:\")\n",
        "print(100*100*100*(t2-t1)/(nv*60*60))\n",
        "\n",
        "print(\"True beta (3)\")\n",
        "beta_map=beta.reshape(dimv[0],dimv[1],dimv[2],beta.shape[1])\n",
        "beta_est_map=beta_est.reshape(dimv[0],dimv[1],dimv[2],beta.shape[1])\n",
        "\n",
        "\n",
        "# Show true beta, 3rd x-slice, 3rd parameter\n",
        "imshow(beta_map[3,:,:,3].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDkUiAca6p5G",
        "colab_type": "text"
      },
      "source": [
        "Now, let's try the neighbour method, see how the results change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZFyOOob6qtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = time.time()\n",
        "for i in np.arange(beta.shape[0]):\n",
        "  \n",
        "  XtYtmp = matrix(XtY[i,:,:]) \n",
        "  ZtYtmp = matrix(ZtY[i,:,:]) \n",
        "  YtYtmp = matrix(YtY[i,:,:]) \n",
        "  YtZtmp = matrix(YtZ[i,:,:])\n",
        "  YtXtmp = matrix(YtX[i,:,:])\n",
        "  \n",
        "  if i==0:\n",
        "    theta_est = minimize(PLS, theta0, args=(ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  else:\n",
        "    theta_est = minimize(PLSneighbour, theta0, args=(beta_current, ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "  # Get current beta\n",
        "  beta_current = PLS_getBeta(theta_est, ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P,tinds, rinds, cinds)\n",
        "  beta_est[i,:,:] = beta_current[:]\n",
        "  \n",
        "t2 = time.time()\n",
        "print(\"Time taken in seconds for this example:\")\n",
        "print(t2-t1)\n",
        "print(\"Estimated time taken for this example on a nifti of size (100x100x100), in hours:\")\n",
        "print(1000*(t2-t1)/(60*60))\n",
        "\n",
        "# Lets look at the differences\n",
        "beta_map=beta.reshape(10,10,10,beta.shape[1])\n",
        "beta_est_map=beta_est.reshape(10,10,10,beta.shape[1])\n",
        "imshow((beta_est_map[3,:,:,3]-beta_map[3,:,:,3]).reshape(10,10), \\\n",
        "                    interpolation='nearest', aspect='auto')\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfeS68Db9J1v",
        "colab_type": "text"
      },
      "source": [
        "Lets try checking the re-used starting point method.\n",
        "\n",
        "**Conclusion:** The added penalty had neglible effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8nM3SJC9KAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = time.time()\n",
        "for i in np.arange(beta.shape[0]):\n",
        "  XtYtmp = matrix(XtY[i,:,:]) \n",
        "  ZtYtmp = matrix(ZtY[i,:,:]) \n",
        "  YtYtmp = matrix(YtY[i,:,:]) \n",
        "  YtZtmp = matrix(YtZ[i,:,:])\n",
        "  YtXtmp = matrix(YtX[i,:,:])\n",
        "  \n",
        "  if i==0:\n",
        "    theta_est = minimize(PLS, theta0, args=(ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  else:\n",
        "    theta_est = minimize(PLS, theta_est, args=(ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "    \n",
        "  # Get current beta\n",
        "  beta_current = PLS_getBeta(theta_est, ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P,tinds, rinds, cinds)\n",
        "  beta_est[i,:,:] = beta_current[:]\n",
        "  \n",
        "t2 = time.time()\n",
        "print(\"Time taken in seconds for this example:\")\n",
        "print(t2-t1)\n",
        "print(\"Estimated time taken for this example on a nifti of size (100x100x100), in hours:\")\n",
        "print(1000*(t2-t1)/(60*60))\n",
        "\n",
        "# Lets look at the differences\n",
        "beta_map=beta.reshape(10,10,10,beta.shape[1])\n",
        "beta_est_map=beta_est.reshape(10,10,10,beta.shape[1])\n",
        "imshow((beta_est_map[3,:,:,3]-beta_map[3,:,:,3]).reshape(10,10), \\\n",
        "                    interpolation='nearest', aspect='auto')\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x78FGBZoIYEI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**Conclusion:** Often marked improvement in terms of time efficiency!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KowcFHBSSAIa",
        "colab_type": "text"
      },
      "source": [
        "### Idea 3: Broadcast everything we can\n",
        "\n",
        "The title for this idea speaks for itself. The only operations that cannot be broadcast are those that must be done in `cvxopt`, the only for which it is absolutely must use `cvxopt` is the sparse cholesky decomposition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkCJOA2DkdNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try dask formatting first\n",
        "import dask.array as da\n",
        "\n",
        "XtX_da = da.from_array(XtX, chunks=(1, 4000, 4000))\n",
        "XtY_da = da.from_array(XtY, chunks=(100, 400, 400))\n",
        "XtZ_da = da.from_array(XtZ, chunks=(1, 4000, 4000))\n",
        "YtX_da = da.from_array(YtX, chunks=(100, 4000, 4000))\n",
        "YtY_da = da.from_array(YtY, chunks=(100, 1, 1))\n",
        "YtZ_da = da.from_array(YtZ, chunks=(100, 400, 400))\n",
        "ZtX_da = da.from_array(ZtX, chunks=(1, 4000, 4000))\n",
        "ZtY_da = da.from_array(ZtY, chunks=(1, 4000, 4000))\n",
        "ZtZ_da = da.from_array(ZtZ, chunks=(1, 4000, 4000)).map_blocks(sparse.COO).rechunk('auto')\n",
        "\n",
        "nv = np.prod(dimv) # Number of voxels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsNtMTsfZEzm",
        "colab_type": "text"
      },
      "source": [
        "#### Helper functions for idea 3\n",
        "\n",
        "Idea 3 needs broadcasted equivalents of the previous functions; these are given below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJFifADzZNvL",
        "colab_type": "text"
      },
      "source": [
        "#### Get the 3D mapping\n",
        "\n",
        "This function returns a 3D mapping equivalent to the previous mapping from a vector of parameters, theta, to indices which maps the parameters the to lower triangular block diagonal matrix, lambda; but for every voxel.\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **nlevels**: a vector of the number of levels for each grouping factor. e.g. nlevels=[10,2] means there are 10 levels for factor 1 and 2 levels for factor 2.\n",
        " - **nparams**: a vector of the number of variables for each grouping factor. e.g. nparams=[3,4] means there are 3 variables for factor 1 and 4 variables for factor 2.\n",
        " - **nv**: The number of voxels the mapping is required for.\n",
        "\n",
        "All arrays must be np arrays.\n",
        "\n",
        "Note: It is assumed that all voxels have the same mapping here, i.e. they all have the same number of parameters and levels. This could be removed if needed but I suggest removing this assumption would have little practical use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w_peMYTZE6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_3D_mapping(nlevels, nparams, nv, P=None):\n",
        "\n",
        "    # Work out the 2D mapping for one voxel\n",
        "    theta_repeated_inds, row_indices, col_indices = get_mapping(nlevels, nparams)\n",
        "    \n",
        "    print('1')\n",
        "    \n",
        "    # Repeat the row and column indices for each voxel\n",
        "    row_indices_3D = np.tile(row_indices, nv)\n",
        "    col_indices_3D = np.tile(col_indices, nv)\n",
        "    \n",
        "    print('2')\n",
        "    \n",
        "    if not (P is None):\n",
        "      # Work out the inverse permutation\n",
        "      P = np.array(P[:]).reshape(P.size[0])\n",
        "      invP = np.arange(len(P))[np.argsort(P)]\n",
        "      \n",
        "      # Permute the columns using the inverse permutation\n",
        "      print('invP')\n",
        "      print(type(invP))\n",
        "      print(invP)\n",
        "      \n",
        "      col_indices_P = invP[col_indices.astype(np.int64)]\n",
        "      \n",
        "      # Tile across voxels\n",
        "      col_indices_P_3D = np.tile(col_indices_P, nv)\n",
        "    else:\n",
        "      col_indices_P_3D = col_indices_3D\n",
        "    \n",
        "    print('3')\n",
        "    \n",
        "    # Make an array for the coordinate representing which voxel we are looking \n",
        "    # at.\n",
        "    vox_indices_3D = np.repeat(np.arange(nv), len(col_indices))\n",
        "    \n",
        "    print('4')\n",
        "    \n",
        "    # Make 3D theta indices\n",
        "    theta_indices_3D = np.tile(theta_repeated_inds, nv)#+vox_indices_3D*len(col_indices)#Not correct \n",
        "    \n",
        "    print('5')\n",
        "    \n",
        "    # We need unique theta elements for each voxel so we change the 3D theta indices \n",
        "    theta_indices_3D = theta_indices_3D + vox_indices_3D*(np.max(theta_repeated_inds)+1)\n",
        "    \n",
        "    print('6')\n",
        "    \n",
        "    # Return \n",
        "    return(theta_indices_3D.astype(np.int64), row_indices_3D.astype(np.int16), col_indices_3D.astype(np.int16), col_indices_P_3D.astype(np.int16), vox_indices_3D.astype(np.int16))\n",
        "\n",
        "print(nparams)\n",
        "print(nlevels)\n",
        "theta_indices_3D, row_indices_3D, col_indices_3D, col_indices_P_3D, vox_indices_3D = get_3D_mapping(nlevels, nparams, nv)\n",
        "\n",
        "print(len(P))\n",
        "x = np.random.randn(len(P),len(P))\n",
        "\n",
        "import sys\n",
        "print(x.shape)\n",
        "print(x[:,P].reshape(x.shape[0], x.shape[1]).shape)\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "print(x)\n",
        "print(x[:,P].reshape(x.shape[0], x.shape[1]))\n",
        "\n",
        "print(P)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKqSBrIPz32K",
        "colab_type": "text"
      },
      "source": [
        "#### Permute Lambda\n",
        "\n",
        "The below function takes in Lambda and an `approximate minimum degree` permutation vector and applies the permutation to Lambda. (This function is mainly just applied to make it clearer where we are applying the permutation).\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        " - Lambda: The \n",
        " - nparams: The number of parameters vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is9Rmt0fz4BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def permuteLambda(Lambda, P):\n",
        "  \n",
        "  return(Lambda[:,P])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZhUF3lKvWXw",
        "colab_type": "text"
      },
      "source": [
        "### Apply 3D mapping function\n",
        "\n",
        "The below function applies a mapping to a vector of parameters for a list of voxels.\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **theta**: the vector of theta parameters for all voxels (i.e. if one voxel has 6 parameters and there are 10 voxels, then theta is 60 by 1 and laid out as [$p_1$,$p_2$,...,$p_{10}$] where $p_n$ is the parameter vector for voxel n.\n",
        " - **v_inds**: A vector specifying which voxel each entry in the theta vector belongs to. E.g. v_inds[i]=j means the $i^{th}$ parameter entry belongs to voxel $j$.\n",
        " - **t_inds**: A vector specifying how many times each theta parameter should be repeated. For example, if theta=[0.1,0.8,0.3] and theta_inds=[0,0,0,1,2,2], then the values to be mapped into the sparse matrix would be [0.1,0.1,0.1,0.8,0.3,0.3].\n",
        " - **r_inds**: The row indices of the elements mapped into the sparse matrix.\n",
        " - **c_inds**: The column indices of the elements mapped into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcdMAE1syUfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapping_3D(theta, v_inds, t_inds, r_inds, c_inds):\n",
        "  \n",
        "  # Create the coordinate matrix\n",
        "  coords = np.array([v_inds, r_inds, c_inds])\n",
        "\n",
        "  # Create Lambda\n",
        "  lam = sparse.COO(coords, theta[t_inds.astype(np.int64)].tolist(), shape=(np.max(v_inds)+1,np.max(r_inds)+1,np.max(c_inds)+1))\n",
        "\n",
        "  return(lam)\n",
        "\n",
        "# Random theta to test with\n",
        "print(len(vox_indices_3D))\n",
        "print(len(theta_indices_3D))\n",
        "print(len(row_indices_3D))\n",
        "print(len(col_indices_3D))\n",
        "print(np.max(theta_indices_3D))\n",
        "print(nv)\n",
        "print((np.sum((nparams*(nparams+1)/2)))*nv)\n",
        "print(np.int16((np.sum((nparams*(nparams+1)/2)))*nv))\n",
        "theta = np.random.randn(np.int64((np.sum((nparams*(nparams+1)/2)))*nv))\n",
        "\n",
        "print(theta.shape)\n",
        "\n",
        "# Get lambda\n",
        "lam = mapping_3D(theta, vox_indices_3D, theta_indices_3D, row_indices_3D, col_indices_3D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kg_SG7m8vsI",
        "colab_type": "text"
      },
      "source": [
        "#### Initial Theta function\n",
        "\n",
        "The below function calculates the initial theta vector for the whole brain.\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        " - nv: The number of voxels.\n",
        " - nparams: The number of parameters vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTBeQKxE8v2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_theta(nv, nparams):\n",
        "  \n",
        "  # Get the initial parameter values for a single voxel\n",
        "  # These are given by the lower triangle of an identity\n",
        "  # for each factor of size #params by #params\n",
        "  init_theta_tmp = np.array([])\n",
        "  for nump in nparams:\n",
        "    \n",
        "    I = np.eye(nump)\n",
        "    lower = I[np.tril_indices(nump)]\n",
        "    init_theta_tmp = np.hstack((init_theta_tmp, np.array(lower[:])))\n",
        "    \n",
        "  # Repeat for all voxels and return.\n",
        "  return(np.tile(init_theta_tmp, reps=nv))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbsGqStlYAas",
        "colab_type": "text"
      },
      "source": [
        "#### Broadcasted PLS function\n",
        "\n",
        "Commented lines still need doing... documentation also coming soon..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkkDPCSRYG-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler\n",
        "import dask\n",
        "\n",
        "def PLS_broadcasted(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, n, tinds, rinds, cinds, cinds_permuted, vinds):\n",
        "    \n",
        "    t1 = time.time()\n",
        "    # Obtain Lambda and Lambda*P\n",
        "    Lambda = mapping_3D(theta[:], vinds, tinds, rinds, cinds_permuted)\n",
        "    #LambdaP = mapping_3D(theta[:], vinds, tinds, rinds, cinds)\n",
        "    \n",
        "    print('1')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Obtain Lambda'\n",
        "    Lambdat = Lambda.transpose((0,2,1))\n",
        "\n",
        "    print('2')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Obtain Lambda'Z'Y and Lambda'Z'X\n",
        "    LambdatZtY = da.matmul(Lambdat,ZtY).rechunk((16000, 10, 1)) #### SLOW: PRESUMABLY COS LAM NOT DASK ARRAY\n",
        "    LambdatZtX = da.matmul(Lambdat,ZtX).rechunk((100, 400, 400))\n",
        "\n",
        "    print('3')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Obtain the cholesky decomposition \n",
        "    LambdatZtZLambda = da.matmul(da.matmul(Lambdat,ZtZ),Lambda)\n",
        "    I = da.eye(Lambda.shape[1], chunks=4000).map_blocks(sparse.COO).rechunk('auto') #fast - could be removed though\n",
        "    \n",
        "    print('4')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    print((LambdatZtZLambda+I).shape)\n",
        "    \n",
        "    # Get P in numpy format\n",
        "    P_np = np.array(P[:]).reshape(P.size[0])\n",
        "    \n",
        "    # Permute lambda'Z'X and lambda'Z'Y\n",
        "    print(LambdatZtX.chunks)\n",
        "    #LambdatZtXP = LambdatZtX[:,P_np,:].compute() #NB - about half a second\n",
        "    \n",
        "    LambdatZtXP, LambdatZtYP = dask.compute(LambdatZtX[:,P_np,:], LambdatZtY[:,P_np,:])\n",
        "    print('5')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    print(LambdatZtY.shape)\n",
        "    print(LambdatZtY.chunks)\n",
        "    #LambdatZtYP = LambdatZtY[:,P_np,:].compute() # 86 seconds? Weird\n",
        "    \n",
        "    \n",
        "    print('6')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Add identity to Lambda'Z'ZLambda\n",
        "    LambdatZtZLambdaplusI = LambdatZtZLambda + I\n",
        "    LambdatZtZLambdaplusI = LambdatZtZLambdaplusI.compute() # 27 seconds - Probably chunking problems\n",
        "    \n",
        "    print('7')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Initialize empty arrays to store RZX and Cu\n",
        "    RZX = np.zeros((Lambda.shape[0], ZtZ.shape[1], XtX.shape[1]))\n",
        "    Cu = np.zeros((Lambda.shape[0], ZtZ.shape[1], 1))\n",
        "    L = np.zeros((Lambda.shape[0], ZtZ.shape[1], ZtZ.shape[1])) # All fast - 0.4s-ish\n",
        "    \n",
        "    print('8')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    for i in np.arange(Lambda.shape[0]):\n",
        "      print(i)\n",
        "      \n",
        "      # Perform sparse cholesky on lambda'Z'Zlambda + I\n",
        "      data = LambdatZtZLambdaplusI[i,:,:].data\n",
        "      coords = LambdatZtZLambdaplusI[i,:,:].coords\n",
        "      chol_dict = sparse_chol(spmatrix(data, coords[0,:],coords[1,:],size=(Lambda.shape[1],Lambda.shape[1])), perm=P, retF=True, retP=False, retL=False)\n",
        "      F = chol_dict['F']\n",
        "      \n",
        "      # Obtain Cu\n",
        "      Cu_tmp = matrix(LambdatZtYP[i,:,:])\n",
        "      cholmod.solve(F,Cu_tmp,sys=4)\n",
        "      \n",
        "      # Save Cu\n",
        "      Cu[i,:,:] = np.array(Cu_tmp)\n",
        "      \n",
        "      # Obtain RZX\n",
        "      RZXtmp = matrix(LambdatZtXP[i,:,:])\n",
        "      cholmod.solve(F,RZXtmp,sys=4)\n",
        "      \n",
        "      # Save RZX\n",
        "      RZX[i,:,:] = np.array(RZXtmp)\n",
        "    \n",
        "      # Obtain L (for later - note: has to be done last as this changes F)\n",
        "      Ltmp=cholmod.getfactor(F)\n",
        "      L[i,:,:] = np.array(matrix(Ltmp))\n",
        "      \n",
        "    \n",
        "    print('9')# Shockingly fast - 23.156267166137695\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "      \n",
        "    # Obtain RXtRX\n",
        "    RZX = da.from_array(RZX, chunks=(100, 400, 400))\n",
        "    RXtRX = XtX - da.matmul(RZX.transpose(0,2,1),RZX)\n",
        "    \n",
        "\n",
        "    # Obtain beta estimates \n",
        "    Cu = da.from_array(Cu, chunks=(400, 400, 1))\n",
        "    XtYminusRZXtCu = XtY - da.matmul(RZX.transpose(0,2,1),Cu)\n",
        "    \n",
        "    #Get betahat\n",
        "    RXtRX, XtYminusRZXtCu = dask.compute(RXtRX, XtYminusRZXtCu)\n",
        "    betahat = da.from_array(np.linalg.solve(RXtRX, XtYminusRZXtCu), chunks=(100, 400, 400))\n",
        "    print(time.time()-t1)\n",
        "    betahat = da.apply_gufunc(np.linalg.solve,  \"(i,j),(i,k)->(j,k)\", RXtRX, XtYminusRZXtCu, vectorize=True,output_dtypes=RXtRX.dtype).rechunk('auto') # suspected time kill\n",
        "    \n",
        "    \n",
        "    \n",
        "    print('10') #5s\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Obtain L\n",
        "    # Obtain RXtRX\n",
        "    L = da.from_array(L, chunks=(100, 400, 400))\n",
        "    \n",
        "    \n",
        "    print('11')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Obtain u estimates\n",
        "    CuminusRZXbetahat = Cu - da.matmul(RZX, betahat)\n",
        "    \n",
        "    #Get betahat\n",
        "    uhat = da.apply_gufunc(np.linalg.solve,  \"(i,j),(i,k)->(j,k)\", L.transpose(0,2,1), CuminusRZXbetahat, vectorize=True,output_dtypes=RXtRX.dtype).rechunk('auto')\n",
        "    \n",
        "    print('12')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    print(L.shape)\n",
        "    print(CuminusRZXbetahat.shape)\n",
        "    print(uhat.shape)\n",
        "    \n",
        "    # Permute U from the left to make the correct uhat\n",
        "    uhat = uhat[:,P_np,:]\n",
        "\n",
        "    # Obtain b estimates\n",
        "    print(Lambda.shape)\n",
        "    print(uhat.shape)\n",
        "    bhat = da.matmul(Lambda,uhat)\n",
        "\n",
        "    # Obtain residuals sum of squares\n",
        "    resss = YtY-2*da.matmul(YtX,betahat)-2*da.matmul(YtZ,bhat)+2*da.matmul(da.matmul(betahat.transpose(0,2,1),XtZ),bhat)+da.matmul(betahat.transpose(0,2,1),da.matmul(XtX,betahat))+da.matmul(bhat.transpose(0,2,1),da.matmul(ZtZ,bhat))\n",
        "\n",
        "    # Obtain penalised residual sum of squares\n",
        "    pss = (resss + da.matmul(uhat.transpose(0,2,1),uhat)).compute()\n",
        "\n",
        "    print('13')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Obtain Log(|L|^2)\n",
        "    #logdet = 2*da.trace(da.log(L))\n",
        "    logdet = 2*np.sum(np.log(np.diagonal(L.compute(),axis1=1,axis2=2)))\n",
        "\n",
        "    print('14')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Obtain log likelihood\n",
        "    logllh = -logdet/2-n/2*(1+da.log(2*np.pi*pss)-np.log(n))\n",
        "\n",
        "    print('15')\n",
        "    print(time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    return(-logllh)\n",
        "\n",
        "# Make the initial theta estimate\n",
        "theta0 = init_theta(nv,nparams)\n",
        "\n",
        "# Obtain the mapping indices\n",
        "tinds, rinds, cinds, cinds_permuted, vinds = get_3D_mapping(nlevels, nparams, nv, P)\n",
        "\n",
        "t1 = time.time()\n",
        "llhmap = PLS_broadcasted(theta0, ZtX_da, ZtY_da, XtX_da, ZtZ_da, XtY_da, YtX_da, YtZ_da, XtZ_da, YtY_da, P, n, tinds, rinds, cinds, cinds_permuted, vinds)\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)\n",
        "print((t2-t1)*1000*100/(60*60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb75nTHT6R6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cvxopt2Scipy(M):\n",
        "  \n",
        "  Vdim = M.V.size[0]*M.V.size[1]\n",
        "  Idim = M.I.size[0]*M.I.size[1]\n",
        "  Jdim = M.J.size[0]*M.J.size[1]\n",
        "  V = np.array(M.V).reshape(Vdim)\n",
        "  I = np.array(M.I).reshape(Idim)\n",
        "  J = np.array(M.J).reshape(Jdim)\n",
        "  \n",
        "  return(scipy.sparse.coo_matrix((V,(I,J))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9-EHSWWr9NP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PLS_broadcasted2(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, n, tinds, rinds, cinds, cinds_permuted, vinds):\n",
        "    print('running_it')\n",
        "    #ZtX_ov = cvxopt.matrix(ZtX[0,:,:]) \n",
        "    #ZtY_ov = cvxopt.matrix(ZtY[10,:,:])\n",
        "    #XtX_ov = cvxopt.matrix(XtX[0,:,:])\n",
        "    #ZtZ_ov = cvxopt.sparse(cvxopt.matrix(ZtZ[0,:,:]))\n",
        "    #XtY_ov = cvxopt.matrix(XtY[10,:,:])\n",
        "    #YtX_ov = cvxopt.matrix(YtX[10,:,:])\n",
        "    #YtZ_ov = cvxopt.matrix(YtZ[10,:,:])\n",
        "    #XtZ_ov = cvxopt.matrix(XtZ[0,:,:])\n",
        "    #YtY_ov = cvxopt.matrix(YtY[10,:,:])\n",
        "    \n",
        "    #t1 = time.time()\n",
        "    # Obtain Lambda and Lambda*P\n",
        "    Lambda = mapping_3D(theta[:], vinds, tinds, rinds, cinds)\n",
        "    #LambdaP = mapping_3D(theta[:], vinds, tinds, rinds, cinds)\n",
        "    \n",
        "    #theta_ov = theta[0:(len(theta)//ZtY.shape[0])]\n",
        "    #tinds_ov,rinds_ov,cinds_ov=get_mapping(nlevels, nparams)\n",
        "    #Lambda_ov = mapping(theta_ov, tinds_ov, rinds_ov, cinds_ov)\n",
        "    #print('lambda check')\n",
        "    #print(type(cvxopt2Scipy(Lambda_ov)))\n",
        "    #print(type(Lambda[10,:,:]))\n",
        "    #print((cvxopt2Scipy(Lambda_ov) - Lambda[10,:,:].to_scipy_sparse()).toarray())\n",
        "    \n",
        "    \n",
        "    #print('1')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain Lambda'\n",
        "    Lambdat = sparse.COO(Lambda.transpose((0,2,1)))\n",
        "    #Lambdat_ov = spmatrix.trans(Lambda_ov)\n",
        "    #print('lambda transpose check')\n",
        "    #print((cvxopt2Scipy(Lambdat_ov) - Lambdat[10,:,:].to_scipy_sparse()).toarray())\n",
        "    \n",
        "    #print('2')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain Lambda'Z'Y and Lambda'Z'X\n",
        "    LambdatZtY = np.matmul(Lambdat,ZtY)\n",
        "    LambdatZtX = np.matmul(Lambdat,ZtX)\n",
        "    \n",
        "    #LambdatZtY_ov = Lambdat_ov*ZtY_ov\n",
        "    #LambdatZtX_ov = Lambdat_ov*ZtX_ov\n",
        "    #print('Lambda ZtY check')\n",
        "    #print(type(LambdatZtY[10,:,:]))\n",
        "    #print(type(LambdatZtY_ov))\n",
        "    #print(np.array(LambdatZtY_ov)-np.array(LambdatZtY[10,:,:]))\n",
        "    #print('Lambda ZtX check')\n",
        "    #print(type(LambdatZtX[10,:,:]))\n",
        "    #print(type(LambdatZtX_ov))\n",
        "    #print(np.array(LambdatZtX_ov)-np.array(LambdatZtX[10,:,:]))\n",
        "    \n",
        "\n",
        "    #print('3')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    ZtZLambda = np.matmul(ZtZ,Lambda)\n",
        "    #ZtZLambda_ov = ZtZ_ov*Lambda_ov\n",
        "    #print(np.array(ZtZLambda[10,:,:])-np.array(cvxopt.matrix(ZtZLambda_ov)))\n",
        "    #print(type(ZtZLambda))\n",
        "    #print(type(ZtZLambda_ov))\n",
        "    # Obtain the cholesky decomposition \n",
        "    LambdatZtZLambda = np.matmul(np.matmul(Lambdat,ZtZ),Lambda)\n",
        "    #LambdatZtZLambda_ov = Lambdat_ov*(ZtZ_ov*Lambda_ov)\n",
        "    I = np.eye(Lambda.shape[1])\n",
        "    \n",
        "    \n",
        "    #print('Lambdat ZtZ Lambda check')\n",
        "    #print(np.array(LambdatZtZLambda[10,:,:])-np.array(cvxopt.matrix(LambdatZtZLambda_ov)))\n",
        "    \n",
        "    #print('4')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    #print((LambdatZtZLambda+I).shape)\n",
        "    \n",
        "    # Get P in numpy format\n",
        "    P_np = np.array(P[:]).reshape(P.size[0])\n",
        "    \n",
        "    LambdatZtXP = LambdatZtX[:,P_np,:]\n",
        "    LambdatZtYP = LambdatZtY[:,P_np,:]\n",
        "    \n",
        "    #LambdatZtXP_ov = LambdatZtX_ov[P,:]\n",
        "    #LambdatZtYP_ov = LambdatZtY_ov[P,:]\n",
        "    \n",
        "    #print('permuted ZtX check')\n",
        "    #print(np.array(LambdatZtXP[10,:,:])-np.array(LambdatZtXP_ov))\n",
        "    #print('permuted ZtY check')\n",
        "    #print(np.array(LambdatZtYP[10,:,:])-np.array(LambdatZtYP_ov))\n",
        "    \n",
        "    #print('5')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    #LambdatZtYP = LambdatZtY[:,P_np,:].compute() # 86 seconds? Weird\n",
        "    \n",
        "    \n",
        "    #print('6')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Add identity to Lambda'Z'ZLambda\n",
        "    LambdatZtZLambdaplusI = LambdatZtZLambda + I\n",
        "    \n",
        "    #print('7')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Initialize empty arrays to store RZX and Cu\n",
        "    RZX = np.zeros((Lambda.shape[0], ZtZ.shape[1], XtX.shape[1]))\n",
        "    Cu = np.zeros((Lambda.shape[0], ZtZ.shape[1], 1))\n",
        "    L = np.zeros((Lambda.shape[0], ZtZ.shape[1], ZtZ.shape[1])) # All fast - 0.4s-ish\n",
        "    \n",
        "    #print('8')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    for i in np.arange(Lambda.shape[0]):\n",
        "      #print(i)\n",
        "      \n",
        "      # Perform sparse cholesky on lambda'Z'Zlambda + I\n",
        "      #print(type(LambdatZtZLambdaplusI[i,:,:]))\n",
        "      chol_dict = sparse_chol(cvxopt.sparse(matrix(LambdatZtZLambdaplusI[i,:,:])), perm=P, retF=True, retP=False, retL=False)\n",
        "      F = chol_dict['F']\n",
        "      \n",
        "      # Obtain Cu\n",
        "      Cu_tmp = matrix(LambdatZtYP[i,:,:])\n",
        "      cholmod.solve(F,Cu_tmp,sys=4)\n",
        "      \n",
        "      # Save Cu\n",
        "      Cu[i,:,:] = np.array(Cu_tmp)\n",
        "      \n",
        "      # Obtain RZX\n",
        "      RZXtmp = matrix(LambdatZtXP[i,:,:])\n",
        "      cholmod.solve(F,RZXtmp,sys=4)\n",
        "      \n",
        "      # Save RZX\n",
        "      RZX[i,:,:] = np.array(RZXtmp)\n",
        "    \n",
        "      # Obtain L (for later - note: has to be done last as this changes F)\n",
        "      Ltmp=cholmod.getfactor(F)\n",
        "      L[i,:,:] = np.array(matrix(Ltmp))\n",
        "      \n",
        "      \n",
        "      \n",
        "      #if i == 10:\n",
        "        \n",
        "      #  print(ZtZ.shape)\n",
        "      #  print(ZtZ_ov.size)\n",
        "      #  I_ov = spmatrix(1.0, range(ZtZ_ov.size[1]), range(ZtZ_ov.size[1]))\n",
        "\n",
        "      #  print(type(LambdatZtZLambda_ov+I_ov))\n",
        "      #  print((LambdatZtZLambda_ov+I_ov).size)\n",
        "      #  chol_dict_ov = sparse_chol(LambdatZtZLambda_ov+I_ov, perm=P, retF=True, retP=False, retL=False)\n",
        "      #  F_ov = chol_dict_ov['F']\n",
        "        \n",
        "      #  Cu_ov = LambdatZtY_ov[P,:]\n",
        "      #  cholmod.solve(F_ov,Cu_ov,sys=4)\n",
        "        \n",
        "      #  print('Cu check')\n",
        "      #  print(Cu[10,:,:]-Cu_ov)\n",
        "        \n",
        "      #  RZX_ov = LambdatZtX_ov[P,:]\n",
        "      #  cholmod.solve(F_ov,RZX_ov,sys=4)\n",
        "        \n",
        "      #  print('RZX check')\n",
        "      #  print(RZX[10,:,:]-RZX_ov)\n",
        "        \n",
        "    \n",
        "    #print('9')# Shockingly fast - 23.156267166137695\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "      \n",
        "    # Obtain RXtRX\n",
        "    RXtRX = XtX - np.matmul(RZX.transpose(0,2,1),RZX)\n",
        "    \n",
        "    \n",
        "    #RXtRX_ov = XtX_ov - matrix.trans(RZX_ov)*RZX_ov\n",
        "\n",
        "    #print('RXtRX check')\n",
        "    #print(RXtRX_ov-RXtRX[10,:,:])\n",
        "    \n",
        "    # Obtain beta estimates \n",
        "    XtYminusRZXtCu = XtY - np.matmul(RZX.transpose(0,2,1),Cu)\n",
        "    \n",
        "    #XtYminusRZXtCu_ov = XtY_ov - matrix.trans(RZX_ov)*Cu_ov\n",
        "    \n",
        "    #print('XtY - RZXtCu check')\n",
        "    #print(XtYminusRZXtCu_ov - XtYminusRZXtCu[10,:,:])\n",
        "    \n",
        "    #Get betahat\n",
        "    betahat = np.linalg.solve(RXtRX, XtYminusRZXtCu)\n",
        "    \n",
        "    \n",
        "    #betahat_ov = XtY_ov - matrix.trans(RZX_ov)*Cu_ov\n",
        "    #lapack.posv(RXtRX_ov, betahat_ov)\n",
        "    \n",
        "    #print('betahat check')\n",
        "    #print(betahat_ov-betahat[10,:,:])\n",
        "    \n",
        "    #print('10') #5s\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain L\n",
        "    # Obtain RXtRX\n",
        "    \n",
        "    \n",
        "    #print('11')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain u estimates\n",
        "    CuminusRZXbetahat = Cu - np.matmul(RZX, betahat)\n",
        "    \n",
        "    #Get betahat\n",
        "    uhat = np.linalg.solve(L.transpose(0,2,1), CuminusRZXbetahat)\n",
        "    \n",
        "    \n",
        "    #uhat_ov = Cu_ov-RZX_ov*betahat_ov\n",
        "    #cholmod.solve(F_ov,uhat_ov,sys=5)\n",
        "    #cholmod.solve(F_ov,uhat_ov,sys=8)\n",
        "    \n",
        "    #print('12')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    #print(L.shape)\n",
        "    #print(CuminusRZXbetahat.shape)\n",
        "    #print(uhat.shape)\n",
        "    \n",
        "    # Permute U from the left to make the correct uhat\n",
        "    invP_np = np.arange(len(P_np))[np.argsort(P_np)]\n",
        "    uhat = uhat[:,invP_np,:]\n",
        "\n",
        "    #print('uhat check')\n",
        "    #print(uhat_ov - uhat[10,:,:])\n",
        "    \n",
        "    # Obtain b estimates\n",
        "    #print(Lambda.shape)\n",
        "    #print(uhat.shape)\n",
        "    bhat = np.matmul(Lambda,uhat)\n",
        "\n",
        "    #bhat_ov = Lambda_ov*uhat_ov\n",
        "    #print('bhat check')\n",
        "    #print(bhat_ov-bhat[10,:,:])\n",
        "    \n",
        "    # Obtain residuals sum of squares\n",
        "    resss = YtY-2*np.matmul(YtX,betahat)-2*np.matmul(YtZ,bhat)+2*np.matmul(np.matmul(betahat.transpose(0,2,1),XtZ),bhat)+np.matmul(betahat.transpose(0,2,1),np.matmul(XtX,betahat))+np.matmul(bhat.transpose(0,2,1),np.matmul(ZtZ,bhat))\n",
        "\n",
        "    #resss_ov = YtY_ov-2*YtX_ov*betahat_ov-2*YtZ_ov*bhat_ov+2*matrix.trans(betahat_ov)*XtZ_ov*bhat_ov+matrix.trans(betahat_ov)*XtX_ov*betahat_ov+matrix.trans(bhat_ov)*ZtZ_ov*bhat_ov\n",
        "   \n",
        "    #print('resss check')\n",
        "    #print(resss[10,:,:]-resss_ov)\n",
        "    \n",
        "    # Obtain penalised residual sum of squares\n",
        "    pss = resss + np.matmul(uhat.transpose(0,2,1),uhat)\n",
        "    #pss_ov = resss_ov + matrix.trans(uhat_ov)*uhat_ov\n",
        "\n",
        "    #print('pss check')\n",
        "    #print(pss[10,:,:]-pss_ov)\n",
        "    \n",
        "    #print('13')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain Log(|L|^2)\n",
        "    #logdet = 2*da.trace(da.log(L))\n",
        "    #tmp = np.log(np.diagonal(L,axis1=1,axis2=2))\n",
        "    #print(tmp.shape)\n",
        "    #print(np.sum(tmp).shape)\n",
        "    #print(np.sum(tmp, axis=0).shape)\n",
        "    #print(np.sum(tmp, axis=1).shape)\n",
        "    #print(2*np.sum(np.log(np.diagonal(L,axis1=1,axis2=2)),axis=1).shape)\n",
        "    logdet = 2*np.sum(np.log(np.diagonal(L,axis1=1,axis2=2)),axis=1).reshape((nv,1,1))\n",
        "    #logdet_ov = 2*sum(cvxopt.log(cholmod.diag(F_ov)))\n",
        "    #print(logdet)\n",
        "    #print(logdet.shape)\n",
        "    #print('logdet check')\n",
        "    #print(logdet[10]-logdet_ov)\n",
        "    \n",
        "    #print('14')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain log likelihood\n",
        "    logllh = -logdet/2-n/2*(1+np.log(2*np.pi*pss)-np.log(n))\n",
        "\n",
        "    #print('15')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    return(np.amax(-logllh))\n",
        "\n",
        "    #return(-logllh)\n",
        "  \n",
        "# Make the initial theta estimate\n",
        "theta0 = init_theta(nv,nparams)\n",
        "\n",
        "# Obtain the mapping indices\n",
        "tinds, rinds, cinds, cinds_permuted, vinds = get_3D_mapping(nlevels, nparams, nv, P)\n",
        "\n",
        "t1 = time.time()\n",
        "llhmap = PLS_broadcasted2(theta0, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, n, tinds, rinds, cinds, cinds_permuted, vinds)\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)\n",
        "print((t2-t1)*100*100*100/(nv*60*60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQC-daPsD8wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(llhmap.shape)\n",
        "llhmap_imageformat = llhmap.reshape((dimv[0],dimv[1],dimv[2]))\n",
        "\n",
        "imshow(llhmap_imageformat[1,:,:].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')\n",
        "\n",
        "plt.colorbar()\n",
        "\n",
        "print((t2-t1)*10/3*10/3*10/3*100/(60*60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9krmSRHV47jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(type(theta0))\n",
        "print(len(theta0)/nv)\n",
        "\n",
        "theta0tmp = theta0[0:(len(theta0)//nv)]\n",
        "\n",
        "# Obtain a random Lambda matrix with the correct sparsity for the permutation vector\n",
        "tindstmp,rindstmp,cindstmp=get_mapping(nlevels, nparams)\n",
        "\n",
        "t1 = time.time()\n",
        "llh2map = np.zeros(nv)\n",
        "\n",
        "for i in np.arange(nv):\n",
        "  #print(i)\n",
        "  XtYtmp = matrix(XtY[i,:,:]) \n",
        "  ZtYtmp = matrix(ZtY[i,:,:]) \n",
        "  YtYtmp = matrix(YtY[i,:,:]) \n",
        "  YtZtmp = matrix(YtZ[i,:,:])\n",
        "  YtXtmp = matrix(YtX[i,:,:])\n",
        "  \n",
        "  llh2map[i] = PLS(theta0tmp, ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tindstmp, rindstmp, cindstmp)\n",
        "\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)\n",
        "print((t2-t1)*10/3*10/3*10/3*100/(60*60))\n",
        "  \n",
        "print(llh2map.shape)\n",
        "llh2map_imageformat = llh2map.reshape((dimv[0],dimv[1],dimv[2]))\n",
        "\n",
        "imshow(llh2map_imageformat[1,:,:].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuHjWoarZI7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = time.time()\n",
        "minimize(PLS_broadcasted3, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, n, tinds, rinds, cinds, cinds_permuted, vinds), method='L-BFGS-B', tol=1e-6)\n",
        "print((time.time()-t1)*100*100*100/(nv*60*60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy6lhX5mjgLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PLS_broadcasted3(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, n, tinds, rinds, cinds, cinds_permuted, vinds):\n",
        "    print('running_it')\n",
        "    #ZtX_ov = cvxopt.matrix(ZtX[0,:,:]) \n",
        "    #ZtY_ov = cvxopt.matrix(ZtY[10,:,:])\n",
        "    #XtX_ov = cvxopt.matrix(XtX[0,:,:])\n",
        "    #ZtZ_ov = cvxopt.sparse(cvxopt.matrix(ZtZ[0,:,:]))\n",
        "    #XtY_ov = cvxopt.matrix(XtY[10,:,:])\n",
        "    #YtX_ov = cvxopt.matrix(YtX[10,:,:])\n",
        "    #YtZ_ov = cvxopt.matrix(YtZ[10,:,:])\n",
        "    #XtZ_ov = cvxopt.matrix(XtZ[0,:,:])\n",
        "    #YtY_ov = cvxopt.matrix(YtY[10,:,:])\n",
        "    \n",
        "    #t1 = time.time()\n",
        "    # Obtain Lambda and Lambda*P\n",
        "    Lambda = mapping_3D(theta[:], vinds, tinds, rinds, cinds)\n",
        "    #LambdaP = mapping_3D(theta[:], vinds, tinds, rinds, cinds)\n",
        "    \n",
        "    #theta_ov = theta[0:(len(theta)//ZtY.shape[0])]\n",
        "    #tinds_ov,rinds_ov,cinds_ov=get_mapping(nlevels, nparams)\n",
        "    #Lambda_ov = mapping(theta_ov, tinds_ov, rinds_ov, cinds_ov)\n",
        "    #print('lambda check')\n",
        "    #print(type(cvxopt2Scipy(Lambda_ov)))\n",
        "    #print(type(Lambda[10,:,:]))\n",
        "    #print((cvxopt2Scipy(Lambda_ov) - Lambda[10,:,:].to_scipy_sparse()).toarray())\n",
        "    \n",
        "    \n",
        "    #print('1')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain Lambda'\n",
        "    Lambdat = sparse.COO(Lambda.transpose((0,2,1)))\n",
        "    #Lambdat_ov = spmatrix.trans(Lambda_ov)\n",
        "    #print('lambda transpose check')\n",
        "    #print((cvxopt2Scipy(Lambdat_ov) - Lambdat[10,:,:].to_scipy_sparse()).toarray())\n",
        "    \n",
        "    #print('2')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain Lambda'Z'Y and Lambda'Z'X\n",
        "    LambdatZtY = np.matmul(Lambdat,ZtY)\n",
        "    LambdatZtX = np.matmul(Lambdat,ZtX)\n",
        "    \n",
        "    #LambdatZtY_ov = Lambdat_ov*ZtY_ov\n",
        "    #LambdatZtX_ov = Lambdat_ov*ZtX_ov\n",
        "    #print('Lambda ZtY check')\n",
        "    #print(type(LambdatZtY[10,:,:]))\n",
        "    #print(type(LambdatZtY_ov))\n",
        "    #print(np.array(LambdatZtY_ov)-np.array(LambdatZtY[10,:,:]))\n",
        "    #print('Lambda ZtX check')\n",
        "    #print(type(LambdatZtX[10,:,:]))\n",
        "    #print(type(LambdatZtX_ov))\n",
        "    #print(np.array(LambdatZtX_ov)-np.array(LambdatZtX[10,:,:]))\n",
        "    \n",
        "\n",
        "    #print('3')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    #print(np.array(ZtZLambda[10,:,:])-np.array(cvxopt.matrix(ZtZLambda_ov)))\n",
        "    #print(type(ZtZLambda))\n",
        "    #print(type(ZtZLambda_ov))\n",
        "    # Obtain the cholesky decomposition \n",
        "    LambdatZtZLambda = np.matmul(np.matmul(Lambdat,ZtZ),Lambda)\n",
        "    #LambdatZtZLambda_ov = Lambdat_ov*(ZtZ_ov*Lambda_ov)\n",
        "    I = np.eye(Lambda.shape[1])\n",
        "    \n",
        "    \n",
        "    #print('Lambdat ZtZ Lambda check')\n",
        "    #print(np.array(LambdatZtZLambda[10,:,:])-np.array(cvxopt.matrix(LambdatZtZLambda_ov)))\n",
        "    \n",
        "    #print('4')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    #print((LambdatZtZLambda+I).shape)\n",
        "    \n",
        "    # Get P in numpy format\n",
        "    #P_np = np.array(P[:]).reshape(P.size[0])\n",
        "    \n",
        "    #LambdatZtXP = LambdatZtX[:,P_np,:]\n",
        "    #LambdatZtYP = LambdatZtY[:,P_np,:]\n",
        "    \n",
        "    #LambdatZtXP_ov = LambdatZtX_ov[P,:]\n",
        "    #LambdatZtYP_ov = LambdatZtY_ov[P,:]\n",
        "    \n",
        "    #print('permuted ZtX check')\n",
        "    #print(np.array(LambdatZtXP[10,:,:])-np.array(LambdatZtXP_ov))\n",
        "    #print('permuted ZtY check')\n",
        "    #print(np.array(LambdatZtYP[10,:,:])-np.array(LambdatZtYP_ov))\n",
        "    \n",
        "    #print('5')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    #LambdatZtYP = LambdatZtY[:,P_np,:].compute() # 86 seconds? Weird\n",
        "    \n",
        "    \n",
        "    #print('6')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Add identity to Lambda'Z'ZLambda\n",
        "    LambdatZtZLambdaplusI = LambdatZtZLambda + I\n",
        "    \n",
        "    #print('7')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    #print('8')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    #print(type(LambdatZtZLambdaplusI[:,P_np,:][:,:,P_np]))\n",
        "    #print(LambdatZtZLambdaplusI[:,P_np,:][:,:,P_np].shape)\n",
        "    L = np.linalg.cholesky(LambdatZtZLambdaplusI)#[:,P_np,:][:,:,P_np])\n",
        "    Cu = np.linalg.solve(L,LambdatZtY)#P)\n",
        "    RZX = np.linalg.solve(L,LambdatZtX)#P)\n",
        "    \n",
        "    #for i in np.arange(Lambda.shape[0]):\n",
        "      #print(i)\n",
        "      \n",
        "      # Perform sparse cholesky on lambda'Z'Zlambda + I\n",
        "      #print(type(LambdatZtZLambdaplusI[i,:,:]))\n",
        "      #chol_dict = sparse_chol(cvxopt.sparse(matrix(LambdatZtZLambdaplusI[i,:,:])), perm=P, retF=True, retP=False, retL=False)\n",
        "      #F = chol_dict['F']\n",
        "      \n",
        "      # Obtain Cu\n",
        "      #Cu_tmp = matrix(LambdatZtYP[i,:,:])\n",
        "      #cholmod.solve(F,Cu_tmp,sys=4)\n",
        "      \n",
        "      # Save Cu\n",
        "      #Cu[i,:,:] = np.array(Cu_tmp)\n",
        "      \n",
        "      # Obtain RZX\n",
        "      #RZXtmp = matrix(LambdatZtXP[i,:,:])\n",
        "      #cholmod.solve(F,RZXtmp,sys=4)\n",
        "      \n",
        "      # Save RZX\n",
        "      #RZX[i,:,:] = np.array(RZXtmp)\n",
        "    \n",
        "      # Obtain L (for later - note: has to be done last as this changes F)\n",
        "      #Ltmp=cholmod.getfactor(F)\n",
        "      #L[i,:,:] = np.array(matrix(Ltmp))\n",
        "      \n",
        "      \n",
        "      \n",
        "      #if i == 10:\n",
        "        \n",
        "      #  print(ZtZ.shape)\n",
        "      #  print(ZtZ_ov.size)\n",
        "      #  I_ov = spmatrix(1.0, range(ZtZ_ov.size[1]), range(ZtZ_ov.size[1]))\n",
        "\n",
        "      #  print(type(LambdatZtZLambda_ov+I_ov))\n",
        "      #  print((LambdatZtZLambda_ov+I_ov).size)\n",
        "      #  chol_dict_ov = sparse_chol(LambdatZtZLambda_ov+I_ov, perm=P, retF=True, retP=False, retL=False)\n",
        "      #  F_ov = chol_dict_ov['F']\n",
        "        \n",
        "      #  Cu_ov = LambdatZtY_ov[P,:]\n",
        "      #  cholmod.solve(F_ov,Cu_ov,sys=4)\n",
        "        \n",
        "      #  print('Cu check')\n",
        "      #  print(Cu[10,:,:]-Cu_ov)\n",
        "        \n",
        "      #  RZX_ov = LambdatZtX_ov[P,:]\n",
        "      #  cholmod.solve(F_ov,RZX_ov,sys=4)\n",
        "        \n",
        "      #  print('RZX check')\n",
        "      #  print(RZX[10,:,:]-RZX_ov)\n",
        "        \n",
        "    \n",
        "    #print('9')# Shockingly fast - 23.156267166137695\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "      \n",
        "    # Obtain RXtRX\n",
        "    RXtRX = XtX - np.matmul(RZX.transpose(0,2,1),RZX)\n",
        "    \n",
        "    \n",
        "    #RXtRX_ov = XtX_ov - matrix.trans(RZX_ov)*RZX_ov\n",
        "\n",
        "    #print('RXtRX check')\n",
        "    #print(RXtRX_ov-RXtRX[10,:,:])\n",
        "    \n",
        "    # Obtain beta estimates \n",
        "    XtYminusRZXtCu = XtY - np.matmul(RZX.transpose(0,2,1),Cu)\n",
        "    \n",
        "    #XtYminusRZXtCu_ov = XtY_ov - matrix.trans(RZX_ov)*Cu_ov\n",
        "    \n",
        "    #print('XtY - RZXtCu check')\n",
        "    #print(XtYminusRZXtCu_ov - XtYminusRZXtCu[10,:,:])\n",
        "    \n",
        "    #Get betahat\n",
        "    betahat = np.linalg.solve(RXtRX, XtYminusRZXtCu)\n",
        "    \n",
        "    \n",
        "    #betahat_ov = XtY_ov - matrix.trans(RZX_ov)*Cu_ov\n",
        "    #lapack.posv(RXtRX_ov, betahat_ov)\n",
        "    \n",
        "    #print('betahat check')\n",
        "    #print(betahat_ov-betahat[10,:,:])\n",
        "    \n",
        "    #print('10') #5s\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain L\n",
        "    # Obtain RXtRX\n",
        "    \n",
        "    \n",
        "    #print('11')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain u estimates\n",
        "    CuminusRZXbetahat = Cu - np.matmul(RZX, betahat)\n",
        "    \n",
        "    #Get betahat\n",
        "    uhat = np.linalg.solve(L.transpose(0,2,1), CuminusRZXbetahat)\n",
        "    \n",
        "    \n",
        "    #uhat_ov = Cu_ov-RZX_ov*betahat_ov\n",
        "    #cholmod.solve(F_ov,uhat_ov,sys=5)\n",
        "    #cholmod.solve(F_ov,uhat_ov,sys=8)\n",
        "    \n",
        "    #print('12')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    #print(L.shape)\n",
        "    #print(CuminusRZXbetahat.shape)\n",
        "    #print(uhat.shape)\n",
        "    \n",
        "    # Permute U from the left to make the correct uhat\n",
        "    #invP_np = np.arange(len(P_np))[np.argsort(P_np)]\n",
        "    #uhat = uhat[:,invP_np,:]\n",
        "\n",
        "    #print('uhat check')\n",
        "    #print(uhat_ov - uhat[10,:,:])\n",
        "    \n",
        "    # Obtain b estimates\n",
        "    #print(Lambda.shape)\n",
        "    #print(uhat.shape)\n",
        "    bhat = np.matmul(Lambda,uhat)\n",
        "\n",
        "    #bhat_ov = Lambda_ov*uhat_ov\n",
        "    #print('bhat check')\n",
        "    #print(bhat_ov-bhat[10,:,:])\n",
        "    \n",
        "    # Obtain residuals sum of squares\n",
        "    resss = YtY-2*np.matmul(YtX,betahat)-2*np.matmul(YtZ,bhat)+2*np.matmul(np.matmul(betahat.transpose(0,2,1),XtZ),bhat)+np.matmul(betahat.transpose(0,2,1),np.matmul(XtX,betahat))+np.matmul(bhat.transpose(0,2,1),np.matmul(ZtZ,bhat))\n",
        "\n",
        "    #resss_ov = YtY_ov-2*YtX_ov*betahat_ov-2*YtZ_ov*bhat_ov+2*matrix.trans(betahat_ov)*XtZ_ov*bhat_ov+matrix.trans(betahat_ov)*XtX_ov*betahat_ov+matrix.trans(bhat_ov)*ZtZ_ov*bhat_ov\n",
        "   \n",
        "    #print('resss check')\n",
        "    #print(resss[10,:,:]-resss_ov)\n",
        "    \n",
        "    # Obtain penalised residual sum of squares\n",
        "    pss = resss + np.matmul(uhat.transpose(0,2,1),uhat)\n",
        "    #pss_ov = resss_ov + matrix.trans(uhat_ov)*uhat_ov\n",
        "\n",
        "    #print('pss check')\n",
        "    #print(pss[10,:,:]-pss_ov)\n",
        "    \n",
        "    #print('13')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain Log(|L|^2)\n",
        "    #logdet = 2*da.trace(da.log(L))\n",
        "    #tmp = np.log(np.diagonal(L,axis1=1,axis2=2))\n",
        "    #print(tmp.shape)\n",
        "    #print(np.sum(tmp).shape)\n",
        "    #print(np.sum(tmp, axis=0).shape)\n",
        "    #print(np.sum(tmp, axis=1).shape)\n",
        "    #print(2*np.sum(np.log(np.diagonal(L,axis1=1,axis2=2)),axis=1).shape)\n",
        "    logdet = 2*np.sum(np.log(np.diagonal(L,axis1=1,axis2=2)),axis=1).reshape((nv,1,1))\n",
        "    #logdet_ov = 2*sum(cvxopt.log(cholmod.diag(F_ov)))\n",
        "    #print(logdet)\n",
        "    #print(logdet.shape)\n",
        "    #print('logdet check')\n",
        "    #print(logdet[10]-logdet_ov)\n",
        "    \n",
        "    #print('14')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    # Obtain log likelihood\n",
        "    logllh = -logdet/2-n/2*(1+np.log(2*np.pi*pss)-np.log(n))\n",
        "\n",
        "    #print('15')\n",
        "    #print(time.time()-t1)\n",
        "    #t1 = time.time()\n",
        "    \n",
        "    #return(np.amax(-logllh))\n",
        "\n",
        "    return(-logllh)\n",
        "  \n",
        "# Make the initial theta estimate\n",
        "theta0 = init_theta(nv,nparams)\n",
        "\n",
        "# Obtain the mapping indices\n",
        "tinds, rinds, cinds, cinds_permuted, vinds = get_3D_mapping(nlevels, nparams, nv, P)\n",
        "\n",
        "t1 = time.time()\n",
        "llhmap = PLS_broadcasted3(theta0, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, n, tinds, rinds, cinds, cinds_permuted, vinds)\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)\n",
        "print((t2-t1)*100*100*100/(nv*60*60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5piUAH8ckqKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(llhmap.shape)\n",
        "llhmap_imageformat = llhmap.reshape((dimv[0],dimv[1],dimv[2]))\n",
        "\n",
        "imshow(llhmap_imageformat[1,:,:].reshape(dimv[0],dimv[1]), \\\n",
        "                    interpolation='nearest', aspect='auto')\n",
        "\n",
        "plt.colorbar()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR2epLIRi95N",
        "colab_type": "text"
      },
      "source": [
        "### Idea 4: Large sparse diagonals\n",
        "\n",
        "Below is a brief concept check... it didn't work consistently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZNddOmJi-OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concept check\n",
        "LamtZtZLam2 = LamtZtZLam\n",
        "t1 = time.time()\n",
        "for i in np.arange(100):\n",
        "  LamtZtZLam2 = cvxopt.spdiag([LamtZtZLam2,LamtZtZLam])\n",
        "f=sparse_chol(LamtZtZLam2)\n",
        "t2 = time.time()\n",
        "print(t2-t1)\n",
        "\n",
        "t1 = time.time()\n",
        "for i in np.arange(101):\n",
        "  f=sparse_chol(LamtZtZLam)\n",
        "t2 = time.time()\n",
        "print(t2-t1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNtb5-TdAal",
        "colab_type": "text"
      },
      "source": [
        "### Idea 5: Smooth between iterations\n",
        "\n",
        "This idea is relatively simple. Specify a number of smoothing steps, say 10 and, at regular intervals between 1 and \"tol\" stop the iteration, smooth the estimates and then continue. This is inspired by the observation in idea 2 that reusing estimates between voxels did have some limited positive effect on time efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXkSM5mmc_gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize temporary X'X, X'Z, Z'X and Z'Z\n",
        "XtZtmp = matrix(XtZ[0,:,:])\n",
        "ZtXtmp = matrix(ZtX[0,:,:])\n",
        "ZtZtmp = cvxopt.sparse(matrix(ZtZ[0,:,:]))\n",
        "XtXtmp = matrix(XtX[0,:,:])\n",
        "\n",
        "# Initial theta value. Bates (2005) suggests using [vech(I_q1),...,vech(I_qr)] where I is the identity matrix\n",
        "theta0 = np.array([])\n",
        "for i in np.arange(r):\n",
        "  theta0 = np.hstack((theta0, mat2vech(np.eye(nparams[i])).reshape(np.int64(nparams[i]*(nparams[i]+1)/2))))\n",
        "\n",
        "# Initialize empty estimates\n",
        "beta_est = np.zeros(beta.shape)\n",
        "theta_est_3D = np.zeros((beta.shape[0], theta0.shape[0]))\n",
        "\n",
        "# Obtain a random Lambda matrix with the correct sparsity for the permutation vector\n",
        "tinds,rinds,cinds=get_mapping(nlevels, nparams)\n",
        "Lam=mapping(np.random.randn(theta0.shape[0]),tinds,rinds,cinds)\n",
        "\n",
        "# Obtain Lambda'Z'ZLambda\n",
        "LamtZtZLam = spmatrix.trans(Lam)*cvxopt.sparse(matrix(ZtZtmp))*Lam\n",
        "\n",
        "\n",
        "# Identity (Actually quicker to calculate outside of estimation)\n",
        "I = spmatrix(1.0, range(Lam.size[0]), range(Lam.size[0]))\n",
        "\n",
        "# Obtaining permutation for PLS\n",
        "P=cvxopt.amd.order(LamtZtZLam)\n",
        "\n",
        "tolArray = np.array([1e-2,1e-4,1e-6])\n",
        "\n",
        "for tolVal in tolArray:\n",
        "  \n",
        "  t1 = time.time()\n",
        "  for i in np.arange(nv):\n",
        "\n",
        "    print(i)\n",
        "\n",
        "    XtYtmp = matrix(XtY[i,:,:]) \n",
        "    ZtYtmp = matrix(ZtY[i,:,:]) \n",
        "    YtYtmp = matrix(YtY[i,:,:]) \n",
        "    YtZtmp = matrix(YtZ[i,:,:])\n",
        "    YtXtmp = matrix(YtX[i,:,:])\n",
        "\n",
        "    if tolVal==tolArray[0]:\n",
        "      theta_est = minimize(PLS, theta0, args=(ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=tolVal)\n",
        "    else:\n",
        "      theta_est = minimize(PLS, theta_est_3D[i,:], args=(ZtXtmp, ZtYtmp, XtXtmp, ZtZtmp, XtYtmp, YtXtmp, YtZtmp, XtZtmp, YtYtmp, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=tolVal)\n",
        "      \n",
        "    nit = theta_est.nit\n",
        "    theta_est_3D[i,:] = theta_est.x\n",
        "    print(nit)\n",
        "\n",
        "  t2 = time.time()\n",
        "  print(\"Time taken in seconds for this tol:\" + str(tolVal))\n",
        "  print(t2-t1)\n",
        "  print(\"Estimated time taken for this example on a nifti of size (100x100x100), in hours:\")\n",
        "  print(100*100*100*(t2-t1)/(nv*60*60))\n",
        "  \n",
        "  t1 = time.time()\n",
        "  \n",
        "  for j in np.arange(theta_est_3D.shape[1]):\n",
        "    \n",
        "    # Get the theta estimates for one theta parameter.\n",
        "    theta_est_us = theta_est_3D[:,j].reshape(dimv[0],dimv[1],dimv[2])\n",
        "    \n",
        "    # Some random affine, not hugely important\n",
        "    affine = np.diag([1, 1, 1, 1])\n",
        "    theta_est_us_nii = nib.Nifti1Image(theta_est_us, affine)\n",
        "\n",
        "    # Smoothed theta nifti\n",
        "    theta_est_s_nii = nilearn.image.smooth_img(theta_est_us_nii, 5)\n",
        "\n",
        "    # Final theta estimate\n",
        "    theta_est_3D[:,j] = theta_est_s_nii.get_fdata().reshape(theta_est_3D[:,j].shape)\n",
        "  \n",
        "  t2 = time.time()\n",
        "  \n",
        "  print(\"Time taken smoothing\")\n",
        "  print(t2-t1)\n",
        "  print(\"Estimated time taken for this smoothing on a nifti of size (100x100x100), in hours:\")\n",
        "  print(100*100*100*(t2-t1)/(nv*60*60))\n",
        "  \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPT7ufJlX6jg",
        "colab_type": "text"
      },
      "source": [
        "### Sandbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54iwTXilX6CV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cvxopt.amd\n",
        "import cvxopt\n",
        "from cvxopt import cholmod\n",
        "import numpy as np\n",
        "import dask.array as da\n",
        "import dask\n",
        "import time\n",
        "import sparse\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ[\"SPARSE_AUTO_DENSIFY\"] = \"1\"\n",
        "runningnptime=np.zeros(1000)\n",
        "runningsptime=np.zeros(1000)\n",
        "runningdatime=np.zeros(1000)\n",
        "runningdaptime=np.zeros(1000)\n",
        "runningcvxtime=np.zeros(1000)\n",
        "runningcvxptime=np.zeros(1000)\n",
        "\n",
        "dask.config.set({'array.chunk-size':'20MiB'})\n",
        "for n in np.arange(800,1000):\n",
        "  for i in np.arange(1):\n",
        "    print(n)\n",
        "    print(i)\n",
        "    X = np.random.randn(n+1,n+1)\n",
        "    X[X<1.7]=0\n",
        "    X = X + np.eye(n+1)\n",
        "\n",
        "    XtX = np.matmul(X.transpose(), X)\n",
        "\n",
        "    XtX_tf = tf.constant(XtX)\n",
        "\n",
        "    XtX_cvxopt = cvxopt.sparse(cvxopt.matrix(XtX))\n",
        "    P = cvxopt.amd.order(XtX_cvxopt)\n",
        "\n",
        "    P_np = np.array(P[:]).reshape(n+1)\n",
        "\n",
        "    #print(P_np)\n",
        "    # To get the permuted X from the amd ordering must do X[:,P]\n",
        "    XtX_P = np.matmul((X[:,P_np]).transpose(),X[:,P_np])\n",
        "    XtX_tf_P = tf.constant(XtX_P)\n",
        "\n",
        "    #print(X[P_np,:].shape)\n",
        "\n",
        "    XtX_P_cvxopt = XtX_cvxopt[P,P]\n",
        "\n",
        "    #print(XtX_P)\n",
        "    #print(XtX_P_cvxopt)\n",
        "\n",
        "    #print(cvxopt.sparse(cvxopt.matrix(XtX_P))-XtX_P_cvxopt)\n",
        "\n",
        "\n",
        "    F=sparse_chol(XtX_cvxopt, perm=None, retF=False, retP=True, retL=True)\n",
        "\n",
        "    L_cvxopt_spchol = F['L']\n",
        "\n",
        "    #print(L_cvxopt_spchol*spmatrix.trans(L_cvxopt_spchol)-cvxopt.sparse(cvxopt.matrix(XtX_P)))\n",
        "\n",
        "    t1 = time.time()\n",
        "    L_np_chol = np.linalg.cholesky(XtX)\n",
        "    t2 = time.time()\n",
        "    nptime=t2-t1\n",
        "\n",
        "    t1 = time.time()\n",
        "    L_np_spchol = np.linalg.cholesky(XtX_P)\n",
        "    t2 = time.time()\n",
        "    sptime=t2-t1\n",
        "\n",
        "    XtX_da = da.from_array(XtX, chunks=\"auto\").map_blocks(sparse.COO)\n",
        "    XtX_da_P = da.from_array(XtX_P, chunks=\"auto\").map_blocks(sparse.COO)\n",
        "    print(XtX_da.chunks)\n",
        "\n",
        "    #da.map_blocks()\n",
        "\n",
        "    t1 = time.time()\n",
        "    L_dap_chol = da.linalg.cholesky(XtX_da_P).compute()\n",
        "    t2 = time.time()\n",
        "    daptime=t2-t1\n",
        "\n",
        "    t1 = time.time()\n",
        "    L_da_chol = da.linalg.cholesky(XtX_da).compute()\n",
        "    t2 = time.time()\n",
        "    datime=t2-t1\n",
        "\n",
        "    t1 = time.time()\n",
        "    L_cvxp_chol = sparse_chol(XtX_P_cvxopt, perm=None, retF=False, retP=False, retL=True)\n",
        "    t2 = time.time()\n",
        "    cvxptime=t2-t1\n",
        "\n",
        "    t1 = time.time()\n",
        "    L_cvx_chol = sparse_chol(XtX_cvxopt, perm=None, retF=False, retP=False, retL=True)\n",
        "    t2 = time.time()\n",
        "    cvxtime=t2-t1\n",
        "\n",
        "    runningnptime[n] = runningnptime[n] + nptime\n",
        "    runningsptime[n] = runningsptime[n] + sptime\n",
        "    runningdatime[n] = runningdatime[n] + datime\n",
        "    runningdaptime[n] = runningdaptime[n] + daptime\n",
        "    runningcvxtime[n] = runningcvxtime[n] + cvxtime\n",
        "    runningcvxptime[n] = runningcvxptime[n] + cvxptime\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYNnmvll3BEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#print(runningdatime)\n",
        "#print(runningdaptime)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "\n",
        "ax1.scatter(np.arange(800,1000),runningdatime[800:1000], marker=\"x\", label='Dask array without amd')\n",
        "ax1.scatter(np.arange(800,1000),runningdaptime[800:1000], marker=\"x\", label='Dask array with amd')\n",
        "ax1.scatter(np.arange(800,1000),runningcvxtime[800:1000], marker=\"x\", label='Cvxopt array without amd')\n",
        "ax1.scatter(np.arange(800,1000),runningcvxptime[800:1000], marker=\"x\", label='Cvxopt array with amd')\n",
        "ax1.scatter(np.arange(800,1000),runningnptime[800:1000], marker=\"x\", label='Numpy array without amd')\n",
        "ax1.scatter(np.arange(800,1000),runningsptime[800:1000], marker=\"x\", label='Numpy array with amd')\n",
        "plt.legend(loc='upper left');\n",
        "plt.ylim(-0.002, 0.1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFOXArMV0NX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.random.randn(10,10)\n",
        "X[X<1.7]=0\n",
        "X = X + np.eye(10)\n",
        "\n",
        "XtX = np.matmul(X.transpose(), X)\n",
        "\n",
        "XtX_cvxopt = cvxopt.sparse(cvxopt.matrix(XtX))\n",
        "P = cvxopt.amd.order(XtX_cvxopt)\n",
        "print(P)\n",
        "\n",
        "print(XtX_cvxopt.J)\n",
        "XtX_permuted = XtX[:,np.array(P[:]).reshape(P.size[0])]\n",
        "print(XtX_permuted.shape)\n",
        "XtX_permuted = scipy.sparse.csr_matrix(XtX_permuted)\n",
        "\n",
        "rowinds = XtX_cvxopt.I\n",
        "colinds = XtX_cvxopt.J\n",
        "data = XtX_cvxopt.V\n",
        "\n",
        "P = np.array(P[:]).reshape(P.size[0])\n",
        "invP = np.arange(len(P))[np.argsort(P)]\n",
        "print(invP)\n",
        "colinds_perm = invP[XtX_cvxopt.J]\n",
        "\n",
        "print(np.array(data[:]).shape)\n",
        "print(np.array(rowinds[:]).shape)\n",
        "print(np.array(colinds_perm[:]).shape)\n",
        "\n",
        "rowinds = np.array(rowinds[:])\n",
        "colinds_perm = np.array(colinds_perm[:])\n",
        "data = np.array(data[:])\n",
        "\n",
        "rowinds = rowinds.reshape(rowinds.shape[0])\n",
        "colinds_perm = colinds_perm.reshape(colinds_perm.shape[0])\n",
        "data = data.reshape(data.shape[0])\n",
        "\n",
        "XtX_permuted2 = scipy.sparse.csr_matrix((data, (rowinds, colinds_perm)))\n",
        "\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "\n",
        "print(XtX_permuted.toarray()-XtX_permuted2.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn8aEz01u2mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(XtX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hWHKvosvWY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numba\n",
        "import cvxopt\n",
        "from cvxopt import cholmod\n",
        "from cvxopt import sparse\n",
        "from cvxopt import matrix\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "@numba.jit(forceobj=True, parallel=True)\n",
        "def multicvxopt(XtX):\n",
        "  \n",
        "  np.linalg.cholesky(XtX)\n",
        "    \n",
        "    \n",
        "def spcholblock(XtX):\n",
        "  \n",
        "  tmp = LambdatZtZLambda[i,:,:].compute()\n",
        "  data = tmp.data\n",
        "  coords = tmp.coords\n",
        "  chol_dict = sparse_chol(spmatrix(data, coords[0,:],coords[1,:],size=(Lambda.shape[1],Lambda.shape[1]))+I, perm=P, retF=True, retP=False, retL=False)\n",
        "  \n",
        "#dask.array.map_blocks()\n",
        "\n",
        "XtX = np.zeros((1,1000,1000))\n",
        "for i in np.arange(1):\n",
        "    X = np.random.randn(1000,1000)\n",
        "    X[X<1.7]=0\n",
        "    X = X + np.eye(1000)\n",
        "\n",
        "    XtXi = np.matmul(X.transpose(), X)\n",
        "\n",
        "    XtX[i,:,:] = XtXi\n",
        "    \n",
        "t1 = time.time()\n",
        "multicvxopt(XtX)\n",
        "t2 = time.time()\n",
        "print(t2-t1)\n",
        "\n",
        "t1 = time.time()\n",
        "multicvxopt(XtX)\n",
        "t2 = time.time()\n",
        "print(t2-t1)\n",
        "\n",
        "\n",
        "t1 = time.time()\n",
        "for i in np.arange(XtX.shape[0]):\n",
        "\n",
        "  XtXi=cvxopt.sparse(cvxopt.matrix(XtX[i,:,:]))\n",
        "  \n",
        "  t1 = time.time()\n",
        "  F=cholmod.symbolic(XtXi)\n",
        "  cholmod.numeric(XtXi, F)\n",
        "  L=cholmod.getfactor(F)\n",
        "  t2 = time.time()\n",
        "  print(t2-t1)\n",
        "  \n",
        "t2 = time.time()\n",
        "print(t2-t1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-oH5gUphhJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sparse import COO\n",
        "import sparse\n",
        "\n",
        "def GaxCholBC2(XtX):\n",
        "  \n",
        "  print(XtX)\n",
        "  \n",
        "  for i in np.arange(XtX.shape[0]):\n",
        "    \n",
        "    print(i)\n",
        "    \n",
        "    for j in np.arange(XtX.shape[1]):\n",
        "      \n",
        "      print(j)\n",
        "\n",
        "      if j > 0:\n",
        "        \n",
        "        FirstProd = XtX[i,j:,:j]\n",
        "        SecondProd = XtX[i,:j,j]\n",
        "          \n",
        "        XtX[i,j:,j] = XtX[i,j:,j] - sparse.matmul(FirstProd,SecondProd)\n",
        "\n",
        "      XtX[i,j:,j] = XtX[i,j:,j]/np.sqrt(XtX[i,j,j])\n",
        "\n",
        "  return(A)\n",
        "\n",
        "XtX = np.zeros((10000,50,50))\n",
        "for i in np.arange(10000):\n",
        "  X = np.random.randn(50,50)\n",
        "  X[X<1.7]=0\n",
        "  X = X + np.eye(50)\n",
        "\n",
        "  XtXi = np.matmul(X.transpose(), X)\n",
        "\n",
        "  XtX[i,:,:] = XtXi\n",
        "  \n",
        "XtX = sparse.COO.from_numpy(XtX)\n",
        "\n",
        "GaxCholBC2(XtX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zULOU154BE6k",
        "colab_type": "text"
      },
      "source": [
        "### Idea 6: Divide and Conquer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdAmz9BtBFC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def divAndConq(init_theta, current_inds, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds):\n",
        "  \n",
        "  # Number of voxels and dimension of block we are looking at\n",
        "  current_dimv = current_inds.shape\n",
        "  current_nv = np.prod(current_dimv)\n",
        "  \n",
        "  # Current indices as a vector\n",
        "  current_inds_vec = current_inds.reshape(current_nv)\n",
        "  \n",
        "  # Matrices for estimating mean of current block\n",
        "  XtX_current = cvxopt.matrix(XtX[0,:,:])\n",
        "  XtY_current = cvxopt.matrix(np.mean(XtY[current_inds_vec,:,:], axis=0))\n",
        "  XtZ_current = cvxopt.matrix(XtZ[0,:,:])\n",
        "  YtX_current = cvxopt.matrix(np.mean(YtX[current_inds_vec,:,:],axis=0))\n",
        "  YtY_current = cvxopt.matrix(np.mean(YtY[current_inds_vec,:,:],axis=0))\n",
        "  YtZ_current = cvxopt.matrix(np.mean(YtZ[current_inds_vec,:,:],axis=0))\n",
        "  ZtX_current = cvxopt.matrix(ZtX[0,:,:])\n",
        "  ZtY_current = cvxopt.matrix(np.mean(ZtY[current_inds_vec,:,:],axis=0))\n",
        "  ZtZ_current = cvxopt.sparse(cvxopt.matrix(ZtZ[0,:,:]))\n",
        "  \n",
        "  # Get new theta\n",
        "  tmp = minimize(PLS, init_theta, args=(ZtX_current, ZtY_current, XtX_current, ZtZ_current, XtY_current, YtX_current, YtZ_current, XtZ_current, YtY_current, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6)\n",
        "  new_theta = tmp.x\n",
        "  \n",
        "  if current_nv > 1e3:\n",
        "    print('nv: ', current_nv)\n",
        "    print('nit: ', tmp.nit)\n",
        "\n",
        "  \n",
        "  if current_dimv[0]!=1 and current_dimv[1]!=1 and current_dimv[2]!=1:\n",
        "  \n",
        "    # Split into blocks - assuming current inds is a block\n",
        "    current_inds_block1 = current_inds[:(current_dimv[0]//2),:(current_dimv[1]//2),:(current_dimv[2]//2)]\n",
        "    current_inds_block2 = current_inds[(current_dimv[0]//2):,:(current_dimv[1]//2),:(current_dimv[2]//2)]\n",
        "    current_inds_block3 = current_inds[:(current_dimv[0]//2),(current_dimv[1]//2):,:(current_dimv[2]//2)]\n",
        "    current_inds_block4 = current_inds[:(current_dimv[0]//2),:(current_dimv[1]//2),(current_dimv[2]//2):]\n",
        "    current_inds_block5 = current_inds[(current_dimv[0]//2):,(current_dimv[1]//2):,:(current_dimv[2]//2)]\n",
        "    current_inds_block6 = current_inds[(current_dimv[0]//2):,:(current_dimv[1]//2),(current_dimv[2]//2):]\n",
        "    current_inds_block7 = current_inds[:(current_dimv[0]//2),(current_dimv[1]//2):,(current_dimv[2]//2):]\n",
        "    current_inds_block8 = current_inds[(current_dimv[0]//2):,(current_dimv[1]//2):,(current_dimv[2]//2):]\n",
        "  \n",
        "    divAndConq(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block3, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block4, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block5, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block6, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block7, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block8, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "  \n",
        "  elif current_dimv[0]!=1 and current_dimv[1]!=1:\n",
        "    \n",
        "    # Split into blocks - assuming current inds is a block\n",
        "    current_inds_block1 = current_inds[:(current_dimv[0]//2),:(current_dimv[1]//2),:]\n",
        "    current_inds_block2 = current_inds[(current_dimv[0]//2):,:(current_dimv[1]//2),:]\n",
        "    current_inds_block3 = current_inds[:(current_dimv[0]//2),(current_dimv[1]//2):,:]\n",
        "    current_inds_block4 = current_inds[(current_dimv[0]//2):,(current_dimv[1]//2):,:]\n",
        "  \n",
        "    divAndConq(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block3, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block4, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    \n",
        "  elif current_dimv[0]!=1 and current_dimv[2]!=1:\n",
        "\n",
        "    # Split into blocks - assuming current inds is a block\n",
        "    current_inds_block1 = current_inds[:(current_dimv[0]//2),:,:(current_dimv[2]//2)]\n",
        "    current_inds_block2 = current_inds[(current_dimv[0]//2):,:,:(current_dimv[2]//2)]\n",
        "    current_inds_block3 = current_inds[:(current_dimv[0]//2),:,(current_dimv[2]//2):]\n",
        "    current_inds_block4 = current_inds[(current_dimv[0]//2):,:,(current_dimv[2]//2):]\n",
        "  \n",
        "    divAndConq(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block3, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block4, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    \n",
        "  \n",
        "  elif current_dimv[1]!=1 and current_dimv[2]!=1:\n",
        "\n",
        "    # Split into blocks - assuming current inds is a block\n",
        "    current_inds_block1 = current_inds[:,:(current_dimv[1]//2),:(current_dimv[2]//2)]\n",
        "    current_inds_block2 = current_inds[:,(current_dimv[1]//2):,:(current_dimv[2]//2)]\n",
        "    current_inds_block3 = current_inds[:,:(current_dimv[1]//2),(current_dimv[2]//2):]\n",
        "    current_inds_block4 = current_inds[:,(current_dimv[1]//2):,(current_dimv[2]//2):]\n",
        "  \n",
        "    divAndConq(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block3, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block4, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    \n",
        "  elif current_dimv[0]!=1:\n",
        "    \n",
        "    current_inds_block1 = current_inds[:(current_dimv[0]//2),:,:]\n",
        "    current_inds_block2 = current_inds[(current_dimv[0]//2):,:,:]\n",
        "    \n",
        "    divAndConq(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "  \n",
        "  elif current_dimv[1]!=1:\n",
        "    \n",
        "    current_inds_block1 = current_inds[:,:(current_dimv[1]//2),:]\n",
        "    current_inds_block2 = current_inds[:,(current_dimv[1]//2):,:]\n",
        "    \n",
        "    divAndConq(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "  \n",
        "  elif current_dimv[2]!=1:\n",
        "    \n",
        "    current_inds_block1 = current_inds[:,:,:(current_dimv[2]//2)]\n",
        "    current_inds_block2 = current_inds[:,:,(current_dimv[2]//2):]\n",
        "    \n",
        "    divAndConq(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "    divAndConq(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "  \n",
        "  #print('considering... ')\n",
        "  #print(current_inds)\n",
        "  #print('current dimv')\n",
        "  #print(current_dimv)\n",
        "  \n",
        "  #PLS(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "  \n",
        "t1 = time.time()\n",
        "divAndConq(theta0, inds, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "t2 = time.time()\n",
        "print('Time taken (seconds):', t2-t1)\n",
        "print('Estimated time taken on Nifti of size 100 by 100 by 100 (hours):', (t2-t1)*100*100*100/(nv*60*60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYPAtn7ZGP5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x = np.random.randn(10,2,2)\n",
        "print(x.shape)\n",
        "\n",
        "\n",
        "print(np.mean(x,axis=0).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBYuLIE2RbOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def divAndConq2(init_theta, current_inds, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds):\n",
        "  \n",
        "  # Number of voxels and dimension of block we are looking at\n",
        "  current_dimv = current_inds.shape\n",
        "  current_nv = np.prod(current_dimv)\n",
        "  \n",
        "  # Current indices as a vector\n",
        "  current_inds_vec = current_inds.reshape(current_nv)\n",
        "  \n",
        "  # Matrices for estimating mean of current block\n",
        "  XtX_current = cvxopt.matrix(XtX[0,:,:])\n",
        "  XtY_current = cvxopt.matrix(np.mean(XtY[current_inds_vec,:,:], axis=0))\n",
        "  XtZ_current = cvxopt.matrix(XtZ[0,:,:])\n",
        "  YtX_current = cvxopt.matrix(np.mean(YtX[current_inds_vec,:,:],axis=0))\n",
        "  YtY_current = cvxopt.matrix(np.mean(YtY[current_inds_vec,:,:],axis=0))\n",
        "  YtZ_current = cvxopt.matrix(np.mean(YtZ[current_inds_vec,:,:],axis=0))\n",
        "  ZtX_current = cvxopt.matrix(ZtX[0,:,:])\n",
        "  ZtY_current = cvxopt.matrix(np.mean(ZtY[current_inds_vec,:,:],axis=0))\n",
        "  ZtZ_current = cvxopt.sparse(cvxopt.matrix(ZtZ[0,:,:]))\n",
        "  \n",
        "  # Get new theta\n",
        "  tmp = minimize(PLS, init_theta, args=(ZtX_current, ZtY_current, XtX_current, ZtZ_current, XtY_current, YtX_current, YtZ_current, XtZ_current, YtY_current, n, P, I, tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6)\n",
        "  new_theta = tmp.x\n",
        "  \n",
        "  if current_nv > 1e3:\n",
        "    print('nv: ', current_nv)\n",
        "    print('nit: ', tmp.nit)\n",
        "\n",
        "  \n",
        "  if np.amax(current_dimv)!=1:\n",
        "    \n",
        "    if np.amax(current_dimv)==current_dimv[0]:\n",
        "  \n",
        "      # Split into blocks - assuming current inds is a block\n",
        "      current_inds_block1 = current_inds[:(current_dimv[0]//2),:,:]\n",
        "      current_inds_block2 = current_inds[(current_dimv[0]//2):,:,:]\n",
        "\n",
        "      divAndConq2(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "      divAndConq2(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "  \n",
        "\n",
        "    elif np.amax(current_dimv)==current_dimv[1]:\n",
        "\n",
        "      current_inds_block1 = current_inds[:,:(current_dimv[1]//2),:]\n",
        "      current_inds_block2 = current_inds[:,(current_dimv[1]//2):,:]\n",
        "\n",
        "      divAndConq2(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "      divAndConq2(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "\n",
        "    elif np.amax(current_dimv)==current_dimv[2]:\n",
        "\n",
        "      current_inds_block1 = current_inds[:,:,:(current_dimv[2]//2)]\n",
        "      current_inds_block2 = current_inds[:,:,(current_dimv[2]//2):]\n",
        "\n",
        "      divAndConq2(new_theta, current_inds_block1, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "      divAndConq2(new_theta, current_inds_block2, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "  \n",
        "  #print('considering... ')\n",
        "  #print(current_inds)\n",
        "  #print('current dimv')\n",
        "  #print(current_dimv)\n",
        "  \n",
        "  #PLS(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "  \n",
        "t1 = time.time()\n",
        "divAndConq2(theta0, inds, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, n, P, I, tinds, rinds, cinds)\n",
        "t2 = time.time()\n",
        "print('Time taken (seconds):', t2-t1)\n",
        "print('Estimated time taken on Nifti of size 100 by 100 by 100 (hours):', (t2-t1)*100*100*100/(nv*60*60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYP93Pg1GY4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dimv = np.array([30,30,30])\n",
        "\n",
        "nv = np.prod(dimv)\n",
        "\n",
        "inds = np.arange(nv).reshape(dimv)\n",
        "\n",
        "print(inds)\n",
        "\n",
        "print(inds.reshape(nv))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JnpaQhXHKKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(3//2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfBCTa1oIRMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(theta0.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i2LJ0xgnfqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-mEMsaendYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initial theta value. Bates (2005) suggests using [vech(I_q1),...,vech(I_qr)] where I is the identity matrix\n",
        "theta0 = np.array([])\n",
        "for i in np.arange(r):\n",
        "  theta0 = np.hstack((theta0, mat2vech(np.eye(nparams[i])).reshape(np.int64(nparams[i]*(nparams[i]+1)/2))))\n",
        "\n",
        "P = cvxopt.amd.order(cvxopt.sparse(cvxopt.matrix(ZtZ[0,:,:])))\n",
        "I = spmatrix(1.0, range(ZtZ.shape[1]), range(ZtZ.shape[1]))\n",
        "tinds,rinds,cinds=get_mapping(nlevels, nparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RoeSTyUSVJP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6X7u5HongjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}